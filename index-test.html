<!DOCTYPE html>
<html lang="en" class="dark"> <!-- Default to dark theme -->
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AIDEFEND Framework Viewer</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        :root {
            /* Light Theme (Softer, Material-inspired) */
            --bg-color: #f0f2f5; --text-color: #37474f; --text-secondary-color: #607d8b; --header-bg: #ffffff; --header-border: #e0e0e0; --column-bg: #ffffff; --column-border: #e0e0e0; --column-header-text: #455a64; --column-header-hover-text: #1976d2; --technique-item-bg: #ffffff; --technique-item-border: #e8eaf6; --technique-item-text: #2196f3; --technique-item-hover-bg: #e3f2fd; --technique-item-hover-text: #1565c0; --technique-item-highlight-bg: #bbdefb; --technique-item-highlight-text: #0d47a1; --technique-id-text: #78909c; --modal-content-bg: #ffffff; --modal-close-btn-text: #757575; --modal-close-btn-hover-text: #212121; --search-bg: #eceff1; --search-border: #b0bec5; --search-focus-ring: #2196f3; --scrollbar-track: #e0e0e0; --scrollbar-thumb: #90a4ae; --scrollbar-thumb-hover: #78909c; --icon-fill: #607d8b; --icon-hover-fill: #2196f3; --link-color: #1976d2; --link-hover-color: #0d47a1; --shadow-umbra: rgba(0, 0, 0, 0.15); --shadow-penumbra: rgba(0, 0, 0, 0.1); --shadow-ambient: rgba(0, 0, 0, 0.08); --footer-bg: #e0e0e0; --footer-text: #757575; --footer-border: #c0c0c0;
        }
        html.dark {
            /* Dark Theme (Material Inspired - slightly adjusted for consistency) */
            --bg-color: #1a1a1a; --text-color: #e0e0e0; --text-secondary-color: #b0b0b0; --header-bg: #222222; --header-border: #333333; --column-bg: #222222; --column-border: #333333; --column-header-text: #f0f0f0; --column-header-hover-text: #90caf9; --technique-item-bg: #2b2b2b; --technique-item-border: #3d3d3d; --technique-item-text: #90caf9; --technique-item-hover-bg: #383838; --technique-item-hover-text: #bbdefb; --technique-item-highlight-bg: #1e3a5f; --technique-item-highlight-text: #e3f2fd; --technique-id-text: #c0c0c0; --modal-content-bg: #222222; --modal-close-btn-text: #b0b0b0; --modal-close-btn-hover-text: #f0f0f0; --search-bg: #333333; --search-border: #444444; --search-focus-ring: #90caf9; --scrollbar-track: #2b2b2b; --scrollbar-thumb: #555555; --scrollbar-thumb-hover: #777777; --icon-fill: #c0c0c0; --icon-hover-fill: #90caf9; --link-color: #90caf9; --link-hover-color: #e3f2fd; --shadow-umbra: rgba(0, 0, 0, 0.5); --shadow-penumbra: rgba(0, 0, 0, 0.35); --shadow-ambient: rgba(0, 0, 0, 0.3); --footer-bg: #2b2b2b; --footer-text: #a0a0a0; --footer-border: #3d3d3d;
        }
        html { height: 100%; }
        body { font-family: 'Inter', sans-serif; background-color: var(--bg-color); color: var(--text-color); transition: background-color 0.3s ease, color 0.3s ease; min-height: 100%; display: flex; flex-direction: column; }
        #page-container { flex: 1 0 auto; }
        footer { flex-shrink: 0; }
        body.modal-open { overflow: hidden; }
        ::-webkit-scrollbar { width: 8px; height: 8px; }
        ::-webkit-scrollbar-track { background: var(--scrollbar-track); border-radius: 10px; }
        ::-webkit-scrollbar-thumb { background: var(--scrollbar-thumb); border-radius: 10px; }
        ::-webkit-scrollbar-thumb:hover { background: var(--scrollbar-thumb-hover); }
        .elevation-2 { box-shadow: 0px 3px 1px -2px var(--shadow-umbra), 0px 2px 2px 0px var(--shadow-penumbra), 0px 1px 5px 0px var(--shadow-ambient); }
        .elevation-4 { box-shadow: 0px 2px 4px -1px var(--shadow-umbra), 0px 4px 5px 0px var(--shadow-penumbra), 0px 1px 10px 0px var(--shadow-ambient); }
        .elevation-8 { box-shadow: 0px 5px 5px -3px var(--shadow-umbra), 0px 8px 10px 1px var(--shadow-penumbra), 0px 3px 14px 2px var(--shadow-ambient); }
        .header-main { background-color: var(--header-bg); transition: background-color 0.3s ease, border-color 0.3s ease; }
        .header-title { font-size: 2rem; font-weight: 500; color: var(--text-color); }
        .header-subtitle { font-size: 0.875rem; color: var(--text-secondary-color); }
        .search-input-wrapper { position: relative; display: flex; align-items: center; }
        .search-container input { background-color: var(--search-bg); border: 1px solid var(--search-border); color: var(--text-color); border-radius: 0.5rem; padding-right: 2.5rem; }
        .search-container input::placeholder { color: var(--text-secondary-color); }
        .search-container input:focus { --tw-ring-color: var(--search-focus-ring); border-color: var(--search-focus-ring); box-shadow: 0 0 0 2px var(--search-focus-ring); }
        .search-clear-button { position: absolute; right: 0.5rem; padding: 0.25rem; color: var(--text-secondary-color); cursor: pointer; display: none; }
        .search-clear-button:hover { color: var(--text-color); }
        .search-clear-button svg { width: 1.25rem; height: 1.25rem; }
        .header-button { background-color: transparent; color: var(--icon-fill); border: none; padding: 0.625rem; border-radius: 50%; display: flex; align-items: center; justify-content: center; }
        .header-button:hover { background-color: rgba(0,0,0,0.04); color: var(--icon-hover-fill); }
        html.dark .header-button:hover { background-color: rgba(255,255,255,0.08); }
        .header-button svg { width: 1.5rem; height: 1.5rem; fill: currentColor; margin-right: 0; }
        .header-button.text-button { padding: 0.5rem 1rem; border-radius: 0.25rem; font-weight: 500; }
        .header-button.text-button svg { margin-right: 0.5rem; width: 1.25rem; height: 1.25rem; }
        .tactic-column-grid { display: flex; flex-wrap: wrap; gap: 0.28rem; padding: 1.5rem 0.5rem; align-items: flex-start; justify-content: center; }
        .tactic-column { flex-grow: 1; flex-shrink: 1; flex-basis: 160px; max-width: 210px; background-color: var(--column-bg); border-radius: 0.75rem; padding: 0.8rem; min-height: 200px; transition: background-color 0.3s ease, border-color 0.3s ease, box-shadow 0.3s ease; display: flex; flex-direction: column; }
        .tactic-column-header { font-size: 1.25rem; font-weight: 500; color: var(--column-header-text); margin-bottom: 1rem; padding-bottom: 0.75rem; border-bottom: 1px solid var(--column-border); transition: color 0.3s ease, border-color 0.3s ease; cursor: pointer; text-align: center; }
        .tactic-column-header:hover { color: var(--column-header-hover-text); }
        .technique-item { display: block; background-color: var(--technique-item-bg); padding: 0.75rem 1rem; font-size: 0.875rem; color: var(--technique-item-text); border-radius: 0.5rem; cursor: pointer; transition: background-color 0.2s ease-in-out, color 0.2s ease-in-out, transform 0.2s ease, box-shadow 0.2s ease; line-height: 1.5; margin-bottom: 0.75rem; }
        .technique-item:hover { background-color: var(--technique-item-hover-bg); color: var(--technique-item-hover-text); transform: translateY(-2px); }
        .technique-item.highlight { background-color: var(--technique-item-highlight-bg); color: var(--technique-item-highlight-text); font-weight: 600; }
        .technique-id { font-weight: 500; color: var(--technique-id-text); font-size: 0.75rem; margin-right: 0.35rem; display: block; margin-bottom: 0.125rem; }
        .technique-name { font-weight: 500; }
        .modal-overlay { position: fixed; top: 0; left: 0; right: 0; bottom: 0; background-color: rgba(0, 0, 0, 0.5); display: flex; align-items: center; justify-content: center; z-index: 1000; opacity: 0; visibility: hidden; transition: opacity 0.2s ease, visibility 0.2s ease; }
        html.dark .modal-overlay { background-color: rgba(0, 0, 0, 0.7); }
        .modal-overlay.active { opacity: 1; visibility: visible; }
        .modal-content { background-color: var(--modal-content-bg); color: var(--text-color); padding: 2rem; border-radius: 0.5rem; width: 90%; max-width: 800px; max-height: 90vh; overflow-y: auto; position: relative; transition: background-color 0.3s ease, color 0.3s ease, transform 0.2s ease; transform: scale(0.95); }
        .modal-overlay.active .modal-content { transform: scale(1); }
        .modal-content h2.modal-main-title { font-size: 1.75rem; font-weight: 500; margin-bottom: 1.5rem; color: var(--column-header-hover-text); }
        .modal-content h2 { font-size: 1.5rem; font-weight: 500; margin-bottom: 1rem; color: var(--column-header-hover-text); }
        .modal-content h3 { font-size: 1.25rem; font-weight: 500; margin-top: 1.5rem; margin-bottom: 0.75rem; color: var(--text-color); }
        .modal-content h4 { font-size: 1rem; font-weight: 500; margin-top: 1rem; margin-bottom: 0.5rem; color: var(--text-color); }
        .modal-content p { font-size: 0.9375rem; line-height: 1.7; margin-bottom: 1rem; }
        .modal-content ul { list-style-type: disc; margin-left: 1.5rem; font-size: 0.9375rem; line-height: 1.7; margin-bottom: 1rem; }
        .modal-content ul ul { margin-top: 0.5rem; margin-bottom: 0.5rem; }
        .modal-content li { margin-bottom: 0.375rem; }
        .modal-content a { color: var(--link-color); text-decoration: none; font-weight: 500; }
        .modal-content a:hover { color: var(--link-hover-color); text-decoration: underline; }
        .modal-close-button { position: absolute; top: 1rem; right: 1rem; background: none; border: none; font-size: 1.75rem; color: var(--modal-close-btn-text); cursor: pointer; line-height: 1; transition: color 0.3s ease; padding: 0.25rem; }
        .modal-close-button:hover { color: var(--modal-close-btn-hover-text); }
        .defends-against-framework { font-weight: 500; color: var(--technique-item-text); opacity: 0.9; }
        .defends-against-item { margin-left: 1rem; list-style-type: disc; }
        .site-footer { background-color: var(--footer-bg); color: var(--footer-text); padding: 1rem 1rem; text-align: center; font-size: 0.875rem; border-top: 1px solid var(--footer-border); margin-top: auto; }
        .site-footer p { margin-bottom: 0.25rem; line-height: 1.5; }
        html.dark .header-version-text { opacity: 1; color: var(--text-secondary-color); }
        .accordion-item { border-bottom: 1px solid var(--header-border); }
        .accordion-header { display: flex; justify-content: space-between; align-items: center; padding: 1.25rem 0.5rem; cursor: pointer; font-size: 1.125rem; font-weight: 500; color: var(--text-color); transition: color 0.2s ease; }
        .accordion-header:hover { color: var(--column-header-hover-text); }
        .accordion-icon { width: 1.25rem; height: 1.25rem; transition: transform 0.3s ease-in-out; flex-shrink: 0; margin-left: 1rem; }
        .accordion-content { max-height: 0; overflow: hidden; transition: max-height 0.3s ease-in-out, padding 0.3s ease-in-out; font-size: 0.9375rem; line-height: 1.7; color: var(--text-secondary-color); }
        .accordion-content-inner { padding: 0 0.5rem 1.25rem 0.5rem; }
        .gemini-helper-btn { display: inline-flex; align-items: center; justify-content: center; padding: 0.5rem 1rem; margin: 0 0 0.5rem 1rem; vertical-align: middle; border: 1px solid var(--technique-item-text); color: var(--technique-item-text); background-color: transparent; border-radius: 0.5rem; font-weight: 500; font-size: 0.875rem; cursor: pointer; transition: background-color 0.2s ease, color 0.2s ease; }
        .gemini-helper-btn:hover:not(:disabled) { background-color: var(--technique-item-highlight-bg); color: var(--technique-item-highlight-text); }
        .gemini-helper-btn:disabled { opacity: 0.5; cursor: not-allowed; }
        .gemini-helper-btn svg { width: 1.25rem; height: 1.25rem; margin-right: 0.5rem; }
        #gemini-response-container { background-color: var(--search-bg); border: 1px solid var(--column-border); border-radius: 0.5rem; padding: 1rem; margin-top: 1rem; min-height: 50px; display: none; }
        #gemini-response-container pre { white-space: pre-wrap; word-wrap: break-word; font-family: monospace; font-size: 0.875rem; line-height: 1.6; }
        #gemini-response-container code { display: block; background-color: var(--bg-color); padding: 0.75rem; border-radius: 0.25rem; margin-top: 0.5rem; margin-bottom: 0.5rem; }
        .loader { width: 24px; height: 24px; border: 3px solid var(--technique-item-text); border-bottom-color: transparent; border-radius: 50%; display: inline-block; box-sizing: border-box; animation: rotation 1s linear infinite; margin: 1rem auto; }
        @keyframes rotation { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }
        .prose { line-height: 1.7; } .prose h3 { margin-top: 1em; margin-bottom: 0.5em; } .prose pre { margin-top: 0; }
    </style>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
</head>
<body class="text-gray-800">
    <div id="page-container">
        <header class="header-main py-4 px-6 fixed top-0 left-0 right-0 z-50 elevation-4">
             <div class="container mx-auto">
                 <div class="flex justify-between items-center mb-3">
                     <div class="text-left">
                         <h1 class="header-title">AIDEFEND™</h1>
                         <p class="header-subtitle">An AI-focused defensive countermeasures knowledge base</p>
                         <p class="text-xs opacity-60 mt-0.5 header-version-text">Version 1.0</p>
                     </div>
                     <div class="flex items-center space-x-1"> 
                         <button id="aboutBtn" class="header-button text-button" title="About AIDEFEND">
                             <!-- Icon will be injected by JS --> About
                         </button>
                         <button id="themeToggleBtn" class="header-button" title="Toggle Theme">
                             <!-- SVG icon will be injected here by JavaScript -->
                         </button>
                     </div>
                 </div>
                 <div class="search-input-wrapper search-container">
                     <input type="text" id="search-bar" placeholder="Search keywords, AIDEFEND/ATLAS techniques, MAESTRO threats, OWASP Top10s, etc..." class="w-full p-2.5 text-sm">
                     <button id="searchClearBtn" class="search-clear-button" title="Clear search">
                         <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="w-5 h-5">
                             <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16ZM8.28 7.22a.75.75 0 00-1.06 1.06L8.94 10l-1.72 1.72a.75.75 0 101.06 1.06L10 11.06l1.72 1.72a.75.75 0 101.06-1.06L11.06 10l1.72-1.72a.75.75 0 00-1.06-1.06L10 8.94 8.28 7.22Z" clip-rule="evenodd" />
                         </svg>
                     </button>
                 </div>
             </div>
        </header>

        <div id="content-wrapper" class="pt-44 px-4 sm:px-6 lg:px-8 pb-8"> 
            <main id="main-content" class="container mx-auto"></main>
        </div>
    </div> 
    
    <div id="infoModal" class="modal-overlay">
        <div id="modalBackdrop" class="absolute inset-0"></div> 
        <div class="modal-content elevation-8">
            <button id="modalClose" class="modal-close-button">&times;</button>
            <div id="modalBody"></div>
        </div>
    </div>
    
    <footer class="site-footer">
        <p>AIDEFEND, An AI-focused defensive countermeasures knowledge base.</p>
        <p>Information based on the <a href="https://cloudsecurityalliance.org/blog/2025/02/06/agentic-ai-threat-modeling-framework-maestro" target="_blank" rel="noopener noreferrer" class="text-sky-600 dark:text-sky-400 hover:underline">MAESTRO framework</a>, <a href="https://d3fend.mitre.org/" target="_blank" rel="noopener noreferrer" class="text-sky-600 dark:text-sky-400 hover:underline">MITRE D3FEND</a>, <a href="https://atlas.mitre.org/" target="_blank" rel="noopener noreferrer" class="text-sky-600 dark:text-sky-400 hover:underline">ATLAS</a>, <a href="https://attack.mitre.org/" target="_blank" rel="noopener noreferrer" class="text-sky-600 dark:text-sky-400 hover:underline">ATT&CK</a>, and <a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/" target="_blank" rel="noopener noreferrer" class="text-sky-600 dark:text-sky-400 hover:underline">OWASP Top 10 LLM 2025</a>/<a href="https://owasp.org/www-project-machine-learning-security-top-10/" target="_blank" rel="noopener noreferrer" class="text-sky-600 dark:text-sky-400 hover:underline">OWASP Top 10 ML Security 2023</a>.</p>
        <p>2025 Edward's Playground - AIDEFEND framework Initiative. For informational purposes only.</p>
        <p>All content on this website is licensed under the <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener noreferrer" class="text-sky-600 dark:text-sky-400 hover:underline">Creative Commons Attribution 4.0 International License (CC BY 4.0)</a></p>
    </footer>


    <script>
        // --- Data ---
        const aidefendIntroduction = {
            mainTitle: "About AIDEFEND: An AI Defense Framework",
            sections: [
                {
                    title: "What is AIDEFEND?",
                    paragraphs: [
                        "AIDEFEND (Artificial Intelligence Defense Framework) is a knowledge base of defensive countermeasures designed to protect AI/ML systems. Inspired by cybersecurity frameworks like MITRE D3FEND, MITRE ATT&CK®, and MITRE ATLAS®, AIDEFEND complements MITRE ATLAS® by focusing on AI defense.<br><br><strong>Please note: <u>This work is a personal initiative.</u></strong> It was inspired by resources including frameworks (D3FEND, ATT&CK, ATLAS) by MITRE, the MAESTRO Threat Modeling framework by Ken Huang (Cloud Security Alliance Research Fellow), and the OWASP Top 10 Lists (LLM Applications 2025, ML Security 2023) from OWASP. However, <u><strong>this work is not affiliated with, endorsed by, or otherwise connected to the MITRE Corporation, the creator of the MAESTRO framework (Ken Huang), or OWASP.</u></strong>"
                    ]
                },
                {
                    title: "What has been developed?",
                    paragraphs: [
                        "Developed using the seven defensive tactics from MITRE D3FEND (Model, Harden, Detect, Isolate, Deceive, Evict, Restore), AIDEFEND organizes AI-specific defensive techniques. These techniques are mapped to AI attacks and threats from sources such as MITRE ATLAS®, MAESTRO, and OWASP Top 10 lists (LLM Applications 2025, Machine Learning Security 2023), providing a comprehensive view of how each defense mitigates known vulnerabilities."
                    ]
                },
                {
                    title: "How can this framework be utilized?",
                    paragraphs: [
                        "AIDEFEND is designed as a practical tool for organizations to systematically enhance their AI security posture. It is presented in a matrix format, aligning defensive techniques with the D3FEND tactical categories. This structure allows security professionals to:"
                    ],
                    listItems: [
                        "<strong>Assess Current Capabilities:</strong> Evaluate existing AI defenses against the AIDEFEND framework.",
                        "<strong>Identify Gaps:</strong> Pinpoint areas where AI-specific defenses are lacking.",
                        "<strong>Prioritize Defenses:</strong> Use the threat mappings (to ATLAS, MAESTRO, OWASP) to select techniques that address the most relevant risks to their specific AI systems, informed by their own risk assessments.",
                        "<strong>Plan Implementation:</strong> Leverage the provided descriptions, implementation strategies, and tool suggestions to develop actionable plans.",
                        "<strong>Enhance AI Security Posture:</strong> Systematically improve the resilience of AI deployments."
                    ],
                    concludingParagraphs: [
                            "Each technique in the matrix includes a unique ID, name, a description of the defensive method focused on AI, practical implementation strategies, and examples of open-source and commercial tools. The \"Defends Against\" column explicitly links the technique to threats from MITRE ATLAS®, MAESTRO, and the relevant OWASP Top 10 lists."
                    ]
                },
                {
                    title: "Who is behind this initiative?",
                    paragraphs: [
                        "This work is led by Edward Lee. I'm passionate about Cybersecurity, AI and emerging technologies, and will always be a learner. <a href=\"https://www.linkedin.com/in/go-edwardlee/\" target=\"_blank\" rel=\"noopener noreferrer\">Connect with me on LinkedIn</a>."
                    ]
                },
                {
                    title: "Version & Date",
                    paragraphs: [
                        "Version: 1.0",
                        "Last Updated: June 6, 2025"
                    ]
                },
                {
                    title: "Frameworks & Resources Included",
                    paragraphs: [
                        "MAESTRO Framework: An Agentic AI threat modeling framework created by Ken Huang.",
                        "MITRE D3FEND™ Framework: A knowledge graph of cybersecurity countermeasure techniques developed by MITRE.",
                        "MITRE ATT&CK® Framework: A globally accessible knowledge base of adversary tactics and techniques based on real-world observations developed by MITRE.",
                        "MITRE ATLAS™ Framework: A threat modeling framework for AI systems, cataloging adversary behaviors specific to machine learning developed by MITRE",
                        "OWASP Top 10 for LLM Applications 2025: A curated list of the most critical security risks to large language model applications.",
                        "OWASP Top 10 for Machine Learning Security 2023: A security awareness and risk prioritization guide addressing common vulnerabilities in ML systems."
                    ]
                }
            ]
        };

        const aidefendData = {
    introduction: aidefendIntroduction,
    tactics: [
        {
            "name": "Model",
            "purpose": "The \"Model\" tactic, in the context of AI security, focuses on developing a comprehensive understanding and detailed mapping of all AI/ML assets, their configurations, data flows, operational behaviors, and interdependencies. This foundational knowledge is crucial for informing and enabling all subsequent defensive actions. It involves knowing precisely what AI systems exist within the organization, how they are architected, what data they ingest and produce, their critical dependencies (both internal and external), and their expected operational parameters and potential emergent behaviors.",
            "techniques": [
                { "id": "AID-M-001", "name": "AI Asset Inventory & Mapping", "description": "Systematically catalog and map all AI/ML assets, including models (categorized by type, version, deployment location, and ownership), datasets (training, validation, testing, and operational), data pipelines, and APIs. This process includes mapping their configurations, data flows (sources, transformations, destinations), and interdependencies (e.g., reliance on third-party APIs, upstream data providers, or specific libraries). The goal is to achieve comprehensive visibility into all components that constitute the AI ecosystem and require protection. This technique is foundational as it underpins the ability to apply targeted security controls and assess risk accurately.", "implementationStrategies": ["Establish and maintain a dynamic, up-to-date inventory of all AI models, datasets, software components, and associated infrastructure.", "Map data flows for each AI system, documenting data sources, lineage, processing stages, storage locations, and consumers.", "Document dependencies for each AI asset, including software libraries, external services, and other AI models.", "Regularly audit the inventory and mappings for accuracy and completeness, updating them as AI systems evolve.", "Assign clear ownership and accountability for each inventoried AI asset and its security.", "Integrate AI asset inventory with broader IT asset management and configuration management databases (CMDBs) where appropriate.", "Utilize automated discovery tools where possible, but supplement with manual verification, especially for novel AI components.", "Include specialized AI accelerators (GPUs, TPUs, NPUs, FPGAs) and their firmware versions in the AI asset inventory, relevant for AID-H-009."], "toolsOpenSource": ["Custom scripts for querying model registries (e.g., MLflow, Kubeflow) and data storage.", "Great Expectations (for data asset profiling and documentation).", "DVC (Data Version Control, for tracking dataset versions and lineage).", "Apache Atlas, DataHub, Amundsen, OpenMetadata (for comprehensive metadata management, data discovery, and lineage).", "General IT asset management tools like Snipe-IT or Budibase may be adapted."], "toolsCommercial": ["AI Security Posture Management (AI-SPM) platforms: Wiz AI-SPM, Microsoft Defender for Cloud, Palo Alto Networks Prisma Cloud AI-SPM.", "Data catalog and governance platforms: Alation, Collibra, Informatica Enterprise Data Catalog, OvalEdge.", "MLOps platforms with model registry and artifact tracking: Azure ML, Google Vertex AI, Databricks, Amazon SageMaker.", "Specialized AI inventory management software."], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0007 Discover ML Artifacts", "AML.T0002 Acquire Public ML Artifacts"] }, { "framework": "MAESTRO", "items": ["Foundational for assessing risks across all layers", "Agent Supply Chain (L7) understanding"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["Indirectly LLM03:2025 Supply Chain"] }, { "framework": "OWASP ML Top 10 2023", "items": ["Indirectly ML06:2023 AI Supply Chain Attacks"] }] },
                { "id": "AID-M-002", "name": "Data Provenance & Lineage Tracking", "description": "Establish and maintain verifiable records of the origin, history, and transformations of data used in AI systems, particularly training and fine-tuning data. This includes tracking model updates and their associated data versions. The objective is to ensure the trustworthiness and integrity of data and models by knowing their complete lifecycle, from source to deployment, and to facilitate auditing and incident investigation. This often involves cryptographic methods like signing or checksumming datasets and subunits and models at critical stages.", "implementationStrategies": ["Implement robust data version control systems (e.g., DVC, Git-LFS for data).", "Maintain detailed metadata for datasets (e.g., \"datasheets for datasets\") and models (e.g., \"model cards\").", "Employ cryptographic checksums (e.g., SHA-256) or digital signatures.", "Rigorously vet and document third-party or public data sources.", "Automate lineage tracking where possible.", "Regularly audit data provenance and lineage records."], "toolsOpenSource": ["DVC", "MLflow", "Apache Atlas, DataHub, Amundsen, OpenMetadata", "LakeFS", "Pachyderm"], "toolsCommercial": ["Azure ML", "Google Vertex AI", "Collibra, Alation, Informatica PowerCenter, Talend Data Catalog", "Databricks"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0020 Poison Training Data", "AML.T0008 ML Supply Chain Compromise", "AML.T0018.000 Backdoor ML Model: Poison ML Model"] }, { "framework": "MAESTRO", "items": ["Data Poisoning (L2: Data Operations)", "Compromised RAG Pipelines (L2: Data Operations)", "Model Skewing (L2: Data Operations)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM04:2025 Data and Model Poisoning", "LLM03:2025 Supply Chain"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML02:2023 Data Poisoning Attack", "ML10:2023 Model Poisoning", "ML07:2023 Transfer Learning Attack"] }] },
                { "id": "AID-M-003", "name": "Model Behavior Baseline & Documentation", "description": "Establish, document, and maintain a comprehensive baseline of expected AI model behavior. This includes defining its intended purpose, architectural details, training data characteristics, operational assumptions, limitations, and key performance metrics (e.g., accuracy, precision, recall, output distributions, latency, confidence scores) under normal conditions. This documentation, often in the form of model cards, and the established behavioral baseline serve as a reference to detect anomalies, drift, or unexpected outputs that might indicate an attack or system degradation, and to inform risk assessments and incident response.", "implementationStrategies": ["Develop detailed model cards for each deployed AI model.", "Establish quantitative baselines for key performance and operational metrics.", "Simulate expected usage patterns to record normative behavior.", "Regularly review and update model documentation and baselines.", "Store model cards and baseline documentation in a centralized repository.", "Where XAI methods are utilized for model understanding or diagnostics, baseline their typical outputs (e.g., feature attributions, decision rules) for a diverse set of known inputs. Document expected explanatory behavior to help identify anomalies investigated by AID-D-006.", "For autonomous agents, record the cryptographically signed mission objectives and goal hierarchy in the model card; these become the reference used by runtime Goal-Integrity Monitoring (see AID-D-010)."], "toolsOpenSource": ["Alibi Detect", "Evidently AI, ClearML, NannyML, Langfuse, Phoenix", "Google's Model Card Toolkit", "Sphinx or MkDocs"], "toolsCommercial": ["Fiddler, Arize AI, WhyLabs", "IBM Watson OpenScale, Azure Model Monitor, Amazon SageMaker Model Monitor, Google Vertex AI Model Monitoring", "BytePlus ModelArk", "Google Cloud Model Card service"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0015 Evade ML Model", "AML.T0054 LLM Jailbreak", "AML.T0021 Erode ML Model Integrity"] }, { "framework": "MAESTRO", "items": ["Evasion of Security AI Agents (L6)", "Unpredictable agent behavior / Performance Degradation (L5)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM01:2025 Prompt Injection (jailbreaking aspect)", "LLM09:2025 Misinformation"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML01:2023 Input Manipulation Attack", "ML08:2023 Model Skewing"] }] },
                { "id": "AID-M-004", "name": "AI Threat Modeling & Risk Assessment", "description": "Systematically identify, analyze, and prioritize potential AI-specific threats and vulnerabilities for each AI component (e.g., data, models, algorithms, pipelines, agentic capabilities, APIs) throughout its lifecycle. This process involves understanding how an adversary might attack the AI system and assessing the potential impact of such attacks. The outcomes guide the design of appropriate defensive measures and inform risk management strategies. This proactive approach is essential for building resilient AI systems.", "implementationStrategies": ["Utilize established threat modeling methodologies (STRIDE, PASTA, OCTAVE) adapted for AI.", "Leverage AI-specific threat frameworks (ATLAS, MAESTRO, OWASP).", "For agentic AI, consider tool misuse, memory tampering, goal manipulation, etc.", "Explicitly include the model training process, environment, and MLOps pipeline components in threat modeling exercises, considering threats of training data manipulation, training code compromise, and environment exploitation (relevant to defenses like AID-H-007).", "For systems employing federated learning, specifically model threats related to malicious client participation, insecure aggregation protocols, and potential inference attacks against client data, and evaluate countermeasures like AID-H-008.", "Explicitly model threats related to AI hardware security, including side-channel attacks, fault injection, and physical tampering against AI accelerators (addressed by AID-H-009).", "Involve a multi-disciplinary team.", "Prioritize risks based on likelihood and impact.", "Document threat models and integrate into MLOps.", "Regularly review and update threat models."], "toolsOpenSource": ["MITRE ATLAS Navigator", "MAESTRO framework documentation", "OWASP Top 10 checklists", "OWASP Threat Dragon, Microsoft Threat Modeling Tool", "Academic frameworks (ATM for LLMs, ATFAA)", "NIST AI RMF and Playbook"], "toolsCommercial": ["AI security consulting services", "AI governance and risk management platforms (OneTrust AI Governance, FlowForma)", "Some AI red teaming platforms"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["Proactively addresses all relevant tactics (Reconnaissance, Resource Development, Initial Access, ML Model Access, Execution, Impact, etc.)"] }, { "framework": "MAESTRO", "items": ["Systematically addresses threats across all 7 Layers and Cross-Layer threats"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["Enables proactive consideration for all 10 risks (LLM01-LLM10)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["Enables proactive consideration for all 10 risks (ML01-ML10)"] }] },
                { "id": "AID-M-005", "name": "AI Configuration Benchmarking & Secure Baselines", "description": "Establish, document, maintain, and regularly audit secure configurations for all components of AI systems. This includes the underlying infrastructure (cloud instances, GPU clusters, networks), ML libraries and frameworks, agent runtimes, MLOps pipelines, and specific settings within AI platform APIs (e.g., LLM function access). Configurations are benchmarked against industry standards (e.g., CIS Benchmarks, NIST SSDF), vendor guidance, and internal security policies to identify and remediate misconfigurations that could be exploited by attackers.", "implementationStrategies": ["Develop and enforce secure baseline configurations.", "Harden default settings for AI platforms and tools.", "Utilize security benchmarks (CIS, NIST SSDF) and vulnerability databases.", "Implement Infrastructure as Code (IaC) and use IaC security scanners.", "Regularly audit deployed configurations for drift.", "Integrate AI-specific configuration policies into CSPM tools."], "toolsOpenSource": ["OpenSCAP", "Checkov, Terrascan, tfsec", "CIS Benchmarks", "NIST SSDF"], "toolsCommercial": ["CSPM tools (Wiz, Prisma Cloud, Microsoft Defender)", "Vulnerability management solutions", "Configuration management tools (Ansible, Chef, Puppet)"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0011 Initial Access (misconfigurations)", "AML.T0009 Execution (insecure settings)"] }, { "framework": "MAESTRO", "items": ["Misconfigurations in L4: Deployment & Infrastructure", "Insecure default settings in L3: Agent Frameworks or L1: Foundation Models"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM03:2025 Supply Chain", "Indirectly LLM06:2025 Excessive Agency"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML06:2023 AI Supply Chain Attacks (misconfigured components)"] }] },
                { "id": "AID-M-006", "name": "Human-in-the-Loop (HITL) Control Point Mapping", "description": "Systematically identify, document, map, and validate all designed human intervention, oversight, and control points within AI systems. This is especially critical for agentic AI and systems capable of high-impact autonomous decision-making. The process includes defining the triggers, procedures, required operator training, and authority levels for human review, override, or emergency system halt. The goal is to ensure that human control can be effectively, safely, and reliably exercised when automated defenses fail, novel threats emerge, or ethical boundaries are approached.", "implementationStrategies": ["Integrate HITL checkpoint design into the AI system development lifecycle from the earliest stages.", "Clearly document all HITL interaction points, including expected scenarios, operator actions, and system responses, within the AI system's operational guide.", "Define and test clear escalation paths for human intervention, specifying roles and responsibilities.", "Develop comprehensive training programs for operators responsible for HITL actions, including simulation of emergency scenarios.", "Regularly audit and test HITL mechanisms (e.g., through \"fire drill\" exercises) to ensure their continued functionality and operator preparedness.", "Implement robust logging and monitoring for all HITL activations and interventions for later review and auditing."], "toolsOpenSource": ["Business Process Model and Notation (BPMN) tools (e.g., Camunda Modeler, jBPM).", "Diagramming tools (e.g., diagrams.net (formerly draw.io)).", "Workflow engines (e.g., Apache Airflow, Prefect, with custom HITL tasks).", "Documentation platforms (e.g., Confluence, Sphinx, MkDocs)."], "toolsCommercial": ["Enterprise Architecture (EA) modeling tools.", "Specialized AI governance platforms with HITL workflow design and management features.", "Simulation platforms for operator training."], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["Indirectly mitigates AML.T0048 External Harms (by enabling human intervention to prevent or reduce harm from autonomous AI decisions)", "AML.T0009 Execution (if human oversight can interrupt or redirect harmful execution paths initiated by compromised AI)."] }, { "framework": "MAESTRO", "items": ["Runaway Agent Behavior (L7: Agent Ecosystem)", "Agent Goal Manipulation (L7: Agent Ecosystem) by providing an override mechanism", "Unpredictable agent behavior / Performance Degradation (L5: Evaluation & Observability) by allowing human assessment and control", "Failure of Safety Interlocks (L6: Security & Compliance)."] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM06:2025 Excessive Agency (by providing a defined mechanism for human control over agent actions and decisions, acting as a crucial backstop)."] }, { "framework": "OWASP ML Top 10 2023", "items": ["Contributes to overall system safety and robustness, helping to manage the impact of various attacks by ensuring human oversight can be asserted."] }] }
            ]
        },
        {
            "name": "Harden",
            "purpose": "The \"Harden\" tactic encompasses proactive measures taken to reinforce AI systems and reduce their attack surface before an attack occurs. These techniques aim to make AI models, the data they rely on, and the infrastructure they inhabit more resilient to compromise. This involves building security into the design and development phases and applying preventative controls to make successful attacks more difficult, costly, and less impactful for adversaries.",
            "techniques": [
                {
                    "id": "AID-H-001", "name": "Adversarial Training & Robust Model Architectures", "description": "Proactively improve a model's resilience to adversarial inputs by training it with examples specifically crafted to try and fool it (adversarial examples). This process \"vaccinates\" the model, making it more robust against evasion attacks where slight, often imperceptible, perturbations to input data cause misclassification or other erroneous behavior. This can be complemented by selecting or designing model architectures (e.g., ensembles, specific types of neural network layers or activation functions) that are inherently more resistant to such manipulations.", "perfImpact": { "level": "High", "description": "Note: Performance Impact: High (on Training Time & Cost). This technique directly increases the number of computations required during model training. Instead of just one forward/backward pass per batch, adversarial training methods like Projected Gradient Descent (PGD) require multiple passes to generate adversarial examples. Training Time: Can increase training duration by 3x to 15x, depending on the number of attack steps (e.g., PGD-10 vs. PGD-100) and the complexity of the attack generation. Inference Latency: Minimal to no impact." }, "implementationStrategies": [ "Generate diverse adversarial examples (FGSM, PGD, C&W).", "Incorporate adversarial examples into training data.", "Utilize robust model architectures (ensembles, certified robustness).", "Employ defensive distillation.", "Apply feature squeezing or input transformations.", "Regularly evaluate and retrain for robustness." ], "toolsOpenSource": [ "Adversarial Robustness Toolbox (ART) by IBM", "Foolbox", "CleverHans", "TensorFlow Privacy, PyTorch Opacus" ], "toolsCommercial": [ "Robust Intelligence", "HiddenLayer MLSec", "Bosch AIShield", "Adversa AI", "Microsoft Counterfit" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": ["AML.T0015 Evade ML Model", "AML.T0006 Defense Evasion"] }, { "framework": "MAESTRO", "items": ["Adversarial Examples (L1)", "Evasion of Security AI Agents (L6)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML01:2023 Input Manipulation Attack"] } ],
                    "subTechniques": [
                        { "id": "AID-H-001.001", "name": "Adversarial Training for Evasion Attack Defense", "description": "Focuses on training models to resist adversarial inputs designed to cause misclassification during inference. This subTechnique involves generating adversarial examples and incorporating them into the training process to improve the model's robustness against evasion attacks.", "implementationStrategies": [ "Generate adversarial examples using methods like Fast Gradient Sign Method (FGSM), Projected Gradient Descent (PGD), or Carlini & Wagner (C&W).", "Augment the training dataset with these adversarial examples, ensuring a balance between clean and adversarial data.", "Use techniques like adversarial logit pairing to align the model's outputs on clean and adversarial inputs.", "Regularly evaluate the model's robustness using unseen adversarial examples to ensure generalization.", "Consider trade-offs between robustness and accuracy, adjusting the adversarial training strength (e.g., perturbation budget ε) accordingly." ], "toolsOpenSource": [ "Adversarial Robustness Toolbox (ART) by IBM", "Foolbox", "CleverHans" ], "toolsCommercial": [ "Robust Intelligence", "HiddenLayer MLSec", "Bosch AIShield" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": ["AML.T0015 Evade ML Model"] }, { "framework": "MAESTRO", "items": ["Adversarial Examples (L1)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML01:2023 Input Manipulation Attack"] } ] },
                        { "id": "AID-H-001.002", "name": "Adversarial Training for Poisoning Attack Defense", "description": "Focuses on making models resilient to poisoned training data that could introduce backdoors or degrade performance. This subTechnique involves training models to detect and mitigate the effects of poisoned data points.", "implementationStrategies": [ "Use data sanitization techniques to identify and remove poisoned samples before training.", "Implement robust training methods, such as differentially private training or anomaly detection during training.", "Employ ensemble methods to reduce the impact of poisoned data on individual models.", "Monitor model performance and behavior for signs of backdoor activation or performance degradation.", "Regularly audit training data sources and provenance to prevent poisoning at the source." ], "toolsOpenSource": [ "TensorFlow Privacy", "PyTorch Opacus", "Alibi Detect (for anomaly detection)" ], "toolsCommercial": [ "Robust Intelligence", "HiddenLayer MLSec" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": ["AML.T0020 Poison Training Data", "AML.T0018 Backdoor ML Model"] }, { "framework": "MAESTRO", "items": ["Data Poisoning (L2)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM04:2025 Data and Model Poisoning"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML02:2023 Data Poisoning Attack"] } ] },
                        { "id": "AID-H-001.003", "name": "Ensemble Methods for Robust Architectures", "description": "Utilizes multiple models to improve robustness by aggregating their predictions. This subTechnique leverages the diversity of models to reduce the likelihood of successful adversarial attacks.", "implementationStrategies": [ "Train multiple models with different architectures, initializations, or subsets of data.", "Use voting mechanisms (e.g., majority vote, weighted average) to combine predictions.", "Implement adversarial training on individual models within the ensemble for added robustness.", "Regularly evaluate the ensemble's performance against adversarial examples to ensure effectiveness.", "Consider computational costs and latency impacts when deploying ensembles in production." ], "toolsOpenSource": [ "Scikit-learn (for ensemble methods)", "TensorFlow/Keras (for model stacking)", "PyTorch (for custom ensemble implementations)" ], "toolsCommercial": [ "Microsoft Counterfit", "Robust Intelligence" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": ["AML.T0015 Evade ML Model"] }, { "framework": "MAESTRO", "items": ["Adversarial Examples (L1)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML01:2023 Input Manipulation Attack"] } ] },
                        { "id": "AID-H-001.004", "name": "Certified Defenses for Robust Architectures", "description": "Implements models with provable guarantees against certain types of adversarial perturbations. This subTechnique focuses on designing architectures that can provide certified robustness within a defined perturbation radius.", "perfImpact": { "level": "Very High", "description": "Note: Performance Impact: Very High (on Training Time & Model Complexity). This technique requires significantly more complex training procedures and simpler model architectures amenable to formal verification. Training Time: Can be >20x slower than standard training due to the overhead of propagating bounds or performing the necessary calculations for certification at each step. Accuracy: Often results in a noticeable drop in model accuracy on clean, non-adversarial data, which is a key performance trade-off." }, "implementationStrategies": [ "Use certified defense methods like interval bound propagation (IBP) or randomized smoothing.", "Define a certified robustness radius (e.g., L2 norm of 0.5) and ensure the model meets this criterion.", "Integrate certified defenses into the model training pipeline, adjusting hyperparameters to balance robustness and accuracy.", "Validate the certified robustness through adversarial testing and formal verification methods.", "Consider the computational overhead and scalability of certified defenses for large models." ], "toolsOpenSource": [ "Certified Robustness libraries (e.g., DiffAI, ERAN)", "PyTorch/TensorFlow with custom implementations" ], "toolsCommercial": [ "Adversa AI", "Bosch AIShield" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": ["AML.T0015 Evade ML Model"] }, { "framework": "MAESTRO", "items": ["Adversarial Examples (L1)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML01:2023 Input Manipulation Attack"] } ] }
                    ]
                },
                {
                    "id": "AID-H-002", "name": "AI-Contextualized Data Sanitization & Input Validation", "description": "Implement rigorous validation, sanitization, and filtering mechanisms for all data fed into AI systems. This applies to training data, fine-tuning data, and live operational inputs (including user prompts for LLMs). The goal is to detect and remove or neutralize malicious content, anomalous data, out-of-distribution samples, or inputs structured to exploit vulnerabilities like prompt injection or data poisoning before they can adversely affect the model or downstream systems. For LLMs, this involves specific techniques like stripping or encoding control tokens and filtering for known injection patterns or harmful content. For multimodal systems, this includes validating and sanitizing inputs across all modalities (e.g., text, image, audio, video) and ensuring that inputs in one modality cannot be readily used to trigger vulnerabilities, bypass controls, or inject malicious content into another modality processing pathway.", "toolsOpenSource": [ "Rebuff", "TensorFlow Data Validation", "Great Expectations", "LangChain Guardrails", "LlamaFirewall", "NVIDIA NeMo Guardrails", "Pydantic" ], "toolsCommercial": [ "OpenAI Moderation API, Google Perspective API", "CalypsoAI Validator", "Securiti LLM Firewall for Prompts", "WAFs with AI/LLM rulesets", "Data quality platforms" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": ["AML.T0020 Poison Training Data", "AML.T0051 LLM Prompt Injection", "AML.T0070 RAG Poisoning", "AML.T0054 LLM Jailbreak"] }, { "framework": "MAESTRO", "items": ["Data Poisoning (L2)", "Input Validation Attacks (L3)", "Compromised RAG Pipelines (L2)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM01:2025 Prompt Injection", "LLM04:2025 Data and Model Poisoning", "LLM08:2025 Vector and Embedding Weaknesses"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML01:2023 Input Manipulation Attack", "ML02:2023 Data Poisoning Attack"] } ],
                    "subTechniques": [
                        { "id": "AID-H-002.001", "name": "Training & Fine-Tuning Data Sanitization", "description": "Focuses on detecting and removing poisoned samples, unwanted biases, or sensitive data from datasets before they are used for model training or fine-tuning. This pre-processing step is critical for preventing the model from learning vulnerabilities or undesirable behaviors from the outset.", "implementationStrategies": [ "Perform exploratory data analysis (EDA) to understand data distributions and identify outliers.", "Use automated data validation tools to check data against a defined schema and constraints.", "Employ anomaly detection models to identify and quarantine data points that are statistically different from the rest of the dataset.", "Scan for and remove personally identifiable information (PII) or other sensitive data.", "Verify the integrity and source of any third-party or public datasets used for training." ] },
                        { "id": "AID-H-002.002", "name": "Inference-Time Prompt & Input Validation", "description": "Focuses on real-time defense against malicious inputs at the point of inference, such as prompt injection, jailbreaking attempts, or other input-based evasions. This technique acts as a guardrail for the live, operational model.", "implementationStrategies": [ "Apply strict input validation and type checking on all user-provided data.", "Sanitize LLM prompts by stripping or encoding control tokens, escape characters, and known malicious patterns.", "Use a secondary, smaller 'guardrail' model to inspect prompts for harmful intent or policy violations before they are sent to the primary model.", "Implement heuristic-based filters and regex to block known injection sequences.", "Re-prompting or instruction-based defenses where the model is explicitly told how to handle user input safely." ] },
                        { "id": "AID-H-002.003", "name": "Multimodal Input Sanitization", "description": "Focuses on the unique challenges of validating and sanitizing non-textual inputs like images, audio, and video. This includes checks for adversarial perturbations specific to these modalities and ensuring consistency across them.", "implementationStrategies": [ "For images, strip potentially malicious EXIF metadata, use defensive transformations (e.g., JPEG compression, blurring), and employ deepfake or adversarial patch detection models.", "For audio, filter for hidden commands, adversarial noise, or steganographically embedded data.", "For all modalities, normalize inputs to a consistent format and distribution.", "Implement cross-modal consistency checks to ensure that information presented in different modalities does not conflict in a way that suggests manipulation (e.g., an image of a cat paired with text that attempts to inject a prompt about a dog)." ] }
                    ]
                },
                {
                    "id": "AID-H-003", "name": "Secure ML Supply Chain Management", "description": "Apply rigorous software supply chain security principles throughout the AI/ML development and operational lifecycle. This involves verifying the integrity, authenticity, and security of all components, including source code, pre-trained models, datasets, ML libraries, development tools, and deployment infrastructure. The aim is to prevent the introduction of vulnerabilities, backdoors, malicious code (e.g., via compromised dependencies), or tampered artifacts into the AI system. This is critical as AI systems often rely on a complex ecosystem of third-party elements.", "toolsOpenSource": [ "Trivy, Syft, Grype (for SCA)", "Sigstore, in-toto (for signing and attestations)", "OWASP Dependency-Check", "pip-audit", "MLflow Model Registry", "Hugging Face Hub (with security features)", "DVC (Data Version Control)", "LakeFS", "Great Expectations (for data validation)", "Microsoft Presidio (for PII scanning)", "Open-source secure boot implementations (e.g., U-Boot)", "Firmware analysis tools (e.g., binwalk)", "Intel SGX SDK, Open Enclave SDK" ], "toolsCommercial": [ "Snyk", "Mend (formerly WhiteSource)", "JFrog Xray", "Veracode SCA", "Checkmarx SCA", "Protect AI Platform (ModelScan)", "Databricks Model Registry", "Amazon SageMaker Model Registry", "Google Vertex AI Model Registry", "Gretel.ai", "Databricks Unity Catalog", "Alation, Collibra (for data governance and lineage)", "NVIDIA Confidential Computing", "Azure Confidential Computing", "Google Cloud Confidential Computing", "Hardware Security Modules (HSMs)" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0010.000: AI Supply Chain Compromise: Hardware", "AML.T0010.001: AI Supply Chain Compromise: AI Software", "AML.T0010.002: AI Supply Chain Compromise: Data", "AML.T0010.003: AI Supply Chain Compromise: Model", "AML.T0011.001: User Execution: Malicious Package", "AML.T0019: Publish Poisoned Datasets", "AML.T0058: Publish Poisoned Models", "AML.T0076: Corrupt AI Model" ] }, { "framework": "MAESTRO", "items": [ "Compromised Framework Components (L3)", "Compromised Container Images (L4)", "Supply Chain Attacks (Cross-Layer)", "Model Tampering (L1)", "Backdoor Attacks (L1)", "Data Poisoning (L2)", "Compromised RAG Pipelines (L2)", "Physical Tampering (L4)", "Side-Channel Attacks (L4)", "Compromised Hardware Accelerators (L4)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM03:2025 Supply Chain", "LLM04:2025 Data and Model Poisoning" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML02:2023 Data Poisoning Attack", "ML06:2023 AI Supply Chain Attacks", "ML07:2023 Transfer Learning Attack", "ML10:2023 Model Poisoning" ] } ],
                    "subTechniques": [
                        { "id": "AID-H-003.001", "name": "Software Dependency & Package Security", "description": "Ensure integrity of all third-party code and libraries (Python packages, containers, build tools) used to develop and serve AI workloads.", "implementationStrategies": [ "Run SCA scanners (Syft/Grype, Trivy) on every build pipeline.", "Pin exact versions & hashes in requirements/lock files; block implicit upgrades.", "Sign artifacts with Sigstore cosign + in-toto link metadata.", "Fail the build if a dependency is yanked or contains critical CVEs." ], "toolsOpenSource": [ "Trivy, Syft, Grype (for SCA)", "Sigstore, in-toto (for signing and attestations)", "OWASP Dependency-Check", "pip-audit" ], "toolsCommercial": [ "Snyk", "Mend (formerly WhiteSource)", "JFrog Xray", "Veracode SCA", "Checkmarx SCA" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0010.001: AI Supply Chain Compromise: AI Software" , "AML.T0011.001: User Execution: Malicious Package" ] }, { "framework": "MAESTRO", "items": [ "Compromised Framework Components (L3)", "Compromised Container Images (L4)", "Supply Chain Attacks (Cross-Layer)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM03:2025 Supply Chain (Traditional Third-party Package Vulnerabilities)" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML06:2023 AI Supply Chain Attacks" ] } ] },
                        { "id": "AID-H-003.002", "name": "Model Artifact Verification & Secure Distribution", "description": "Protect pre-trained or fine-tuned model binaries, weights and checkpoints from tampering in transit or at rest.", "implementationStrategies": [ "Store models in an internal registry; require SHA-256 or Sigstore signatures before promotion to prod.", "Use reproducible model packaging (e.g., MLflow model version pinning) and verify on deploy.", "Serve models over mTLS; enforce content-hash pinning at the inference layer." ], "toolsOpenSource": [ "MLflow Model Registry", "Hugging Face Hub (with security features)", "Sigstore/cosign (for signing model files)" ], "toolsCommercial": [ "Protect AI Platform (ModelScan)", "Databricks Model Registry", "Amazon SageMaker Model Registry", "Google Vertex AI Model Registry" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0010.003: AI Supply Chain Compromise: Model" , "AML.T0058: Publish Poisoned Models" , "AML.T0076: Corrupt AI Model" ] }, { "framework": "MAESTRO", "items": [ "Model Tampering (L1)", "Backdoor Attacks (L1)", "Supply Chain Attacks (Cross-Layer)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM03:2025 Supply Chain (Vulnerable Pre-Trained Model)", "LLM04:2025 Data and Model Poisoning" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML06:2023 AI Supply Chain Attacks", "ML10:2023 Model Poisoning" ] } ] },
                        { "id": "AID-H-003.003", "name": "Dataset Supply Chain Validation", "description": "Authenticate, checksum and licence-check every external dataset (training, fine-tuning, RAG).", "implementationStrategies": [ "Maintain per-file hashes in DVC/LakeFS; block pipeline if hash drift.", "Run licence & PII scanners (Gretel, Presidio) before datasets enter feature store.", "Embed signed provenance metadata (‘datasheets for datasets’) for auditing." ], "toolsOpenSource": [ "DVC (Data Version Control)", "LakeFS", "Great Expectations (for data validation)", "Microsoft Presidio (for PII scanning)" ], "toolsCommercial": [ "Gretel.ai", "Databricks Unity Catalog", "Alation, Collibra (for data governance and lineage)" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0010.002: AI Supply Chain Compromise: Data" , "AML.T0019: Publish Poisoned Datasets" ] }, { "framework": "MAESTRO", "items": [ "Data Poisoning (L2)", "Compromised RAG Pipelines (L2)", "Supply Chain Attacks (Cross-Layer)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM03:2025 Supply Chain (Outdated or Deprecated Models/Datasets)", "LLM04:2025 Data and Model Poisoning" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML02:2023 Data Poisoning Attack" , "ML07:2023 Transfer Learning Attack" ] } ] },
                        { "id": "AID-H-003.004", "name": "Hardware & Firmware Integrity Assurance", "description": "Verify accelerator cards, firmware and BIOS/UEFI images are genuine and un-modified before joining an AI cluster.", "implementationStrategies": [ "Secure-boot GPUs/TPUs; attestation via TPM/CCA or NVIDIA Confidential Computing.", "Continuously monitor firmware versions and revoke out-of-policy images.", "Run side-channel/fault-injection self-tests during maintenance windows." ], "toolsOpenSource": [ "Open-source secure boot implementations (e.g., U-Boot)", "Firmware analysis tools (e.g., binwalk)", "Intel SGX SDK, Open Enclave SDK" ], "toolsCommercial": [ "NVIDIA Confidential Computing", "Azure Confidential Computing", "Google Cloud Confidential Computing", "Hardware Security Modules (HSMs)" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0010.000: AI Supply Chain Compromise: Hardware" ] }, { "framework": "MAESTRO", "items": [ "Physical Tampering (L4)", "Side-Channel Attacks (L4)", "Compromised Hardware Accelerators (L4)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM03:2025 Supply Chain (LLM Model on Device supply-chain vulnerabilities)" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML06:2023 AI Supply Chain Attacks" ] } ] }
                    ]
                },
                {
                    "id": "AID-H-004", "name": "Identity & Access Management (IAM) for AI Systems", "description": "Implement and enforce comprehensive Identity and Access Management (IAM) controls for all AI resources, including models, APIs, data stores, agentic tools, and administrative interfaces. This involves applying the principle of least privilege, strong authentication, and robust authorization to limit who and what can interact with, modify, or manage AI systems.", "toolsOpenSource": [ "Keycloak", "FreeIPA", "OpenUnison", "HashiCorp Boundary", "OAuth2-Proxy", "SPIFFE/SPIRE (for service identity)", "Istio, Linkerd (for mTLS)", "Libraries for JWT or PASETO for message signing", "gRPC with TLS authentication" ], "toolsCommercial": [ "Okta, Ping Identity, Auth0 (IDaaS)", "CyberArk, Delinea, BeyondTrust (PAM)", "Cloud Provider IAM (AWS IAM, Azure AD, Google Cloud IAM)", "API Gateways (Kong, Apigee, MuleSoft)", "Cloud Provider Secret Managers (AWS Secrets Manager, Azure Key Vault)", "Enterprise Service Mesh solutions"], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0012: Valid Accounts", "AML.T0040: AI Model Inference API Access" ] }, { "framework": "MAESTRO", "items": [ "Agent Identity Attack (L7)", "Compromised Agent Registry (L7)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM02:2025 Sensitive Information Disclosure", "LLM03:2025 Supply Chain", "LLM06:2025 Excessive Agency" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML05:2023 Model Theft" ] } ],
                    "subTechniques": [
                        { "id": "AID-H-004.001", "name": "User & Privileged Access Management", "description": "Focuses on securing access for human users, such as developers, data scientists, and system administrators, who manage and interact with AI systems. The goal is to enforce strong authentication and granular permissions for human identities.", "implementationStrategies": [ "Enforce Multi-Factor Authentication (MFA) for all users accessing sensitive AI environments.", "Implement Role-Based Access Control (RBAC) to grant permissions based on job function.", "Utilize Privileged Access Management (PAM) solutions for administrators to control and audit high-risk actions.", "Conduct regular access reviews and promptly de-provision inactive accounts." ], "toolsOpenSource": [ "Keycloak", "FreeIPA", "OpenUnison", "HashiCorp Boundary" ], "toolsCommercial": [ "Okta, Ping Identity, Auth0 (IDaaS)", "CyberArk, Delinea, BeyondTrust (PAM)", "Cloud Provider IAM (AWS IAM, Azure AD, Google Cloud IAM)" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0012: Valid Accounts" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM02:2025 Sensitive Information Disclosure" ] } ] },
                        { "id": "AID-H-004.002", "name": "Service & API Authentication", "description": "Focuses on securing machine-to-machine communication for AI services. This includes authenticating service accounts, applications, and other services that need to interact with AI model APIs, data stores, or MLOps pipelines.", "implementationStrategies": [ "Use OAuth 2.0 client credentials flow for service-to-service authentication.", "Implement short-lived, securely managed API keys for external service access.", "Enforce mutual TLS (mTLS) for all internal API traffic to ensure both client and server are authenticated.", "Use cloud provider IAM roles (e.g., AWS IAM Roles for Service Accounts) for workloads running in the cloud." ], "toolsOpenSource": [ "OAuth2-Proxy", "SPIFFE/SPIRE (for service identity)", "Istio, Linkerd (for mTLS)" ], "toolsCommercial": [ "API Gateways (Kong, Apigee, MuleSoft)", "Cloud Provider Secret Managers (AWS Secrets Manager, Azure Key Vault)" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0040: AI Model Inference API Access" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM03:2025 Supply Chain" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML05:2023 Model Theft" ] } ] },
                        { "id": "AID-H-004.003", "name": "Secure Agent-to-Agent Communication", "description": "Focuses on the unique challenge of securing communications within multi-agent systems. This ensures that autonomous agents can trust each other, and that their messages cannot be spoofed, tampered with, or replayed by an adversary.", "implementationStrategies": [ "Issue unique, cryptographically verifiable identities to each AI agent (e.g., using SPIFFE/SPIRE).", "Digitally sign every inter-agent message to ensure integrity and non-repudiation.", "Encrypt all agent-to-agent communication channels (e.g., via TLS).", "Include sequence numbers or timestamps in messages to prevent replay attacks." ], "toolsOpenSource": [ "SPIFFE/SPIRE", "Libraries for JWT or PASETO for message signing", "gRPC with TLS authentication" ], "toolsCommercial": [ "Enterprise Service Mesh solutions", "Entitle AI (emerging market for agent governance)" ], "defendsAgainst": [ { "framework": "MAESTRO", "items": [ "Agent Identity Attack (L7)", "Compromised Agent Registry (L7)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM06:2025 Excessive Agency" ] } ] }
                    ]
                },
                {
                    "id": "AID-H-005", "name": "Privacy-Preserving Machine Learning (PPML) Techniques", "description": "Employ a range of advanced cryptographic and statistical techniques during AI model training, fine-tuning, and inference to protect the privacy of sensitive information within datasets. These methods aim to prevent the leakage of individual data records, membership inference, or the reconstruction of sensitive inputs from model outputs.", "toolsOpenSource": [ "PyTorch Opacus", "TensorFlow Privacy", "Google DP Library", "OpenDP", "Microsoft SEAL", "PALISADE", "HElib", "OpenFHE", "TensorFlow Federated", "PySyft", "Flower", "NVIDIA FLARE", "MP-SPDZ", "SCALE-MAMBA", "Obliv-C", "CrypTen", "SDV (Synthetic Data Vault)", "CTGAN", "TGAN" ], "toolsCommercial": [ "Gretel.ai", "Immuta", "SarUS", "Duality Technologies", "Enveil", "Zama.ai", "Owkin", "Substra Foundation", "IBM Federated Learning", "TripleBlind", "Cape Privacy", "Inpher", "Mostly AI", "Hazy", "Tonic.ai" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0024.000: Exfiltration via AI Inference API: Infer Training Data Membership", "AML.T0024.001: Exfiltration via AI Inference API: Invert AI Model", "AML.T0057: LLM Data Leakage" ] }, { "framework": "MAESTRO", "items": [ "Attacks on Decentralized Learning (Cross-Layer)", "Data Exfiltration (L2)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM02:2025 Sensitive Information Disclosure" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML03:2023 Model Inversion Attack", "ML04:2023 Membership Inference Attack", "ML07:2023 Transfer Learning Attack" ] } ],
                    "subTechniques": [
                        { "id": "AID-H-005.001", "name": "Differential Privacy for AI", "description": "Implements differential privacy mechanisms to add calibrated noise to model training, outputs, or data queries, ensuring that individual data points cannot be identified while maintaining overall utility.", "perfImpact": { "level": "High", "description": "Note: Performance Impact: High (on Training Time & Model Utility). This technique adds computational overhead to each training step by adding noise and clipping gradients. Training Time: Can slow down the training process by 2x to 5x. Accuracy: May moderately reduce the final accuracy of the model, which is a direct trade-off for the privacy guarantees provided." }, "implementationStrategies": [ "Implement Differentially Private Stochastic Gradient Descent (DP-SGD) for model training.", "Apply output perturbation techniques adding calibrated noise to model predictions.", "Manage privacy budget (epsilon, delta) across multiple queries or training epochs.", "Use gradient clipping and noise addition to bound sensitivity of updates.", "Implement local differential privacy for distributed data collection scenarios." ], "toolsOpenSource": ["PyTorch Opacus", "TensorFlow Privacy", "Google DP Library", "OpenDP"], "toolsCommercial": ["Gretel.ai", "Immuta", "SarUS"], "defendsAgainst": [{"framework": "MITRE ATLAS", "items": ["AML.T0024.000: Exfiltration via AI Inference API: Infer Training Data Membership"]}, {"framework": "OWASP LLM Top 10 2025", "items": ["LLM02:2025 Sensitive Information Disclosure"]}, {"framework": "OWASP ML Top 10 2023", "items": ["ML04:2023 Membership Inference Attack"]}] },
                        { "id": "AID-H-005.002", "name": "Homomorphic Encryption for AI", "description": "Enables computation on encrypted data, allowing models to train or perform inference without ever decrypting sensitive information, providing strong cryptographic guarantees.", "perfImpact": { "level": "Extreme", "description": "Note: Performance Impact: Extreme (on Inference Latency & Computational Cost). Performing computations on encrypted data is orders of magnitude slower than on plaintext. Inference Latency: Can be 100x to 10,000x higher than a non-encrypted model. A prediction that takes milliseconds on plaintext could take many seconds or even minutes. Training: Training a model with HE is often computationally prohibitive for all but the simplest models." }, "implementationStrategies": [ "Implement fully homomorphic encryption (FHE) schemes for simple model architectures.", "Use partially homomorphic encryption for specific operations (addition, multiplication).", "Apply leveled homomorphic encryption for known-depth computations.", "Optimize encryption parameters for the specific ML workload to balance security and performance.", "Implement hybrid approaches combining HE with secure enclaves for complex operations." ], "toolsOpenSource": ["Microsoft SEAL", "PALISADE", "HElib", "OpenFHE"], "toolsCommercial": ["Duality Technologies", "Enveil", "Zama.ai"], "defendsAgainst": [{"framework": "OWASP LLM Top 10 2025", "items": ["LLM02:2025 Sensitive Information Disclosure"]}, {"framework": "OWASP ML Top 10 2023", "items": ["ML03:2023 Model Inversion Attack"]}] }
                    ]
                },
                { "id": "AID-H-006", "name": "Secure AI System Design & Zero Trust Architecture Integration", "description": "Embed security principles deeply into the architecture and design of AI systems from their inception (\"secure by design\"). This includes applying Zero Trust Architecture (ZTA) principles, which operate on the premise of \"never trust, always verify.\" Every access request to or from any AI component, data store, API, or service must be explicitly verified, regardless of whether the source is internal or external to the network. This approach minimizes implicit trust zones and helps contain breaches. It also encompasses planning for secure model updates, patching, and eventual decommissioning of AI systems and artifacts.", "implementationStrategies": ["Incorporate security into all MLOps phases.", "Conduct AI-specific threat modeling early.", "Evaluate security of third-party components.", "Zero Trust: Strong authentication/authorization for all entities.", "Microsegmentation to isolate AI workloads and data.", "Enforce least privilege access.", "Continuously monitor and validate security posture.", "Protect data at rest, in transit, and in use (PPML).", "Establish secure processes for updates, patching, decommissioning.", "Consider separating agent control and execution planes."], "toolsOpenSource": ["Guidance: NIST SP 800-207 (ZTA)", "Microsegmentation: Kubernetes Network Policies, Calico, Cilium, Istio", "IAM: Keycloak, Open Policy Agent (OPA)"], "toolsCommercial": ["ZTNA solutions (Zscaler, Prisma Access, Cisco Secure Access)", "Microsegmentation platforms (Illumio, Guardicore, Cisco Secure Workload)", "PAM solutions", "Cloud provider ZT services (AWS IAM, Azure AD Conditional Access, Google BeyondCorp)"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["Broadly mitigates: AML.T0011 Initial Access, AML.T0010 Privilege Escalation, Lateral Movement principles, AML.T0008 ML Supply Chain Compromise"] }, { "framework": "MAESTRO", "items": ["Addresses threats across all layers by strict verification, segmentation, least privilege. Secures L4, L6, and mitigates Cross-Layer threats."] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM06:2025 Excessive Agency", "LLM03:2025 Supply Chain", "LLM02:2025 Sensitive Information Disclosure"] }, { "framework": "OWASP ML Top 10 2023", "items": ["Reduces overall attack surface; ML05:2023 Model Theft becomes harder."] }] },
                { "id": "AID-H-007", "name": "Secure & Resilient Training Process Hardening", "description": "Implement robust security measures to protect the integrity, confidentiality, and stability of the AI model training process itself. This involves securing the training environment (infrastructure, code, data access), continuously monitoring training jobs for anomalous behavior (e.g., unexpected resource consumption, convergence failures, unusual metric fluctuations that could indicate subtle data poisoning effects not caught by pre-filtering or direct manipulation of training code), and ensuring training reproducibility and auditability.", "implementationStrategies": ["Utilize dedicated, isolated, and hardened environments for model training activities, distinct from development or production serving environments.", "Apply the principle of least privilege for training jobs, granting only necessary access to data, code repositories, and computational resources.", "Monitor training metrics (e.g., loss functions, accuracy, gradient norms, learning rates) in real-time for unexpected spikes, drops, or patterns inconsistent with established training profiles for similar models/data.", "Implement automated checks for training stability, convergence within expected epochs, and prevention of issues like gradient starvation or explosion.", "Strictly version control all training code, configurations, dependencies (e.g., via requirements.txt or environment.yml), and environment snapshots (e.g., container images).", "Employ confidential computing (e.g., secure enclaves) for the training process when the training algorithm, intermediate model states, or proprietary data processing steps are highly sensitive.", "Log all training job parameters, code versions, data versions, and resulting metrics to ensure reproducibility and facilitate audits."], "toolsOpenSource": ["MLOps platforms with experiment tracking and monitoring (e.g., MLflow, Kubeflow, ClearML).", "Containerization technologies (e.g., Docker, Podman) for creating reproducible training environments.", "Infrastructure monitoring tools (e.g., Prometheus, Grafana) adapted for training job metrics.", "Confidential Computing SDKs (e.g., Intel SGX SDK, Open Enclave SDK).", "Version control systems (e.g., Git, DVC)."], "toolsCommercial": ["Cloud-based MLOps platforms (e.g., Amazon SageMaker, Google Vertex AI, Azure Machine Learning) with built-in training monitoring, security features, and experiment management.", "Commercial confidential computing services from cloud providers.", "Specialized AI training security monitoring solutions."], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0020 Poison Training Data (by detecting anomalous training dynamics caused by subtle poisoning)", "AML.T0019 Poison ML Model (if poisoning occurs through manipulation of the training process, code, or environment rather than just static model parameters)", "AML.T0008 ML Supply Chain Compromise (if a compromised development tool or library specifically targets and manipulates the training loop)."] }, { "framework": "MAESTRO", "items": ["Data Poisoning (L2: Data Operations, by monitoring its impact during training)", "Compromised Training Environment (L4: Deployment & Infrastructure)", "Resource Hijacking (L4: Deployment & & Infrastructure, if training resources are targeted by malware or unauthorized processes)", "Training Algorithm Manipulation (L1: Foundation Models or L3: Agent Frameworks)."] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM04:2025 Data and Model Poisoning (by providing an additional layer to detect sophisticated poisoning attempts that manifest during the training process itself)."] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML02:2023 Data Poisoning Attack (detecting subtle or run-time effects)", "ML10:2023 Model Poisoning (if poisoning involves altering training code or runtime)."] }] },
                { "id": "AID-H-008", "name": "Robust Federated Learning Aggregation", "description": "Implement and enforce secure aggregation protocols and defenses against malicious or unreliable client updates within Federated Learning (FL) architectures. This technique aims to prevent attackers controlling a subset of participating clients from disproportionately influencing, poisoning, or degrading the global model, or inferring information about other clients' data.", "implementationStrategies": ["Employ secure aggregation schemes (e.g., those based on homomorphic encryption or secure multi-party computation) that allow the server to compute the sum of client updates without seeing individual contributions.", "Utilize robust aggregation rules (e.g., median, trimmed mean, Krum, Multi-Krum, FoolsGold, Byzantine-robust Stochastic Gradient Descent variants) designed to identify and mitigate the impact of outlier or malicious model updates from compromised clients.", "Implement client-side validation checks or anomaly detection on model updates before they are sent to the aggregation server.", "Monitor the statistical properties of updates received from clients over time to detect consistently anomalous or suspicious contributions.", "Consider client reputation systems or differential weighting based on historical contribution quality or trust levels in long-running FL systems.", "Combine with differential privacy applied to client updates before aggregation to further protect individual client data."], "toolsOpenSource": ["TensorFlow Federated (TFF) (supports some secure aggregation algorithms).", "PySyft (OpenMined) (focuses on privacy-preserving ML, including FL with secure computation).", "Flower (framework adaptable for various FL strategies, including custom robust aggregation).", "NVIDIA FLARE (Federated Learning Application Runtime Environment).", "Libraries for robust statistics (e.g., parts of SciPy, specialized research libraries)."], "toolsCommercial": ["Enterprise Federated Learning platforms (e.g., from Owkin, Substra Foundation, IBM) may offer built-in options for robust and secure aggregation.", "Solutions leveraging confidential computing for parts of the FL aggregation process."], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0020 Poison Training Data (specifically in the context of federated learning where malicious clients submit poisoned updates)", "AML.T0019 Poison ML Model (where the global model is poisoned via aggregation of malicious client models)."] }, { "framework": "MAESTRO", "items": ["Data Poisoning (L2: Data Operations, within FL setups)", "Model Skewing (L2: Data Operations, in FL)", "Attacks on Decentralized Learning (Cross-Layer)", "Inference Attacks against FL participants (if secure aggregation also provides confidentiality)."] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM04:2025 Data and Model Poisoning (especially relevant for distributed or federated fine-tuning/training of LLMs)."] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML02:2023 Data Poisoning Attack (specifically in FL)", "ML10:2023 Model Poisoning (via compromised clients in FL)."] }] },
                { "id": "AID-H-009", "name": "AI Accelerator & Hardware Integrity", "description": "Implement measures to protect the physical integrity and operational security of specialized AI hardware (GPUs, TPUs, NPUs, FPGAs) and the platforms hosting them against physical tampering, side-channel attacks (power, timing, EM), fault injection, and hardware Trojans. This aims to ensure the confidentiality and integrity of AI computations and model parameters processed by the hardware.", "implementationStrategies": ["Utilize secure boot processes for AI accelerators and host systems.", "Employ hardware-based countermeasures against known side-channel attacks (e.g., noise injection, power supply randomization, shielded enclosures for critical components).", "Implement fault detection and response mechanisms in AI hardware and firmware.", "Source AI hardware from trusted supply chains and conduct physical inspections where feasible.", "Leverage Physical Unclonable Functions (PUFs) for device authentication and cryptographic key generation.", "Utilize secure enclaves/confidential computing for sensitive AI computations on accelerators where supported.", "Regularly update firmware for AI accelerators and supporting hardware components from verified sources.", "Monitor physical environmental conditions (temperature, voltage) around AI hardware for anomalies that could facilitate attacks."], "toolsOpenSource": ["Open-source secure boot implementations (e.g., U-Boot with secure boot features).", "Firmware analysis tools (e.g., binwalk, firmadyne for inspection).", "Research tools for side-channel analysis and countermeasures."], "toolsCommercial": ["Hardware Security Modules (HSMs) for key management related to AI hardware.", "Commercial confidential computing platforms that extend to accelerators (e.g., NVIDIA Confidential Computing).", "Specialized hardware integrity verification services.", "Vendor-specific security features in AI accelerators (e.g., secure enclaves within GPUs/TPUs)."], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0010.000 AI Supply Chain Compromise: Hardware", "AML.T0024.002 Extract ML Model (if extraction relies on side-channel attacks against hardware)", "AML.T0025 Exfiltration via Cyber Means (if side-channels are the means). (Potentially new ATLAS technique: \"Exploit AI Hardware Vulnerability\" or \"AI Hardware Side-Channel Attack\")."] }, { "framework": "MAESTRO", "items": ["Physical Tampering (L4: Deployment & Infrastructure)", "Side-Channel Attacks (L4: Deployment & Infrastructure / L1: Foundation Models if model parameters are leaked)", "Compromised Hardware Accelerators (L4)."] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM03:2025 Supply Chain (by ensuring integrity of underlying hardware components)", "LLM02:2025 Sensitive Information Disclosure (if disclosure is via hardware side-channels)."] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML05:2023 Model Theft (if theft is facilitated by hardware-level attacks)", "ML06:2023 AI Supply Chain Attacks (specifically hardware components)."] }] },
                { "id": "AID-H-010", "name": "Transformer Architecture Defenses", "description": "Implement security measures specifically designed to mitigate vulnerabilities inherent in the Transformer architecture, such as attention mechanism manipulation, position embedding attacks, and risks associated with self-attention complexity. These defenses aim to protect against attacks that exploit how Transformers process and prioritize information.", "implementationStrategies": [ "Utilize methods to secure attention mechanisms, such as using robust attention scoring or adding noise to attention weights to reduce susceptibility to manipulation.", "Implement position embedding hardening techniques to prevent attackers from manipulating input sequence order to alter model outputs.", "Apply regularization techniques on attention distributions to prevent sparse, high-confidence attention on malicious tokens.", "Monitor for and detect anomalous attention patterns that deviate from established baselines for given tasks.", "Employ architectural variations like gated attention or sparse attention patterns that are inherently more robust to certain attacks." ], "toolsOpenSource": [ "TextAttack (for generating adversarial examples against Transformers)", "Libraries for implementing custom attention mechanisms (PyTorch, TensorFlow)", "Research code from academic papers on Transformer security." ], "toolsCommercial": [ "AI security platforms offering model-specific vulnerability scanning.", "Adversarial attack simulation tools with profiles for Transformer models." ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": ["AML.T0015 Evade ML Model", "AML.T0043 Craft Adversarial Data"] }, { "framework": "MAESTRO", "items": ["Adversarial Examples (L1)", "Reprogramming Attacks (L1)", "Input Validation Attacks (L3)", "Framework Evasion (L3)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML01:2023 Input Manipulation Attack"] } ] },
                { "id": "AID-H-011", "name": "Diffusion Model Attack Mitigation", "description": "Deploy defenses to counter attacks targeting generative diffusion models. These include preventing the generation of harmful or copyrighted content, detecting adversarial noise that hijacks the generation process, and mitigating attacks that aim to extract training data from the model's outputs.", "implementationStrategies": [ "Implement robust concept filtering and output sanitization to block the generation of unsafe or restricted content.", "Utilize watermarking techniques for generated outputs to trace model origin and identify synthetic media (see AID-DV-004).", "Employ differential privacy during training to reduce the risk of training data extraction.", "Detect and filter adversarial noise in the initial latent space or during the reverse diffusion process.", "Fine-tune models with alignment techniques to steer them away from generating problematic content." ], "toolsOpenSource": [ "Diffusers library (Hugging Face) with safety checkers", "Google's SynthID for watermarking", "Libraries for differential privacy (Opacus, TensorFlow Privacy)" ], "toolsCommercial": [ "Content moderation APIs that can scan generated images/media.", "AI safety platforms offering generative model protection." ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": ["AML.T0048.002 External Harms: Societal Harm", "AML.T0024.001 Invert ML Model", "AML.T0048.004 External Harms: AI Intellectual Property Theft"] }, { "framework": "MAESTRO", "items": ["Data Exfiltration (L2)", "Model Inversion/Extraction (L2)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM09:2025 Misinformation", "LLM02:2025 Sensitive Information Disclosure"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML09:2023 Output Integrity Attack", "ML03:2023 Model Inversion Attack"] } ] },
                { "id": "AID-H-012", "name": "Graph Neural Network (GNN) Poisoning Defense", "description": "Implement defenses to secure Graph Neural Networks (GNNs) against data poisoning attacks that manipulate the graph structure (nodes, edges) or node features. The goal is to ensure the integrity of the graph data and the robustness of the GNN's predictions against malicious alterations.", "implementationStrategies": [ "Apply graph structure filtering and anomaly detection to identify and remove suspicious nodes or edges before training.", "Use robust aggregation functions in GNN layers that are less sensitive to outlier nodes or malicious neighbors.", "Implement certification methods to provide guarantees of robustness against a certain number of graph perturbations.", "Regularize the model to prevent over-reliance on a small number of influential nodes or edges.", "Analyze node and edge provenance to identify untrusted data sources." ], "toolsOpenSource": [ "PyTorch Geometric, Deep Graph Library (DGL)", "Graph-specific anomaly detection libraries.", "Research implementations of robust GNN architectures." ], "toolsCommercial": [ "Graph database platforms with built-in data quality and integrity checks.", "AI security platforms with GNN-specific threat detection." ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": ["AML.T0020 Poison Training Data"] }, { "framework": "MAESTRO", "items": ["Data Poisoning (L2)", "Data Tampering (L2)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM04:2025 Data and Model Poisoning"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML02:2023 Data Poisoning Attack"] } ] },
                { "id": "AID-H-013", "name": "Reinforcement Learning (RL) Reward Hacking Prevention", "description": "Design and implement safeguards to prevent Reinforcement Learning (RL) agents from discovering and exploiting flaws in the reward function to achieve high rewards for unintended or harmful behaviors ('reward hacking'). This also includes protecting the reward signal from external manipulation.", "implementationStrategies": [ "Design complex, multi-objective reward functions that are difficult to 'game' and align better with the intended outcome.", "Use inverse reinforcement learning or preference-based learning to derive more robust reward functions from human feedback.", "Implement reward shaping and potential-based reward functions to guide the agent correctly.", "Introduce constraints and penalties for undesirable behaviors or states ('guardrails').", "Monitor agent behavior for emergent, unexpected strategies that achieve high rewards and investigate them in a sandboxed environment.", "Secure the channel through which the reward signal is delivered to the agent to prevent direct manipulation." ], "toolsOpenSource": [ "RL libraries (Stable Baselines3, RLlib, Tianshou)", "Simulators and environments for testing RL agents (Gymnasium, MuJoCo).", "Research tools for safe RL exploration." ], "toolsCommercial": [ "Enterprise RL platforms (AnyLogic, Microsoft Bonsai).", "Simulation platforms for robotics and autonomous systems." ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": ["AML.T0048 External Harms"] }, { "framework": "MAESTRO", "items": ["Agent Goal Manipulation (L7)", "Compromised Agents (L7)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM06:2025 Excessive Agency"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML08:2023 Model Skewing"] } ] },
                { "id": "AID-H-014", "name": "Vision Transformer (ViT) Patch Attack Defense", "description": "Implement defenses specifically aimed at mitigating adversarial patch attacks against Vision Transformers (ViTs). These attacks involve creating a small, localized patch that can be placed anywhere in an image to cause a misclassification, making them a potent physical-world threat.", "implementationStrategies": [ "Employ input sanitization techniques that specifically look for and disrupt localized, high-variance patches that are characteristic of these attacks.", "Use models trained with data augmentation that includes random patch insertions to make the model less sensitive to such artifacts.", "Implement robust vision models or ensemble methods where different models might focus on different parts of the image, making a single patch less effective.", "Analyze the model's attention maps to detect if an excessive amount of attention is being focused on a small, non-salient region of the image.", "Utilize certifiably robust defenses that can guarantee the model's prediction is stable against patches of a certain size." ], "toolsOpenSource": [ "Adversarial attack libraries (ART, Foolbox) for generating patch attacks for testing.", "Computer vision libraries (OpenCV, Pillow) for implementing patch detection and sanitization.", "PyTorch/TensorFlow for implementing custom robust ViT architectures." ], "toolsCommercial": [ "Adversarial attack simulation platforms.", "AI security solutions focused on computer vision systems." ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": ["AML.T0015 Evade ML Model", "AML.T0043.004 Data: Insert Backdoor Trigger"] }, { "framework": "MAESTRO", "items": ["Adversarial Examples (L1)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["N/A (Primarily a vision attack)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML01:2023 Input Manipulation Attack"] } ] }
            ]
        },
        {
            "name": "Detect",
            "purpose": "The \"Detect\" tactic focuses on the timely identification of intrusions, malicious activities, anomalous behaviors, or policy violations occurring within or targeting AI systems. This involves continuous or periodic monitoring of various aspects of the AI ecosystem, including inputs (prompts, data feeds), outputs (predictions, generated content, agent actions), model behavior (performance metrics, drift), system logs (API calls, resource usage), and the integrity of AI artifacts (models, datasets).",
            "techniques": [
                {
                    "id": "AID-D-001", "name": "Adversarial Input & Prompt Injection Detection", "description": "Implement mechanisms to continuously monitor and analyze inputs to AI models, specifically looking for characteristics indicative of adversarial manipulation or malicious prompt content. This includes detecting statistically anomalous inputs (e.g., out-of-distribution samples, inputs with unusual perturbation patterns) and scanning prompts for known malicious patterns, hidden commands, jailbreak sequences, or attempts to inject executable code or harmful instructions. The goal is to block, flag, or sanitize such inputs before they can significantly impact the model's behavior or compromise the system.", "toolsOpenSource": [ "Adversarial Robustness Toolbox (ART)", "Alibi Detect", "SciPy, scikit-learn (for statistical tests and anomaly detection models)", "Rebuff", "LangChain Guardrails", "NVIDIA NeMo Guardrails", "vigil-llm", "OpenCV (for image analysis)", "Pillow (for image manipulation/detection)", "Librosa (for audio analysis)", "Steganography detection tools (e.g., zsteg)" ], "toolsCommercial": [ "Robust Intelligence", "Fiddler AI", "Arize AI", "Lakera Guard", "Protect AI Guardian", "Securiti LLM Firewall", "CalypsoAI Validator", "Hive AI (multimodal content moderation)", "Clarifai (computer vision and multimodal analysis)", "Sensity (deepfake and visual threat detection)" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0015: Evade AI Model", "AML.T0043: Craft Adversarial Data", "AML.T0051: LLM Prompt Injection", "AML.T0054: LLM Jailbreak", "AML.T0068: LLM Prompt Obfuscation" ] }, { "framework": "MAESTRO", "items": [ "Adversarial Examples (L1)", "Evasion of Security AI Agents (L6)", "Input Validation Attacks (L3)", "Reprogramming Attacks (L1)", "Cross-Modal Manipulation Attacks (L1)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM01:2025 Prompt Injection" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML01:2023 Input Manipulation Attack" ] } ],
                    "subTechniques": [
                        { "id": "AID-D-001.001", "name": "Statistical Adversarial Example Detection", "description": "Focuses on detecting traditional adversarial examples in ML models through statistical analysis of input characteristics. This includes identifying out-of-distribution samples, detecting unusual perturbation patterns, and analyzing input feature distributions that deviate from expected baselines.", "implementationStrategies": [ "Deploy statistical tests like Kullback-Leibler (KL) divergence or Maximum Mean Discrepancy (MMD) to compare input distributions against baseline normal traffic.", "Implement anomaly detection models trained on benign input distributions to identify statistical outliers.", "Use input reconstruction techniques to detect adversarial perturbations by comparing original and reconstructed inputs.", "Monitor input feature statistics (mean, variance, entropy) for anomalous patterns.", "Employ ensemble disagreement detection where multiple models' outputs are compared for consistency." ], "toolsOpenSource": [ "Adversarial Robustness Toolbox (ART)", "Alibi Detect", "SciPy, scikit-learn (for statistical tests and anomaly detection models)" ], "toolsCommercial": [ "Robust Intelligence", "Fiddler AI", "Arize AI" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0015: Evade AI Model" ] }, { "framework": "MAESTRO", "items": [ "Adversarial Examples (L1)", "Evasion of Security AI Agents (L6)" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML01:2023 Input Manipulation Attack" ] } ] },
                        { "id": "AID-D-001.002", "name": "LLM Prompt Injection & Jailbreak Detection", "description": "Specifically targets detection of prompt injection attacks, jailbreak attempts, and malicious instruction sequences aimed at Large Language Models. This includes both pattern-based and model-based detection approaches.", "implementationStrategies": [ "Deploy specialized LLM-specific content scanners that look for known injection patterns, control tokens, and jailbreak sequences.", "Utilize secondary 'guardrail' models trained to classify prompts as safe or potentially malicious.", "Implement heuristic-based filters using regex patterns for common injection techniques (e.g., role-playing, instruction overrides).", "Monitor for prompt obfuscation techniques like character substitution, encoding tricks, or linguistic manipulation.", "Analyze prompt perplexity scores and compare against typical user query distributions." ], "toolsOpenSource": [ "Rebuff", "LangChain Guardrails", "NVIDIA NeMo Guardrails", "vigil-llm" ], "toolsCommercial": [ "Lakera Guard", "Protect AI Guardian", "Securiti LLM Firewall", "CalypsoAI Validator" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0051: LLM Prompt Injection" , "AML.T0054: LLM Jailbreak" , "AML.T0068: LLM Prompt Obfuscation" ] }, { "framework": "MAESTRO", "items": [ "Input Validation Attacks (L3)", "Reprogramming Attacks (L1)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM01:2025 Prompt Injection" ] } ] },
                        { "id": "AID-D-001.003", "name": "Multimodal Attack Detection", "description": "Focuses on detecting adversarial inputs and cross-modal attacks in multimodal AI systems. This includes identifying attacks where one modality (e.g., image) is used to inject malicious content targeting another modality's processing (e.g., text).", "implementationStrategies": [ "Deploy modality-specific detection mechanisms (e.g., adversarial patch detection for images, hidden command detection for audio).", "Implement cross-modal consistency checking to ensure alignment between different input modalities.", "Scan for steganographic content or hidden payloads in non-text modalities (QR codes in images, hidden text in audio).", "Monitor attention mechanisms in multimodal models for unusual cross-modal attention patterns.", "Analyze for known cross-modal attack signatures where visual or audio inputs attempt to influence text processing." ], "toolsOpenSource": [ "OpenCV (for image analysis)", "Pillow (for image manipulation/detection)", "Librosa (for audio analysis)", "Steganography detection tools (e.g., zsteg)" ], "toolsCommercial": [ "Hive AI (multimodal content moderation)", "Clarifai (computer vision and multimodal analysis)", "Sensity (deepfake and visual threat detection)" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0043: Craft Adversarial Data" ] }, { "framework": "MAESTRO", "items": [ "Cross-Modal Manipulation Attacks (L1)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM01:2025 Prompt Injection (specifically Scenario #7: Multimodal Injection)" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML01:2023 Input Manipulation Attack" ] } ] }
                    ]
                },
                { "id": "AID-D-002", "name": "AI Model Anomaly & Performance Drift Detection", "description": "Continuously monitor the outputs, performance metrics (e.g., accuracy, confidence scores, precision, recall, F1-score, output distribution), and potentially internal states or feature attributions of AI models during operation. This monitoring aims to detect significant deviations from established baselines or expected behavior. Such anomalies or drift can indicate various issues, including concept drift (changes in the underlying data distribution), data drift (changes in input data characteristics), or malicious activities like ongoing data poisoning attacks, subtle model evasion attempts, or model skewing.", "implementationStrategies": ["Establish and maintain robust baseline of key model performance metrics.", "Track metrics in real-time or near real-time.", "Employ statistical concept drift and data drift detection algorithms.", "Monitor for unusual model decisions or output distributions.", "Investigate flagged anomalies and drift promptly.", "Implement feedback loops for model retraining/recalibration."], "toolsOpenSource": ["Alibi Detect", "River", "Evidently AI", "NannyML", "scikit-multiflow", "TensorFlow Data Validation (TFDV), TensorFlow Model Analysis (TFMA)"], "toolsCommercial": ["IBM Watson OpenScale", "Azure Model Monitor", "Fiddler AI, Arize AI, WhyLabs, Seldon Deploy", "Amazon SageMaker Model Monitor, Google Vertex AI Model Monitoring", "Protect AI (Layer product)"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0020 Poison Training Data (detecting impact)", "AML.T0015 Evade ML Model (detecting anomalies)", "AML.T0021 Erode ML Model Integrity", "AML.T0019 Poison ML Model (detecting impact)"] }, { "framework": "MAESTRO", "items": ["Data Poisoning (L2, impact detection)", "Model Skewing (L2/L5)", "Unpredictable agent behavior / Performance Degradation (L5)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM04:2025 Data and Model Poisoning (impact detection)", "LLM09:2025 Misinformation (if drift leads to factual errors)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML02:2023 Data Poisoning Attack (impact detection)", "ML08:2023 Model Skewing", "ML10:2023 Model Poisoning (behavioral changes)"] }] },
                {
                    "id": "AID-D-003", "name": "AI Output Monitoring & Policy Enforcement", "description": "Actively inspect the outputs generated by AI models (e.g., text responses, classifications, agent actions) in near real-time. This involves enforcing predefined safety, security, and ethical policies on the outputs and taking action (e.g., blocking, sanitizing, alerting) when violations are detected.", "toolsOpenSource": [ "NVIDIA NeMo Guardrails", "Meta's Llama Guard", "LangChain Guardrails", "Microsoft Presidio", "spaCy (for NER)", "Regular expression libraries (re in Python)" ], "toolsCommercial": [ "OpenAI Moderation API", "Google Perspective API", "Hive AI", "Scale AI", "Google Cloud DLP API", "Amazon Macie", "Microsoft Purview", "Nightfall AI" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0048.002: External Harms: Societal Harm", "AML.T0057: LLM Data Leakage" ] }, { "framework": "MAESTRO", "items": [ "Misinformation Generation (L1/L7)", "Data Exfiltration (L2)", "Data Leakage through Observability (L5)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM02:2025 Sensitive Information Disclosure", "LLM05:2025 Improper Output Handling", "LLM09:2025 Misinformation" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML03:2023 Model Inversion Attack", "ML09:2023 Output Integrity Attack" ] } ],
                    "subTechniques": [
                        { "id": "AID-D-003.001", "name": "Harmful Content & Policy Filtering", "description": "Focuses on inspecting AI-generated content for violations of safety and acceptable use policies. This includes detecting hate speech, self-harm content, explicit material, and other categories of harmful or inappropriate output.", "implementationStrategies": [ "Deploy content classification models to scan AI outputs for policy violations.", "Use a secondary 'guardrail' or 'critic' model to review the primary model's output for safety.", "Implement rule-based filters and keyword lists to block known harmful content.", "Check agent-proposed actions against a predefined list of allowed or denied behaviors." ], "toolsOpenSource": [ "NVIDIA NeMo Guardrails", "Meta's Llama Guard", "LangChain Guardrails" ], "toolsCommercial": [ "OpenAI Moderation API", "Google Perspective API", "Hive AI", "Scale AI" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0048.002: External Harms: Societal Harm" ] }, { "framework": "MAESTRO", "items": [ "Misinformation Generation (L1/L7)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM09:2025 Misinformation", "LLM05:2025 Improper Output Handling" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML09:2023 Output Integrity Attack" ] } ] },
                        { "id": "AID-D-003.002", "name": "Sensitive Information & Data Leakage Detection", "description": "Focuses on preventing the AI model from inadvertently disclosing sensitive, confidential, or private information in its outputs. This is critical for protecting user privacy and corporate data.", "implementationStrategies": [ "Use pattern matching (regex) to detect common sensitive data formats (e.g., credit card numbers, social security numbers).", "Employ Named Entity Recognition (NER) models to identify and redact PII like names, addresses, and phone numbers.", "Implement output reconstruction checks to ensure the model is not simply repeating sensitive training data.", "Develop custom detectors for proprietary information or specific internal data formats." ], "toolsOpenSource": [ "Microsoft Presidio", "spaCy (for NER)", "Regular expression libraries (re in Python)" ], "toolsCommercial": [ "Google Cloud DLP API", "Amazon Macie", "Microsoft Purview", "Nightfall AI" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0057: LLM Data Leakage" ] }, { "framework": "MAESTRO", "items": [ "Data Exfiltration (L2)", "Data Leakage through Observability (L5)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM02:2025 Sensitive Information Disclosure" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML03:2023 Model Inversion Attack" ] } ] }
                    ]
                },
                {
                    "id": "AID-D-004", "name": "Model & AI Artifact Integrity Audit & Tamper Detection", "description": "Regularly verify the cryptographic integrity and authenticity of deployed AI models, their parameters, associated datasets, and critical components of their runtime environment. This process aims to detect any unauthorized modifications, tampering, or the insertion of backdoors that could compromise the model's behavior, security, or data confidentiality. It ensures that the AI artifacts in operation are the approved, untampered versions.", "toolsOpenSource": [ "Tripwire (open source version)", "AIDE (Advanced Intrusion Detection Environment)", "GnuPG (for signature verification)", "Standard hashing utilities (sha256sum)", "Intel SGX SDK, Open Enclave SDK", "eBPF tools (Cilium, Falco)", "Keylime (TPM-based attestation)", "Git with pre-commit hooks", "Terraform, OpenTofu (for IaC drift detection)", "Checkov, tfsec (for IaC security scanning)" ], "toolsCommercial": [ "Tripwire Enterprise", "Tenable.io", "Qualys FIM", "Commercial MLOps platforms with artifact tracking", "Azure Attestation", "AWS Nitro Enclaves Attestation", "Fortanix Confidential Computing Manager", "Wiz, Prisma Cloud (CSPM tools)", "Datadog Configuration Management", "GitHub, GitLab (with advanced branch protection)" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0018: Manipulate AI Model", "AML.T0018.002: Manipulate AI Model: Embed Malware", "AML.T0058: Publish Poisoned Models", "AML.T0069: Discover LLM System Information" ] }, { "framework": "MAESTRO", "items": [ "Data Tampering (L2)", "Model Tampering (L1)", "Runtime Code Injection (L4)", "Memory Corruption (L4)", "Misconfigurations (L4)", "Policy Bypass (L6)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM03:2025 Supply Chain", "LLM04:2025 Data and Model Poisoning" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML06:2023 AI Supply Chain Attacks", "ML10:2023 Model Poisoning" ] } ],
                    "subTechniques": [
                        { "id": "AID-D-004.001", "name": "Static Artifact Hash & Signature Verification", "description": "Periodically re-hash stored models, datasets and container layers and compare against the authorised manifest.", "implementationStrategies": [ "Keep authorised hash list in a write-once model registry.", "Schedule nightly sha256sum scans or Tripwire rules over model volumes.", "Alert if an artifact hash deviates or goes missing." ], "toolsOpenSource": [ "Tripwire (open source version)", "AIDE (Advanced Intrusion Detection Environment)", "GnuPG (for signature verification)", "Standard hashing utilities (sha256sum)" ], "toolsCommercial": [ "Tripwire Enterprise", "Tenable.io", "Qualys FIM", "Commercial MLOps platforms with artifact tracking" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0018: Manipulate AI Model" , "AML.T0058: Publish Poisoned Models" ] }, { "framework": "MAESTRO", "items": [ "Data Tampering (L2)", "Model Tampering (L1)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM03:2025 Supply Chain", "LLM04:2025 Data and Model Poisoning" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML10:2023 Model Poisoning" ] } ] },
                        { "id": "AID-D-004.002", "name": "Runtime Attestation & Memory Integrity", "description": "Attest the running model process (code, weights, enclave MRENCLAVE) to detect in-memory patching or DLL injection.", "implementationStrategies": [ "Start inference in a TEE (SGX, SEV, Nitro Enclave) and verify measurement before releasing traffic.", "Use remote-attestation APIs; deny requests if the quote is stale or unrecognised.", "Monitor loaded shared-object hashes with eBPF kernel probes." ], "toolsOpenSource": [ "Intel SGX SDK, Open Enclave SDK", "eBPF tools (Cilium, Falco)", "Keylime (TPM-based attestation)" ], "toolsCommercial": [ "Azure Attestation", "AWS Nitro Enclaves Attestation", "Fortanix Confidential Computing Manager" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0018.002: Manipulate AI Model: Embed Malware" ] }, { "framework": "MAESTRO", "items": [ "Runtime Code Injection (L4)", "Memory Corruption (L4)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM03:2025 Supply Chain" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML06:2023 AI Supply Chain Attacks" ] } ] },
                        { "id": "AID-D-004.003", "name": "Configuration & Policy Drift Monitoring", "description": "Detect unauthorised edits to model-serving YAMLs, feature-store ACLs, RAG index schemas or inference-time policy files.", "implementationStrategies": [ "Store configs in Git with signed commits; enable ‘git watcher’ webhooks.", "Continuously diff live Kubernetes ConfigMaps vs declared IaC.", "Block roll-outs that add privileged host-mounts or change model endpoint ACLs." ], "toolsOpenSource": [ "Git with pre-commit hooks", "Terraform, OpenTofu (for IaC drift detection)", "Checkov, tfsec (for IaC security scanning)" ], "toolsCommercial": [ "Wiz, Prisma Cloud (CSPM tools)", "Datadog Configuration Management", "GitHub, GitLab (with advanced branch protection)" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0069: Discover LLM System Information" ] }, { "framework": "MAESTRO", "items": [ "Misconfigurations (L4)", "Policy Bypass (L6)" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML06:2023 AI Supply Chain Attacks" ] } ] }
                    ]
                },
                { "id": "AID-D-005", "name": "AI Activity Logging, Monitoring & Threat Hunting", "description": "Establish and maintain detailed, comprehensive, and auditable logs of all significant activities related to AI systems. This includes user queries and prompts, model responses and confidence scores, decisions made by AI (especially autonomous agents), tools invoked by agents, data accessed or modified, API calls (to and from the AI system), system errors, and security-relevant events. These logs are then ingested into security monitoring systems (e.g., SIEM) for correlation, automated alerting on suspicious patterns, and proactive threat hunting by security analysts to identify indicators of compromise (IoCs) or novel attack patterns targeting AI systems.", "implementationStrategies": ["Enable verbose logging for all AI interactions (inputs, outputs, agent actions, API calls, errors).", "Ensure logs are timestamped, immutable, and securely stored.", "Ingest AI-specific logs into centralized SIEM/log analytics.", "Correlate AI logs with other security data sources.", "Develop AI-specific detection rules and alerts in SIEM.", "Proactively search logs for subtle IoCs or anomalous behaviors.", "Utilize general AI/ML techniques within the SOC to help identify anomalous patterns in the Al system logs themselves, and integrate findings from specialized AI-Driven Security Analytics for AI Systems (AID-D-008) for more targeted defense.", "Log all activations of HITL control points, manual overrides, and emergency halts for comprehensive audit trails and review, cross-referencing with designs from AID-M-006.", "For agentic AI, log agent goals, plans, decisions, tool selections, tool inputs/outputs, interactions with external knowledge bases (e.g., vector DB queries and retrieved results for RAG), and any state changes.", "Log agent-attestation status, behavioural-fingerprint IDs, trust scores, and code-signing hashes for every running agent session so SOC analytics can correlate rogue-agent indicators in real time.", "See also AID-D-009 / AID-D-010 / AID-D-011 for complementary controls specific to multi-agent fact verification, goal-integrity, and rogue-agent detection."], "toolsOpenSource": ["ELK Stack (EFor autonomous agents, record the cryptographically signed mission objectives and goal hierarchy in the model card; these become the reference used by runtime Goal-Integrity Monitoring (see D-010).lasticsearch, Logstash, Kibana) or OpenSearch Stack", "Grafana Loki", "Sigma (for SIEM rules)", "Fluentd or Vector", "MLOps framework logging (e.g., MLflow)"], "toolsCommercial": ["Splunk (Enterprise, Cloud, SOAR)", "Datadog, Dynatrace, New Relic", "Cloud-native SIEMs (Azure Sentinel, Google Chronicle, AWS Security Hub)", "HiddenLayer MLDR", "AI-SPM tools"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0024.002 Extract ML Model (query patterns)", "AML.T0001 Reconnaissance (unusual queries)", "AML.T0051 LLM Prompt Injection (repeated attempts)", "AML.T0057 LLM Data Leakage (output logging)", "AML.T0012 Valid Accounts (anomalous usage)"] }, { "framework": "MAESTRO", "items": ["Model Stealing (L1)", "Agent Tool Misuse (L7)", "Compromised RAG Pipelines (L2)", "Data Exfiltration (L2)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM10:2025 Unbounded Consumption (usage patterns)", "LLM01:2025 Prompt Injection (logged attempts)", "LLM02:2025 Sensitive Information Disclosure (logged outputs)", "LLM06:2025 Excessive Agency (logged actions)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML05:2023 Model Theft (query patterns)", "ML01:2023 Input Manipulation Attack (logged inputs)"] }] },
                { "id": "AID-D-006", "name": "Explainability (XAI) Manipulation Detection", "description": "Implement mechanisms to monitor and validate the outputs and behavior of eXplainable AI (XAI) methods. The goal is to detect attempts by adversaries to manipulate or mislead these explanations, ensuring that XAI outputs accurately reflect the model's decision-making process and are not crafted to conceal malicious operations, biases, or vulnerabilities. This is crucial if XAI is used for debugging, compliance, security monitoring, or building user trust.", "perfImpact": { "level": "High", "description": "Note: Performance Impact: High (on Inference Latency). The multiple-XAI-method approach multiplies latency, and common methods like SHAP or LIME on deep models can take single-digit seconds, not milliseconds, per prediction. This cost must be carefully considered by architects." }, "implementationStrategies": ["Employ multiple, diverse XAI methods to explain the same model decision and compare their outputs for consistency; significant divergence can indicate manipulation or instability.", "Establish baselines for typical explanation characteristics (e.g., feature importance, rule sets, prototype examples) on known, benign inputs and monitor for deviations.", "Detect instability in explanations where small, inconsequential perturbations to input data lead to drastically different explanations for the same model prediction.", "Monitor for explanations that are overly simplistic for known complex decisions, that consistently highlight irrelevant or nonsensical features, or that fail to identify features known to be critical.", "Specifically test against adversarial attacks designed to fool XAI methods (e.g., \"adversarial explanations\" where the explanation is misleading but the prediction remains unchanged or changes benignly).", "Log XAI outputs and any detected manipulation alerts for investigation by AI assurance teams."], "toolsOpenSource": ["XAI libraries (e.g., SHAP, LIME, Captum for PyTorch, Alibi Explain, ELI5, InterpretML).", "Custom-developed logic for comparing and validating consistency between different explanation outputs.", "Research toolkits for adversarial attacks on XAI (if available for benchmarking)."], "toolsCommercial": ["AI Observability and Monitoring platforms (e.g., Fiddler, Arize AI, WhyLabs) that include XAI features may incorporate or allow the development of robustness checks and manipulation detection for explanations.", "Specialized AI assurance or red teaming tools that assess XAI method reliability."], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0006 Defense Evasion (if XAI is part of a defensive monitoring system and is itself targeted to be fooled). Potentially a new ATLAS technique: \"AML.TXXXX Manipulate AI Explainability\"."] }, { "framework": "MAESTRO", "items": ["Evasion of Auditing/Compliance (L6: Security & Compliance, if manipulated XAI is used to mislead auditors)", "Manipulation of Evaluation Metrics (L5: Evaluation & Observability, if explanations are used as part of the evaluation and are unreliable)", "Obfuscation of Malicious Behavior (Cross-Layer)."] }, { "framework": "OWASP LLM Top 10 2025", "items": ["Indirectly supports investigation of LLM01:2025 Prompt Injection or LLM04:2025 Data and Model Poisoning by ensuring that any XAI methods used to understand the resulting behavior are themselves trustworthy."] }, { "framework": "OWASP ML Top 10 2023", "items": ["Indirectly supports diagnosis of ML08:2023 Model Skewing or ML10:2023 Model Poisoning, by ensuring XAI methods used to identify these issues are not being manipulated."] }] },
                { "id": "AID-D-007", "name": "Multimodal Inconsistency Detection & Defense", "description": "For AI systems processing multiple input modalities (e.g., text, image, audio, video), implement mechanisms to detect and respond to inconsistencies, contradictions, or malicious instructions hidden via cross-modal interactions. This involves analyzing inputs and outputs across modalities to identify attempts to bypass security controls or manipulate one modality using another, and applying defenses to mitigate such threats.", "implementationStrategies": ["Implement semantic consistency checks between information extracted from different modalities (e.g., verify alignment between text captions and image content; ensure audio commands do not contradict visual cues).", "Scan non-primary modalities for embedded instructions or payloads intended for other modalities (e.g., steganographically hidden text in images, QR codes containing malicious prompts, audio watermarks with commands).", "Utilize separate, specialized validation and sanitization pipelines for each modality before data fusion (as outlined in enhancements to AID-H-002).", "Monitor the AI model's internal attention mechanisms (if accessible and interpretable) for unusual or forced cross-modal attention patterns that might indicate manipulation.", "Develop and maintain a library of known cross-modal attack patterns and use this knowledge to inform detection rules and defensive transformations.", "During output generation, verify that outputs are consistent with the fused understanding from all input modalities and do not disproportionately reflect manipulation from a single, potentially compromised, modality.", "Employ ensemble methods where different sub-models or experts process different modalities, with a final decision layer that checks for consensus or flags suspicious discrepancies for human review or automated rejection.", "Implement context-aware filtering that considers the typical relationships and constraints between modalities for a given task."], "toolsOpenSource": ["Computer vision libraries (OpenCV, Pillow) for image analysis (e.g., detecting text in images, QR code scanning, deepfake detection).", "NLP libraries (spaCy, NLTK, Hugging Face Transformers) for text analysis and cross-referencing with visual/audio data.", "Audio processing libraries (Librosa, PyAudio, SpeechRecognition) for audio analysis and transcription for cross-checking.", "Steganography detection tools (e.g., StegDetect, Aletheia, Zsteg).", "Custom rule engines (e.g., based on Drools, or custom Python scripting) for implementing consistency checks.", "Multimodal foundation models themselves (e.g., fine-tuned smaller models acting as \"watchdogs\" for larger ones)."], "toolsCommercial": ["Multimodal AI security platforms (emerging market, offering integrated analysis).", "Advanced data validation platforms with support for multiple data types and cross-validation.", "Content moderation services that handle and analyze multiple modalities for policy violations or malicious content.", "AI red teaming services specializing in multimodal systems."], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0051 LLM Prompt Injection (specifically cross-modal variants like in Scenario #7 of LLM01:2025 )", "AML.T0015 Evade ML Model (if evasion exploits multimodal vulnerabilities)", "AML.T0043 Craft Adversarial Data (for multimodal adversarial examples)."] }, { "framework": "MAESTRO", "items": ["Cross-Modal Manipulation Attacks (L1: Foundation Models / L2: Data Operations)", "Input Validation Attacks (L3: Agent Frameworks, for multimodal inputs)", "Data Poisoning (L2: Data Operations, if multimodal data is used for poisoning and inconsistencies are introduced)."] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM01:2025 Prompt Injection (specifically Multimodal Injection Scenario #7)", "LLM04:2025 Data and Model Poisoning (if using tainted or inconsistent multimodal data)", "LLM08:2025 Vector and Embedding Weaknesses (if multimodal embeddings are manipulated or store inconsistent data)."] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML01:2023 Input Manipulation Attack (specifically for multimodal inputs)", "ML02:2023 Data Poisoning Attack (using inconsistent or malicious multimodal data)."] }] },
                { "id": "AID-D-008", "name": "AI-Driven Security Analytics for AI Systems", "description": "Employ specialized AI/ML models (secondary AI defenders) to analyze telemetry, logs, and behavioral patterns from primary AI systems to detect sophisticated, subtle, or novel attacks that may evade rule-based or traditional detection methods. This includes identifying anomalous interactions, emergent malicious behaviors, coordinated attacks, or signs of AI-generated attacks targeting the primary AI systems.", "implementationStrategies": ["Train anomaly detection models (e.g., autoencoders, GMMs, Isolation Forests) on logs and telemetry from AI systems, including API call sequences, resource usage patterns, query structures, and agent actions.", "Develop ML classifiers (e.g., SVM, Random Forest, Gradient Boosting, NNs) to categorize interactions as benign or potentially malicious based on learned patterns from known attacks and normal behavior baselines.", "Use AI for advanced threat hunting within AI system logs, identifying complex attack sequences, low-and-slow reconnaissance, or unusual data access patterns by AI agents.", "Implement AI-based systems to continuously monitor for concept drift, data drift, or sudden performance degradation in primary AI models that might indicate an ongoing, subtle attack (complementing AID-D-002).", "Apply AI to analyze the behavior of AI agents (e.g., sequences of tool usage, goal achievement patterns) for deviations from intended goals or ethical guidelines, potentially indicating manipulation, hijacking, or emergent undesirable behaviors.", "Continuously retrain and update these secondary AI defender models with new attack data, evolving system behaviors, and feedback from incident response.", "Integrate outputs and alerts from AI defender models into the main SIEM/SOAR platforms for correlation, prioritization, and automated response orchestration.", "Consider ensemble methods for secondary AI defenders to improve robustness and reduce false positives.","Note: Performance Impact: Medium to High (on Monitoring Overhead & Latency). This technique uses a secondary AI model to analyze the primary model's activity. Inference Latency (if inline): Adds the full inference latency of the secondary guardrail model to the total time, potentially a 50-100% increase in overall latency. Cost (if offline): Doubles the computational cost for analysis, as two model inferences are run for each transaction."], "toolsOpenSource": ["General ML libraries (Scikit-learn, TensorFlow, PyTorch, Keras) for building custom detection models.", "Anomaly detection libraries (PyOD, Alibi Detect, TensorFlow Probability).", "Log analysis platforms (ELK Stack/OpenSearch with ML plugins, Apache Spot).", "Streaming data processing frameworks (Apache Kafka, Apache Flink, Apache Spark Streaming) for real-time AI analytics.", "Graph-based analytics libraries (NetworkX, PyTorch Geometric) for analyzing relationships in AI system activity."], "toolsCommercial": ["Security AI platforms that offer AI-on-AI monitoring capabilities (e.g., some advanced EDR/XDR features, User and Entity Behavior Analytics (UEBA) tools).", "Specialized AI security monitoring solutions focusing on AI workload protection.", "AI-powered SIEMs or SOAR platforms with advanced analytics modules.", "Cloud provider ML services for building and deploying custom monitoring models (e.g., SageMaker, Vertex AI, Azure ML)."], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["Many tactics by providing an advanced detection layer. Particularly useful against novel or evasive variants of AML.T0015 Evade ML Model, AML.T0051 LLM Prompt Injection, AML.T0024.002 Extract ML Model, AML.T0006 Active Scanning & Probing, and sophisticated reconnaissance activities (AML.TA0001). Could also help detect AI-generated attacks if their patterns differ from human-initiated ones."] }, { "framework": "MAESTRO", "items": ["Advanced Evasion Techniques (L1, L5, L6)", "Subtle Data or Model Poisoning effects not caught by simpler checks (L1, L2)", "Sophisticated Agent Manipulation (L7)", "Novel Attack Vectors (Cross-Layer)", "Resource Hijacking (L4, through anomalous pattern detection)."] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM01:2025 Prompt Injection (novel or obfuscated injections)", "LLM06:2025 Excessive Agency (subtle deviations in agent behavior)", "LLM10:2025 Unbounded Consumption (anomalous resource usage patterns indicating DoS or economic attacks)."] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML01:2023 Input Manipulation Attack (sophisticated adversarial inputs)", "ML05:2023 Model Theft (anomalous query patterns indicative of advanced extraction)", "ML02:2023 Data Poisoning Attack (detecting subtle behavioral shifts post-deployment)."] }] },
                { "id": "AID-D-009", "name": "Cross-Agent Fact Verification & Hallucination Cascade Prevention", "description": "Implement real-time fact verification and consistency checking mechanisms across multiple AI agents to detect and prevent the propagation of hallucinated or false information through agent networks. This technique employs distributed consensus algorithms, external knowledge base validation, and inter-agent truth verification to break hallucination cascades before they spread through the system.", "implementationStrategies": [ "Deploy distributed fact-checking algorithms that cross-reference agent outputs with multiple trusted knowledge sources before accepting information as factual", "Implement inter-agent consensus mechanisms where critical facts must be verified by multiple independent agents before being accepted into shared knowledge bases", "Utilize external authoritative data sources (APIs, databases, knowledge graphs) for real-time fact verification of agent-generated content", "Deploy contradiction detection algorithms that identify when agents produce conflicting information about the same facts", "Implement confidence scoring for agent-generated facts, with lower confidence facts requiring additional verification before propagation", "Create fact provenance tracking to trace the origin and validation history of information across agent interactions", "Deploy circuit breakers that halt information propagation when hallucination indicators exceed threshold levels" ], "toolsOpenSource": [ "Apache Kafka with custom fact-verification consumers for distributed fact checking", "Neo4j or ArangoDB for knowledge graph-based fact verification", "Apache Airflow for orchestrating complex fact-verification workflows", "Redis or Apache Ignite for high-speed fact caching and consistency checking", "Custom Python libraries using spaCy, NLTK for natural language fact extraction and comparison" ], "toolsCommercial": [ "Google Knowledge Graph API for external fact verification", "Microsoft Cognitive Services for content verification", "Palantir Foundry for large-scale data consistency and verification", "Databricks with MLflow for distributed ML-based fact verification", "Neo4j Enterprise for enterprise-grade knowledge graph verification" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": ["AML.T0021 Erode ML Model Integrity", "AML.T0048.002 External Harms: Societal Harm"] }, { "framework": "MAESTRO", "items": ["Data Poisoning (L2)", "Cross-Modal Manipulation Attacks (L1)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM09:2025 Misinformation"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML08:2023 Model Skewing"] } ] },
                { "id": "AID-D-010", "name": "AI Goal Integrity Monitoring & Deviation Detection", "description": "Continuously monitor and validate AI agent goals, objectives, and decision-making patterns to detect unauthorized goal manipulation or intent deviation. This technique establishes cryptographically signed goal states, implements goal consistency verification, and provides real-time alerting when agents deviate from their intended objectives or exhibit goal manipulation indicators.", "implementationStrategies": [ "Implement cryptographic signing of agent goals and objectives to prevent unauthorized modification", "Deploy continuous goal consistency checking algorithms that verify agent actions align with stated objectives", "Create goal deviation scoring systems that quantify how far agent behavior has drifted from intended goals", "Implement multi-agent goal verification where critical goal changes require consensus from multiple oversight agents", "Deploy behavioral pattern analysis to detect subtle goal manipulation that doesn't trigger direct goal modification alerts", "Create goal rollback mechanisms to restore agents to previous validated goal states when manipulation is detected", "Implement goal provenance tracking to audit the complete history of goal modifications and their sources" ], "toolsOpenSource": [ "HashiCorp Vault for cryptographic goal signing and verification", "Apache Kafka for real-time goal monitoring event streaming", "Prometheus and Grafana for goal deviation metrics and alerting", "Redis for fast goal state caching and comparison", "Custom Python frameworks using cryptography libraries for goal integrity verification" ], "toolsCommercial": [ "CyberArk for privileged goal management and protection", "Splunk for advanced goal deviation analytics and correlation", "Datadog for real-time goal monitoring and alerting", "HashiCorp Vault Enterprise for enterprise goal state management", "IBM QRadar for goal manipulation threat detection" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": ["AML.T0051 LLM Prompt Injection", "AML.T0054 LLM Jailbreak"] }, { "framework": "MAESTRO", "items": ["Agent Goal Manipulation (L7)", "Input Validation Attacks (L3)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM01:2025 Prompt Injection", "LLM06:2025 Excessive Agency"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML01:2023 Input Manipulation Attack"] } ] },
                { "id": "AID-D-011", "name": "Agent Behavioral Attestation & Rogue Detection", "description": "Implement continuous behavioral monitoring and attestation mechanisms to identify rogue or compromised agents in multi-agent systems. This technique uses behavioral fingerprinting, anomaly detection, and peer verification to detect agents that deviate from expected behavioral patterns or exhibit malicious characteristics.", "implementationStrategies": [ "Create behavioral fingerprints for each agent based on normal operational patterns, decision-making characteristics, and interaction styles", "Deploy peer-based agent verification where agents cross-validate each other's behaviors and report anomalies", "Implement continuous behavioral scoring that tracks agent trustworthiness based on historical actions and decisions", "Create agent quarantine mechanisms that automatically isolate agents exhibiting rogue behavior pending investigation", "Deploy behavioral drift detection to identify gradual changes in agent behavior that might indicate compromise", "Implement agent population monitoring to detect unauthorized agent introduction or agent impersonation", "Create behavioral consensus mechanisms where critical decisions require verification from multiple trusted agents" ], "toolsOpenSource": [ "scikit-learn for behavioral pattern analysis and anomaly detection", "Apache Kafka for real-time behavioral event streaming", "InfluxDB for time-series behavioral data storage", "Grafana for behavioral monitoring dashboards", "Custom frameworks using TensorFlow/PyTorch for deep behavioral analysis" ], "toolsCommercial": [ "Splunk for advanced behavioral analytics and correlation", "Darktrace for AI-powered behavioral anomaly detection", "IBM QRadar for behavioral threat intelligence", "Microsoft Sentinel for cloud-based behavioral monitoring", "Vectra AI for network behavioral analysis adapted for agent monitoring" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": ["AML.T0017 Persistence", "AML.T0048 External Harms"] }, { "framework": "MAESTRO", "items": ["Rogue Agent Behavior (L7)", "Agent Identity Attack (L7)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM06:2025 Excessive Agency"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML06:2023 AI Supply Chain Attacks"] } ] }
            ]
        },
        {
            "name": "Isolate",
            "purpose": "The \"Isolate\" tactic involves implementing measures to contain malicious activity and limit its potential spread or impact should an AI system or one of its components become compromised. This includes sandboxing AI processes, segmenting networks to restrict communication, and establishing mechanisms to quickly quarantine or throttle suspicious interactions or misbehaving AI entities.",
            "techniques": [
                {
                    "id": "AID-I-001", "name": "AI Execution Sandboxing & Runtime Isolation", "description": "Execute AI models, autonomous agents, or individual AI tools and plugins within isolated environments such as sandboxes, containers, or microVMs. These environments must be configured with strict limits on resources, permissions, and network connectivity. The primary goal is that if an AI component is compromised or behaves maliciously, the impact is confined to the isolated sandbox, preventing harm to the host system or lateral movement.", "toolsOpenSource": [ "Docker, Podman", "Kubernetes (with PodSecurityPolicies/Admission)", "CNI Plugins (Calico, Cilium)", "Firecracker", "Kata Containers", "gVisor", "seccomp, AppArmor", "Wasmtime, Wasmer" ], "toolsCommercial": [ "Red Hat OpenShift", "Sysdig Secure", "Palo Alto Networks Prisma Cloud", "AWS Lambda (uses Firecracker)", "Google Cloud GKE Sandboxing (uses gVisor)", "RunSafe Aligned" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0053: LLM Plugin Compromise", "AML.T0072: Reverse Shell" ] }, { "framework": "MAESTRO", "items": [ "Compromised Container Images (L4)", "Lateral Movement (Cross-Layer)", "Agent Tool Misuse (L7)", "Resource Hijacking (L4)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM05:2025 Improper Output Handling", "LLM06:2025 Excessive Agency" ] } ],
                    "subTechniques": [
                        { "id": "AID-I-001.001", "name": "Container-Based Isolation", "description": "Utilizes container technologies like Docker or Kubernetes to package and run AI workloads in isolated user-space environments. This approach provides process and filesystem isolation and allows for resource management and network segmentation.", "implementationStrategies": [ "Deploy AI models and services in hardened, minimal-footprint container images.", "Apply Kubernetes security contexts to restrict container privileges (e.g., runAsNonRoot).", "Use network policies to enforce least-privilege communication between AI pods.", "Set strict resource quotas (CPU, memory, GPU) to prevent resource exhaustion attacks.", "Mount filesystems as read-only wherever possible." ], "toolsOpenSource": [ "Docker, Podman", "Kubernetes (with PodSecurityPolicies/Admission)", "CNI Plugins (Calico, Cilium)" ], "toolsCommercial": [ "Red Hat OpenShift", "Sysdig Secure", "Palo Alto Networks Prisma Cloud" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0053: LLM Plugin Compromise" ] }, { "framework": "MAESTRO", "items": [ "Compromised Container Images (L4)", "Lateral Movement (Cross-Layer)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM06:2025 Excessive Agency" ] } ] },
                        { "id": "AID-I-001.002", "name": "MicroVM & Low-Level Sandboxing", "description": "Employs lightweight Virtual Machines (MicroVMs) or kernel-level sandboxing technologies to provide a stronger isolation boundary than traditional containers. This is critical for running untrusted code or highly sensitive AI workloads.", "implementationStrategies": [ "Use lightweight VMs like Firecracker or Kata Containers for strong hardware-virtualized isolation.", "Apply OS-level sandboxing with tools like gVisor to intercept and filter system calls.", "Define strict seccomp-bpf profiles to whitelist only necessary system calls for model inference.", "Utilize WebAssembly (WASM) runtimes to run AI models in a high-performance, secure sandbox with a minimal attack surface.", "Note: Performance Impact: Low to Medium (on Startup Time & CPU/Memory Overhead). Stronger isolation technologies like gVisor or Firecracker impose a greater performance penalty than standard containers. CPU Overhead: Can introduce a 5% to 15% CPU performance overhead compared to running in a standard container. Startup Time: Adds a small but measurable delay, typically 5ms to 50ms of additional startup time per instance." ], "toolsOpenSource": [ "Firecracker", "Kata Containers", "gVisor", "seccomp, AppArmor", "Wasmtime, Wasmer" ], "toolsCommercial": [ "AWS Lambda (uses Firecracker)", "Google Cloud GKE Sandboxing (uses gVisor)", "RunSafe Aligned" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0072: Reverse Shell" ] }, { "framework": "MAESTRO", "items": [ "Agent Tool Misuse (L7)", "Resource Hijacking (L4)" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM05:2025 Improper Output Handling", "LLM06:2025 Excessive Agency" ] } ] }
                    ]
                },
                { "id": "AID-I-002", "name": "Network Segmentation & Isolation for AI Systems", "description": "Implement network segmentation and microsegmentation strategies to isolate AI systems and their components (e.g., training environments, model serving endpoints, data stores, agent control planes) from general corporate networks and other critical IT/OT systems. This involves enforcing strict communication rules through firewalls, proxies, and network policies to limit an attacker's ability to pivot from a compromised AI component to other parts of the network, or to exfiltrate data to unauthorized destinations. This technique reduces the \"blast radius\" of a security incident involving an AI system.", "implementationStrategies": ["Host critical AI components on dedicated network segments (VLANs, VPCs).", "Apply least privilege to network communications for AI systems.", "Utilize API gateways or forward proxies to mediate and control AI traffic.", "Implement microsegmentation (SDN, service mesh, host-based firewalls).", "Separate development/testing environments from production.", "Regularly review and audit network segmentation rules."], "toolsOpenSource": ["Linux Netfilter (iptables, nftables), firewalld", "Kubernetes Network Policies", "Service Mesh (Istio, Linkerd, Kuma)", "CNI plugins (Calico, Cilium)", "Open-source API Gateways (Kong, Tyk, APISIX)"], "toolsCommercial": ["Microsegmentation platforms (Illumio, Guardicore, Cisco Secure Workload, Akamai Guardicore)", "Next-Generation Firewalls (NGFWs)", "Cloud-native firewall services (AWS Network Firewall, Azure Firewall, Google Cloud Firewall)", "Commercial API Gateway solutions"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0025 Exfiltration via Cyber Means", "General Lateral Movement tactics", "AML.T0003 Resource Development (blocking unauthorized downloads)"] }, { "framework": "MAESTRO", "items": ["Data Exfiltration (L2/Cross-Layer)", "Lateral Movement (Cross-Layer)", "Compromised RAG Pipelines (L2, isolating components)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM02:2025 Sensitive Information Disclosure (limits exfil paths)", "LLM03:2025 Supply Chain (isolating third-party components)", "LLM06:2025 Excessive Agency (limits reach of compromised agent)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML05:2023 Model Theft (isolating repositories)", "ML06:2023 AI Supply Chain Attacks (segmenting components)"] }] },
                { "id": "AID-I-003", "name": "Quarantine & Throttling of AI Interactions", "description": "Implement mechanisms to automatically or manually isolate, rate-limit, or place into a restricted \"safe mode\" specific AI system interactions when suspicious activity is detected. This could apply to individual user sessions, API keys, IP addresses, or even entire AI agent instances. The objective is to prevent potential attacks from fully executing, spreading, or causing significant harm by quickly containing or degrading the capabilities of the suspicious entity. This is an active response measure triggered by detection systems.", "implementationStrategies": ["Automated quarantine based on high-risk behavior alerts (cut access, move to honeypot, disable key/account).", "Dynamic rate limiting for anomalous behavior (query spikes, complex queries).", "Stricter rate limits for unauthenticated/less trusted users.", "Design AI systems with a \"safe mode\" or degraded functionality state.", "Utilize SOAR platforms to automate quarantine/throttling actions."], "toolsOpenSource": ["Fail2Ban (adapted for AI logs)", "Custom scripts (Lambda, Azure Functions, Cloud Functions) for automated actions", "API Gateways (Kong, Tyk, Nginx) for rate limiting", "Kubernetes for resource quotas/isolation"], "toolsCommercial": ["API Security and Bot Management solutions (Cloudflare, Akamai, Imperva)", "ThreatWarrior (automated detection/response)", "SIEM/SOAR platforms (Splunk SOAR, Palo Alto XSOAR, IBM QRadar SOAR)", "WAFs with advanced rate limiting"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0024.002 Extract ML Model (rate-limiting)", "AML.T0029 Denial of ML Service (throttling)", "AML.T0034 Cost Harvesting (limiting rates)", "Active exploitation scenarios (quarantine stops)"] }, { "framework": "MAESTRO", "items": ["Model Stealing (L1, throttling)", "DoS on Framework APIs / Data Infrastructure (L3/L2)", "Resource Hijacking (L4, containing processes)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM10:2025 Unbounded Consumption (throttling/quarantining)", "LLM01:2025 Prompt Injection (quarantining repeat offenders)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML05:2023 Model Theft (throttling excessive queries)"] }] },
                { "id": "AID-I-004", "name": "Agent Memory & State Isolation", "description": "Specifically for agentic AI systems, implement mechanisms to isolate and manage the agent's memory (e.g., conversational context, short-term state, knowledge retrieved from vector databases) and periodically reset or flush it. This defense aims to prevent malicious instructions, poisoned data, or exploited states (e.g., a \"jailbroken\" state) from persisting across multiple interactions, sessions, or from affecting other unrelated agent tasks or instances. It helps to limit the temporal scope of a successful manipulation.", "implementationStrategies": ["Implement per-session/per-user conversational context.", "Regularly flush or use short context windows for agent interactions.", "Partition long-term memory (vector DBs) based on trust levels/contexts.", "Implement strict validation/filtering for writes to agent long-term memory.", "Validate and sanitize persisted state information before loading.", "Consider periodic resets of volatile memory for long-running agents.", "Implement checks to prevent an agent from writing overly long or computationally expensive data into shared memory stores that could lead to denial of service for other agents or processes accessing that memory."], "toolsOpenSource": ["LangChain Guardrails or custom callback handlers", "Custom wrappers in agentic frameworks (AutoGen, CrewAI, Semantic Kernel, LlamaIndex)", "Vector databases (Weaviate, Qdrant, Pinecone) with access controls"], "toolsCommercial": ["Lasso Security (agent memory lineage/monitoring)", "Enterprise agent platforms with secure state management"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0018.001 Backdoor ML Model: Poison LLM Memory", "AML.T0017 Persistence (preventing long-term state manipulation)", "AML.T0051 LLM Prompt Injection (limits impact duration)"] }, { "framework": "MAESTRO", "items": ["Agent Goal Manipulation / Agent Tool Misuse (L7, preventing persistent manipulated state)", "Data Poisoning (L2, if agent memory is target)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM01:2025 Prompt Injection (non-persistent malicious context)", "LLM04:2025 Data and Model Poisoning (agent memory as poisoned data)", "LLM08:2025 Vector and Embedding Weaknesses (mitigating malicious data in vector DB)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["Relevant if agent memory is considered part of model state/operational data."] }] },
                { "id": "AID-I-005", "name": "Emergency \"Kill-Switch\" / AI System Halt", "description": "Establish and maintain a reliable, rapidly invokable mechanism to immediately halt, disable, or severely restrict the operation of an AI model or autonomous agent if it exhibits confirmed critical malicious behavior, goes \"rogue\" (acts far outside its intended parameters in a harmful way), or if a severe, ongoing attack is detected and other containment measures are insufficient. This is a last-resort containment measure designed to prevent catastrophic harm or further compromise.", "implementationStrategies": ["Implement automated safety monitors and triggers for critical deviations.", "Provide secure, MFA-protected manual override for human operators.", "Design agents with internal, independent watchdog modules.", "Define clear protocols for kill-switch activation and recovery.", "Develop procedures for safely restarting and verifying AI system post-halt.", "Ensure kill-switch mechanisms are aligned with, and their operational procedures are documented in, the HITL Control Point Mapping (AID-M-006) to ensure clarity on manual activation and authority [referencing existing concepts in 507-509]."], "toolsOpenSource": ["Custom scripts/automation playbooks (Ansible, cloud CLIs) to stop/delete resources", "Circuit breaker patterns in microservices"], "toolsCommercial": ["\"Red Button\" solutions from AI platform vendors", "Edge AI Safeguard solutions", "EDR/XDR solutions (SentinelOne, CrowdStrike) to kill processes/isolate hosts"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0048 External Harms (Societal, Financial, Reputational, User)", "AML.T0029 Denial of ML Service (by runaway agent)", "AML.T0017 Persistence (terminating malicious agent)"] }, { "framework": "MAESTRO", "items": ["Agent acting on compromised goals/tools leading to severe harm (L7)", "Runaway/critically malfunctioning foundation models (L1)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM06:2025 Excessive Agency (ultimate backstop)", "LLM10:2025 Unbounded Consumption (preventing catastrophic costs)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["Any ML attack scenario causing immediate, unacceptable harm requiring emergency shutdown."] }] }
            ]
        },
        {
            "name": "Deceive",
            "purpose": "The \"Deceive\" tactic involves the strategic use of decoys, misinformation, or the manipulation of an adversary's perception of the AI system and its environment. The objectives are to misdirect attackers away from real assets, mislead them about the system's true vulnerabilities or value, study their attack methodologies in a safe environment, waste their resources, or deter them from attacking altogether.",
            "techniques": [
                { "id": "AID-DV-001", "name": "Honeypot AI Services & Decoy Models/APIs", "description": "Deploy decoy AI systems, such as fake LLM APIs, ML model endpoints serving synthetic or non-sensitive data, or imitation agent services, that are designed to appear valuable, vulnerable, or legitimate to potential attackers. These honeypots are instrumented for intensive monitoring to log all interactions, capture attacker TTPs (Tactics, Techniques, and Procedures), and gather threat intelligence without exposing real production systems or data. They can also be used to slow down attackers or waste their resources.", "implementationStrategies": ["Set up AI model instances with controlled weaknesses/attractive characteristics.", "Instrument honeypot AI service for detailed logging.", "Design honeypots to mimic production services but ensure isolation.", "Consider honeypots with slow/slightly erroneous responses.", "Integrate honeypot alerts with SIEM/SOC.", "Seed LLM honeypots with trigger phrases or known jailbreak susceptibility."], "toolsOpenSource": ["General honeypot frameworks (Cowrie, Dionaea, Conpot) adapted", "Sandboxed open-source LLM as honeypot", "Mock API tools (MockServer, WireMock)"], "toolsCommercial": ["Deception technology platforms (TrapX, SentinelOne ShadowPlex, Illusive, Acalvio)", "Specialized AI security vendors with AI honeypot capabilities"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0001 Reconnaissance", "AML.T0024.002 Extract ML Model / AML.T0048.004 External Harms: ML Intellectual Property Theft", "AML.T0051 LLM Prompt Injection / AML.T0054 LLM Jailbreak"] }, { "framework": "MAESTRO", "items": ["Model Stealing (L1)", "Marketplace Manipulation (L7, decoy agents)", "Evasion of Detection (L5, studying evasion attempts)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM01:2025 Prompt Injection (capturing attempts)", "LLM10:2025 Unbounded Consumption (studying resource abuse)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML05:2023 Model Theft (luring to decoy)", "ML01:2023 Input Manipulation Attack (observing attempts)"] }] },
                { "id": "AID-DV-002", "name": "Honey Data, Decoy Artifacts & Canary Tokens for AI", "description": "Strategically seed the AI ecosystem (training datasets, model repositories, configuration files, API documentation) with enticing but fake data, decoy model artifacts (e.g., a seemingly valuable but non-functional or instrumented model file), or canary tokens (e.g., fake API keys, embedded URLs in documents). These \"honey\" elements are designed to be attractive to attackers. If an attacker accesses, exfiltrates, or attempts to use these decoys, it triggers an alert, signaling a breach or malicious activity and potentially providing information about the attacker's actions or location.", "implementationStrategies": ["Embed unique, synthetic honey records in datasets/databases.", "Publish fake/instrumented decoy model artifacts.", "Create and embed decoy API keys/access tokens (Canary Tokens).", "Embed trackable URLs/web bugs in fake sensitive documents.", "Watermark synthetic data in honeypots/decoys.", "Ensure honey elements are isolated and cannot impact production.", "Integrate honey element alerts into security monitoring."], "toolsOpenSource": ["Canarytokens.org by Thinkst", "Synthetic data generation tools (Faker, SDV)", "Custom scripts for decoy files/API keys"], "toolsCommercial": ["Thinkst Canary (commercial platform)", "Deception platforms (Illusive, Acalvio, SentinelOne) with data decoy capabilities", "Some DLP solutions adaptable for honey data"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0025 Exfiltration via Cyber Means (honey data/canaries exfiltrated)", "AML.T0024.002 Extract ML Model (decoy model/canary in docs)", "AML.T0008 ML Supply Chain Compromise (countering with fake vulnerable models)"] }, { "framework": "MAESTRO", "items": ["Data Exfiltration (L2, detecting honey data exfil)", "Model Stealing (L1, decoy models/watermarked data)", "Unauthorized access to layers with honey tokens"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM02:2025 Sensitive Information Disclosure (honey data mimicking sensitive info)", "LLM03:2025 Supply Chain (decoy artifacts accessed)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML05:2023 Model Theft (decoy models/API keys)", "ML01:2023 Input Manipulation Attack (observing attempts)"] }] },
                { "id": "AID-DV-003", "name": "Dynamic Response Manipulation for AI Interactions", "description": "Implement mechanisms where the AI system, upon detecting suspicious or confirmed adversarial interaction patterns (e.g., repeated prompt injection attempts, queries indicative of model extraction), deliberately alters its responses to be misleading, unhelpful, or subtly incorrect to the adversary. This aims to frustrate the attacker's efforts, waste their resources, make automated attacks less reliable, and potentially gather more intelligence on their TTPs without revealing the deception. The AI might simultaneously alert defenders to the ongoing deceptive engagement.", "implementationStrategies": ["Provide subtly incorrect/incomplete/nonsensical outputs to suspected malicious actors.", "Introduce controlled randomization or benign noise into model outputs for suspicious sessions.", "For agentic systems, feign compliance with malicious instructions but perform safe no-ops.", "Subtly degrade quality/utility of responses to queries matching model extraction patterns.", "Ensure deceptive responses are distinguishable by internal monitoring."], "toolsOpenSource": ["AdvTorch MTD (research tools for noisy outputs/MTD)", "Custom logic in AI frameworks (LangChain, Semantic Kernel) for deceptive response mode", "Research prototypes for responsive deception"], "toolsCommercial": ["Advanced LLM firewalls/AI security gateways with deceptive response policies (e.g., SAP Adversarial AI Protector)", "Adaptable deception technology platforms"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0024.002 Extract ML Model (misleading outputs)", "AML.T0051 LLM Prompt Injection / AML.T0054 LLM Jailbreak (unreliable/misleading payloads)", "AML.T0001 Reconnaissance (inaccurate system info)"] }, { "framework": "MAESTRO", "items": ["Model Stealing (L1, frustrating extraction)", "Agent Goal Manipulation / Agent Tool Misuse (L7, agent feigns compliance)", "Evasion of Detection (L5, harder to confirm evasion success)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM01:2025 Prompt Injection (unreliable outcome for attacker)", "LLM02:2025 Sensitive Information Disclosure (fake/obfuscated data)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML05:2023 Model Theft (unusable responses)", "ML01:2023 Input Manipulation Attack (inconsistent/noisy outputs)"] }] },
                { "id": "AID-DV-004", "name": "AI Output Watermarking & Telemetry Traps", "description": "Embed imperceptible or hard-to-remove watermarks, unique identifiers, or telemetry \"beacons\" into the outputs generated by AI models (e.g., text, images, code). If these outputs are found externally (e.g., on the internet, in a competitor's product, in leaked documents), the watermark or beacon can help trace the output back to the originating AI system, potentially identifying model theft, misuse, or data leakage. Telemetry traps involve designing the AI to produce specific, unique (but benign) outputs for certain rare or crafted inputs, which, if observed externally, indicate that the model or its specific knowledge has been compromised or replicated.", "implementationStrategies": ["For text, subtly alter word choices, sentence structures, or token frequencies.", "For images, embed imperceptible digital watermarks in pixel data.", "Instrument model APIs with unique telemetry markers for specific queries.", "Inject unique, identifiable synthetic data points into training set for provenance.", "Ensure watermarks/telemetry don't degrade performance or UX.", "Develop robust methods for detecting watermarks/telemetry externally."], "toolsOpenSource": ["MarkLLM (watermarking LLM text)", "SynthID (Google, watermarking AI-generated images/text)", "Steganography libraries (adaptable)", "Research tools for robust NN output watermarking"], "toolsCommercial": ["Verance Watermarking (AI content)", "Sensity AI Guard (deepfake detection/watermarking)", "Commercial digital watermarking solutions", "Content authenticity platforms"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0024.002 Extract ML Model / AML.T0048.004 External Harms: ML Intellectual Property Theft", "AML.T0057 LLM Data Leakage (tracing watermarked outputs)", "AML.T0048.002 External Harms: Societal Harm (attributing deepfakes/misinfo)"] }, { "framework": "MAESTRO", "items": ["Model Stealing (L1, identifying stolen outputs)", "Data Exfiltration (L2, exfiltrated watermarked data)", "Misinformation Generation (L1/L7, attribution/detection)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM02:2025 Sensitive Information Disclosure (leaked watermarked output)", "LLM09:2025 Misinformation (identifying AI-generated content)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML05:2023 Model Theft (traceable models/outputs)", "ML09:2023 Output Integrity Attack (watermark destruction reveals tampering)"] }] },
                { "id": "AID-DV-005", "name": "Decoy Agent Behaviors & Canary Tasks", "description": "For autonomous AI agents, design and implement decoy or \"canary\" functionalities, goals, or sub-agents that appear valuable or sensitive but are actually monitored traps. If an attacker successfully manipulates an agent (e.g., via prompt injection or memory poisoning) and directs it towards these decoy tasks or to exhibit certain predefined suspicious behaviors, it triggers an alert, revealing the compromise attempt and potentially the attacker's intentions, without risking real assets.", "implementationStrategies": ["Equip agent with shadow/canary goal/tool leading to monitored environment.", "Create dummy 'watcher' agent personas.", "Issue benign 'test prompts' or 'internal audit' instructions to agent.", "Design agents to report attempts to perform actions outside capabilities/ethics.", "Ensure decoy behaviors are well-instrumented and isolated."], "toolsOpenSource": ["Agentic Radar (CLI scanner, adaptable for decoy tests)", "Custom logic in agentic frameworks (AutoGen, CrewAI, Langroid) for canary tasks", "Integration with logging/alerting systems (ELK, Prometheus)"], "toolsCommercial": ["Emerging AI safety/agent monitoring platforms", "Adaptable deception technology platforms"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0010 Privilege Escalation / AML.T0009.002 LLM Plugin Compromise (decoy tool triggers alert)", "AML.T0051 LLM Prompt Injection / AML.T0054 LLM Jailbreak (injection leads to canary task)", "AML.T0018.001 Backdoor ML Model: Poison LLM Memory (poisoned memory leads to decoy goal)"] }, { "framework": "MAESTRO", "items": ["Agent Goal Manipulation / Agent Tool Misuse (L7, luring to decoy tools/goals)", "Agent Identity Attack (directing to canary tasks)", "Orchestration Attacks (L3, interaction with decoy components)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM01:2025 Prompt Injection (detecting successful diversion to decoy)", "LLM06:2025 Excessive Agency (agent attempts to use decoy tool)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["Relevant if agent behavior compromised due to model issues, interaction with decoys could reveal this."] }] },
                { "id": "AID-DV-006", "name": "Deceptive System Information", "description": "When probed by unauthenticated or suspicious users, the AI system provides misleading information about its architecture, capabilities, or underlying models. For example, an API might return headers suggesting it's built on a different framework, or an LLM might respond to 'What model are you?' with a decoy answer.", "implementationStrategies": [ "Modify API server headers (e.g., 'Server', 'X-Powered-By') to return decoy information.", "Configure LLMs with system prompts that instruct them to provide a specific, non-truthful answer to questions about their identity or architecture.", "Create fake API documentation or endpoint responses that suggest different functionalities or data schemas.", "Use API gateways or proxies to intercept and modify responses to reconnaissance-style queries.", "Ensure that deceptive information does not interfere with legitimate use or monitoring of the system." ], "toolsOpenSource": [ "API Gateway configurations (Kong, Tyk, Nginx)", "Web server configuration files (.htaccess for Apache, nginx.conf)", "Custom code in application logic to handle specific queries." ], "toolsCommercial": [ "Deception technology platforms.", "API management and security solutions." ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": ["AML.T0007 Discover ML Artifacts", "AML.T0069 Discover LLM System Information"] }, { "framework": "MAESTRO", "items": ["Malicious Agent Discovery (L7)", "Evasion of Detection (L5)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["Disrupts reconnaissance phase of attacks like ML05:2023 Model Theft."] } ] }
            ]
        },
        {
            "name": "Evict",
            "purpose": "The \"Evict\" tactic focuses on the active removal of an adversary's presence from a compromised AI system and the elimination of any malicious artifacts they may have introduced. Once an intrusion or malicious activity has been detected and contained, eviction procedures are executed to ensure the attacker is thoroughly expelled, their access mechanisms are dismantled, and any lingering malicious code, data, or configurations are purged.",
            "techniques": [
                { "id": "AID-E-001", "name": "Credential Revocation & Rotation for AI Systems", "description": "Immediately revoke, invalidate, or rotate any credentials (e.g., API keys, access tokens, user account passwords, service account credentials, certificates) that are known or suspected to have been compromised or used by an adversary to gain unauthorized access to or interact maliciously with AI systems, models, data, or MLOps pipelines. This action aims to cut off the attacker's current access and prevent them from reusing stolen credentials.", "implementationStrategies": ["Automate credential invalidation upon alert.", "Implement rapid rotation process for all secrets.", "Force password resets for compromised user accounts.", "Revoke/reissue compromised AI agent credentials.", "Remove unauthorized accounts/API keys created by attacker.", "Ensure prompt propagation of revocation."], "toolsOpenSource": ["Cloud provider CLIs/SDKs for IAM automation", "HashiCorp Vault", "Keycloak or other IAM solutions with APIs"], "toolsCommercial": ["PAM solutions (CyberArk, Delinea, BeyondTrust)", "IDaaS platforms (Okta, Ping Identity)", "SIEM/SOAR platforms for automated revocation"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0012 Valid Accounts", "AML.T0011 Initial Access (stolen creds)", "AML.T0017 Persistence (credential-based)"] }, { "framework": "MAESTRO", "items": ["Agent Identity Attack / Compromised Agent Registry (L7)", "Unauthorized access via stolen credentials"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM02:2025 Sensitive Information Disclosure (if creds stolen)", "LLM06:2025 Excessive Agency (if agent creds compromised)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML05:2023 Model Theft (if via compromised creds)"] }] },
                { "id": "AID-E-002", "name": "AI Process & Session Eviction", "description": "Terminate any running AI model instances, agent processes, user sessions, or containerized workloads that are confirmed to be malicious, compromised, or actively involved in an attack. This immediate action halts the adversary's ongoing activities within the AI system and removes their active foothold.", "implementationStrategies": ["Identify and kill malicious AI model/inference server processes.", "Forcefully terminate/reset hijacked AI agent sessions/instances.", "Quarantine/shut down compromised pods/containers in Kubernetes.", "Invalidate active user sessions associated with malicious activity.", "Log eviction of processes/sessions for forensics."], "toolsOpenSource": ["OS process management (kill, pkill, taskkill)", "Container orchestration CLIs (kubectl delete pod --force)", "HIPS (OSSEC, Wazuh)", "Custom scripts for session clearing (Redis FLUSHDB)"], "toolsCommercial": ["EDR solutions (CrowdStrike, SentinelOne, Carbon Black)", "Cloud provider management consoles/APIs for instance termination", "APM tools with session management"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0009 Execution (stops active malicious code)", "AML.T0051 LLM Prompt Injection / AML.T0054 LLM Jailbreak (terminates manipulated session)", "AML.T0017 Persistence (if via running process/session)"] }, { "framework": "MAESTRO", "items": ["Agent Tool Misuse / Agent Goal Manipulation (L7, terminating rogue agent)", "Resource Hijacking (L4, killing resource-abusing processes)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM01:2025 Prompt Injection (ending manipulated session)", "LLM06:2025 Excessive Agency (terminating overreaching agent)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["Any attack resulting in a malicious running process (e.g., ML06:2023 AI Supply Chain Attacks)"] }] },
                {
                    "id": "AID-E-003", "name": "AI Backdoor & Malicious Artifact Removal", "description": "Systematically scan for, identify, and remove any malicious artifacts introduced by an attacker into the AI system. This includes backdoors in models, poisoned data, malicious code, or configuration changes designed to grant persistent access or manipulate AI behavior.", "toolsOpenSource": [ "Adversarial Robustness Toolbox (ART)", "Neural Cleanse (research code)", "Trojan Vision Toolbox", "Great Expectations", "Pandas Profiling", "DVC", "Alibi Detect", "ClamAV", "YARA", "AIDE (Advanced Intrusion Detection Environment)", "osquery" ], "toolsCommercial": [ "HiddenLayer Model Scanner", "Protect AI Platform", "Trojan AI (service)", "Cleanlab", "Data quality platforms (e.g., Informatica, Talend)", "CrowdStrike Falcon", "SentinelOne", "Tanium", "Carbon Black Cloud" ], "defendsAgainst": [ { "framework": "MITRE ATLAS", "items": [ "AML.T0011.001: User Execution: Malicious Package", "AML.T0018: Manipulate AI Model", "AML.T0018.002: Manipulate AI Model: Embed Malware", "AML.T0020: Poison Training Data", "AML.T0070: RAG Poisoning" ] }, { "framework": "OWASP LLM Top 10 2025", "items": [ "LLM03:2025 Supply Chain", "LLM04:2025 Data and Model Poisoning", "LLM08:2025 Vector and Embedding Weaknesses" ] }, { "framework": "OWASP ML Top 10 2023", "items": [ "ML02:2023 Data Poisoning Attack", "ML06:2023 AI Supply Chain Attacks", "ML10:2023 Model Poisoning" ] } ],
                    "subTechniques": [
                        { "id": "AID-E-003.001", "name": "Neural Network Backdoor Detection & Removal", "description": "Focuses on identifying and removing backdoors embedded within neural network model parameters, including trigger-based backdoors that cause misclassification on specific inputs.", "implementationStrategies": [ "Apply neural cleanse techniques to identify potential backdoor triggers.", "Use activation clustering to detect neurons responding to backdoor patterns.", "Implement fine-pruning to remove suspicious neurons while maintaining accuracy.", "Retrain or fine-tune models on certified clean data to overwrite backdoors.", "Employ differential testing between suspect and clean models." ], "toolsOpenSource": ["Adversarial Robustness Toolbox (ART)", "Neural Cleanse (research code)", "Trojan Vision Toolbox"], "toolsCommercial": ["HiddenLayer Model Scanner", "Protect AI Platform", "Trojan AI (service)"], "defendsAgainst": [{"framework": "MITRE ATLAS", "items": ["AML.T0018: Manipulate AI Model"]}, {"framework": "OWASP LLM Top 10 2025", "items": ["LLM04:2025 Data and Model Poisoning"]}, {"framework": "OWASP ML Top 10 2023", "items": ["ML10:2023 Model Poisoning"]}] },
                        { "id": "AID-E-003.002", "name": "Poisoned Data Detection & Cleansing", "description": "Identifies and removes maliciously crafted data points from training sets, vector databases, or other data stores that could influence model behavior or enable attacks.", "implementationStrategies": [ "Use statistical outlier detection to identify anomalous training samples.", "Implement data provenance tracking to identify suspect data sources.", "Apply clustering techniques to find groups of potentially poisoned samples.", "Scan vector databases for embeddings generated from known malicious content.", "Validate data against known-good checksums or cryptographic signatures.", "Implement gradual data removal and retraining to minimize service disruption." ], "toolsOpenSource": ["Great Expectations", "Pandas Profiling", "DVC", "Alibi Detect"], "toolsCommercial": ["Cleanlab", "Data quality platforms (e.g., Informatica, Talend)"], "defendsAgainst": [{"framework": "MITRE ATLAS", "items": ["AML.T0020: Poison Training Data", "AML.T0070: RAG Poisoning"]}, {"framework": "OWASP LLM Top 10 2025", "items": ["LLM04:2025 Data and Model Poisoning", "LLM08:2025 Vector and Embedding Weaknesses"]}, {"framework": "OWASP ML Top 10 2023", "items": ["ML02:2023 Data Poisoning Attack"]}] },
                        { "id": "AID-E-003.003", "name": "Malicious Code & Configuration Cleanup", "description": "Removes malicious scripts, modified configuration files, unauthorized tools, or persistence mechanisms that attackers may have introduced into the AI system infrastructure.", "implementationStrategies": [ "Scan for unauthorized modifications to ML framework files or libraries.", "Check for malicious model loading code or custom layers with backdoors.", "Verify integrity of agent tools and plugins against known-good versions.", "Remove unauthorized scheduled tasks or cron jobs related to AI systems.", "Clean up modified configuration files and restore secure defaults.", "Scan for and remove web shells or reverse shells in AI serving infrastructure." ], "toolsOpenSource": ["ClamAV", "YARA", "AIDE (Advanced Intrusion Detection Environment)", "osquery"], "toolsCommercial": ["CrowdStrike Falcon", "SentinelOne", "Tanium", "Carbon Black Cloud"], "defendsAgainst": [{"framework": "MITRE ATLAS", "items": ["AML.T0018.002: Manipulate AI Model: Embed Malware", "AML.T0011.001: User Execution: Malicious Package"]}, {"framework": "OWASP LLM Top 10 2025", "items": ["LLM03:2025 Supply Chain"]}, {"framework": "OWASP ML Top 10 2023", "items": ["ML06:2023 AI Supply Chain Attacks"]}] }
                    ]
                },
                { "id": "AID-E-004", "name": "System Patching & Hardening Post-AI Attack", "description": "After an attack vector has been identified and the adversary evicted, rapidly apply necessary security patches to vulnerable software components (e.g., ML libraries, operating systems, web servers, agent frameworks) and harden system configurations that were exploited or found to be weak. This step aims to close the specific vulnerabilities used by the attacker and strengthen overall security posture to prevent reinfection or similar future attacks.", "implementationStrategies": ["Apply security patches for exploited CVEs in AI stack.", "Review and harden abused/insecure system configurations.", "Strengthen IAM policies, input/output validation, network segmentation.", "Disable unnecessary services or LLM plugin functionalities.", "Add new detection rules/IOCs based on attack specifics.", "Verify patches and hardening measures."], "toolsOpenSource": ["Package managers (apt, yum, pip, conda)", "Configuration management tools (Ansible, Chef, Puppet)", "Vulnerability scanners (OpenVAS, Trivy)", "Static analysis tools (Bandit)"], "toolsCommercial": ["Automated patch management solutions (Automox, ManageEngine)", "CSPM tools", "Vulnerability management platforms (Tenable, Rapid7)", "SCA tools (Snyk, Mend)"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["Any technique exploiting software vulnerability or misconfiguration (e.g., AML.T0009.001 ML Code Injection, AML.T0011 Initial Access)", "AML.T0021 Erode ML Model Integrity (if due to vulnerability exploitation)"] }, { "framework": "MAESTRO", "items": ["Re-exploitation of vulnerabilities in any layer (L1-L4)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM03:2025 Supply Chain (patching vulnerable component)", "LLM05:2025 Improper Output Handling (patching downstream component)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML06:2023 AI Supply Chain Attacks (if vulnerable library was entry point)"] }] },
                { "id": "AID-E-005", "name": "Secure Communication & Session Re-establishment for AI", "description": "After an incident where communication channels or user/agent sessions related to AI systems might have been compromised, hijacked, or exposed to malicious influence, take steps to securely re-establish these communications. This involves expiring all potentially tainted active sessions, forcing re-authentication for users and agents, clearing any manipulated conversational states, and ensuring that interactions resume over verified, secure channels. The goal is to prevent attackers from leveraging residual compromised sessions or states.", "implementationStrategies": ["Expire all active user sessions and API tokens/session cookies.", "Invalidate/regenerate session tokens for AI agents.", "Clear persistent conversational histories/cached states for affected agents.", "Ensure re-established sessions use strong authentication (MFA) and encryption (HTTPS/TLS).", "Communicate session reset to legitimate users as a security measure.", "Monitor newly established sessions for re-compromise."], "toolsOpenSource": ["Application server admin interfaces for session expiration", "Custom scripts with JWT libraries or flushing session stores (Redis, Memcached)", "IAM systems (Keycloak) with session termination APIs"], "toolsCommercial": ["IDaaS platforms (Okta, Auth0) for session termination", "API Gateways with advanced session management", "Customer communication platforms (Twilio, SendGrid)"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0012 Valid Accounts / AML.T0011 Initial Access (evicting hijacked sessions)", "AML.T0017 Persistence (if relying on active session/manipulated state)"] }, { "framework": "MAESTRO", "items": ["Agent Identity Attack (L7, forcing re-auth and clearing state)", "Session Hijacking affecting any AI layer"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM01:2025 Prompt Injection (clearing manipulated states)", "LLM02:2025 Sensitive Information Disclosure (stopping leaks from ongoing sessions)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["Any attack involving session hijacking or manipulation of ongoing ML API interactions."] }] }
            ]
        },
        {
            "name": "Restore",
            "purpose": "The \"Restore\" tactic focuses on recovering normal AI system operations and data integrity following an attack and subsequent eviction of the adversary. This phase involves safely bringing AI models and applications back online, restoring any corrupted or lost data from trusted backups, and, crucially, learning from the incident to reinforce defenses and improve future resilience.",
            "techniques": [
                { "id": "AID-R-001", "name": "Secure AI Model Restoration & Retraining", "description": "After an incident that may have compromised AI model integrity (e.g., through data poisoning, model poisoning, backdoor insertion, or unauthorized modification), securely restore affected models to a known-good state. This may involve deploying models from trusted, verified backups taken prior to the incident, or, if necessary, retraining or fine-tuning models on clean, validated datasets to eliminate any malicious influence or corruption.", "implementationStrategies": ["Maintain versioned, checksummed backups of production AI models.", "Replace compromised model with latest known-good backup, verifying integrity.", "If training data poisoned, remove poisoned data and retrain/fine-tune the affected model(s) using the cleansed dataset, ensuring the retraining process itself adheres to Secure & Resilient Training Process Hardening (AID-H-007) principles to prevent re-introduction of vulnerabilities.", "If model backdoored, revert to clean version or retrain from scratch.", "Thoroughly validate model performance, behavior, and security post-restoration.", "Document restoration process."], "toolsOpenSource": ["MLOps platforms (MLflow, Kubeflow Pipelines, DVC)", "Delta Lake (for time travel on datasets)", "Standard backup/recovery tools for model artifacts"], "toolsCommercial": ["Enterprise MLOps platforms (Databricks, SageMaker, Vertex AI, Azure ML)", "Data backup/recovery solutions"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0018 Backdoor ML Model / AML.T0019 Poison ML Model", "AML.T0020 Poison Training Data", "AML.T0021 Erode ML Model Integrity"] }, { "framework": "MAESTRO", "items": ["Backdoor Attacks (L1)", "Data Poisoning (L2, retraining)", "Model Skewing (L2, restoring/retraining)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM04:2025 Data and Model Poisoning"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML10:2023 Model Poisoning", "ML02:2023 Data Poisoning Attack"] }] },
                { "id": "AID-R-002", "name": "Data Integrity Recovery for AI Systems", "description": "Restore the integrity of any datasets used by or generated by AI systems that were corrupted, tampered with, or maliciously altered during a security incident. This includes training data, validation data, vector databases for RAG, embeddings stores, configuration data, or logs of AI outputs. Recovery typically involves reverting to known-good backups, using data validation tools to identify and correct inconsistencies, or, in some cases, reconstructing data if backups are insufficient or also compromised.", "implementationStrategies": ["Identify all affected data stores.", "Restore data from most recent, verified backups.", "If backups unavailable, attempt reconstruction/repair (data validation tools, log analysis).", "Re-validate integrity and consistency of recovered data.", "Update data ingestion/processing pipelines to prevent recurrence."], "toolsOpenSource": ["Database backup/restore utilities (pg_dump, mysqldump)", "Cloud provider snapshot/backup services (S3 versioning, Azure Blob snapshots)", "Great Expectations", "Filesystem backup tools (rsync, Bacula)", "Vector DB export/import utilities"], "toolsCommercial": ["Enterprise backup/recovery solutions (Rubrik, Cohesity, Veeam)", "Data quality/integration platforms (Informatica, Talend)", "Cloud provider managed backup services (AWS Backup, Azure Backup)"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["AML.T0020 Poison Training Data (restoring clean dataset)", "AML.T0021 Erode ML Model Integrity (restoring corrupted data stores)"] }, { "framework": "MAESTRO", "items": ["Data Poisoning / Data Tampering (L2)", "Compromised RAG Pipelines (L2, restoring vector DBs)"] }, { "framework": "OWASP LLM Top 10 2025", "items": ["LLM04:2025 Data and Model Poisoning (restoring dataset integrity)", "LLM08:2025 Vector and Embedding Weaknesses (if vector DBs corrupted)"] }, { "framework": "OWASP ML Top 10 2023", "items": ["ML02:2023 Data Poisoning Attack (restoring clean training data)"] }] },
                { "id": "AID-R-003", "name": "Post-Incident AI System Reinforcement & Testing", "description": "Following recovery from a security incident, conduct a thorough review of the attack, the system's response, and the effectiveness of existing defenses. Based on these lessons learned, reinforce security controls, update threat models, and perform rigorous testing (including penetration testing or red teaming specifically targeting the previous attack vector and similar ones) to confirm that vulnerabilities have been addressed and the AI system is more resilient against future, similar attacks.", "implementationStrategies": ["Conduct detailed post-incident review (PIR) / root cause analysis (RCA).", "Update AI threat models (AID-M-004).", "Implement/enhance defensive techniques based on PIR findings.", "Perform targeted security testing (pen testing, AI red teaming).", "Validate effectiveness of patches and hardening measures.", "Update incident response plans and playbooks."], "toolsOpenSource": ["MITRE ATLAS Navigator", "OWASP AI Security & Privacy Guide, OWASP LLM/ML Top 10s", "AI red teaming frameworks (Counterfit, Garak, vigil-llm)", "Vulnerability scanners"], "toolsCommercial": ["AI red teaming services", "Breach and Attack Simulation (BAS) platforms", "Booz Allen's Atlas Notebook", "Immersive Labs", "Commercial penetration testing services"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["Recurrence of same/similar attack techniques by closing gaps. Improves resilience against all ATLAS tactics."] }, { "framework": "MAESTRO", "items": ["Future attacks exploiting similar vulnerabilities across any MAESTRO layer."] }, { "framework": "OWASP LLM Top 10 2025", "items": ["Helps prevent re-exploitation of any LLM Top 10 vulnerabilities."] }, { "framework": "OWASP ML Top 10 2023", "items": ["Helps prevent re-exploitation of any ML Top 10 vulnerabilities."] }] },
                { "id": "AID-R-004", "name": "Stakeholder Notification & AI Incident Knowledge Sharing", "description": "After an AI security incident has been contained, remediated, and systems restored, inform relevant internal and external stakeholders (e.g., developers, users, customers, partners, regulatory bodies if required) about the incident (to the appropriate level of detail), the resolution steps taken, and measures implemented to prevent recurrence. Where appropriate and feasible, share anonymized or generalized learnings, IoCs, or novel attack vector information with the broader AI security community (e.g., ISACs, MITRE ATLAS, OWASP) to help improve collective defense.", "implementationStrategies": ["Develop communication plan for AI security incidents.", "Provide factual post-mortem report to internal teams; summary for external stakeholders.", "Follow legal/compliance requirements for notification (data breach, regulations).", "Consider sharing non-sensitive technical details with trusted communities.", "Update internal documentation (model cards, architecture diagrams, risk assessments).", "Use incident as case study for internal training/awareness."], "toolsOpenSource": ["Incident response plan templates (SANS, NIST)", "Security community mailing lists/forums (FIRST.org, OWASP)", "MISP (Malware Information Sharing Platform)"], "toolsCommercial": ["GRC platforms for incident reporting/notifications", "Threat intelligence sharing platforms", "Secure communication platforms", "Public relations/crisis communication services", "Bridgecrew, RiskRecon (compliance reporting)"], "defendsAgainst": [{ "framework": "MITRE ATLAS", "items": ["Indirectly defends against future attacks by community knowledge sharing. Helps manage 'Impact' phase (reputational, legal)."] }, { "framework": "MAESTRO", "items": ["Improves ecosystem resilience if learnings shared. Addresses L6: Security & Compliance."] }, { "framework": "OWASP LLM Top 10 2025", "items": ["Facilitates better community understanding and defense against LLM Top 10 risks."] }, { "framework": "OWASP ML Top 10 2023", "items": ["Improves collective defense against ML-specific risks."] }] }
            ]
        }
    ]
};

        
// --- DOM Elements ---
        const mainContentEl = document.getElementById('main-content');
        const searchBarEl = document.getElementById('search-bar');
        const searchClearBtnEl = document.getElementById('searchClearBtn');
        const modalEl = document.getElementById('infoModal');
        const modalBackdropEl = document.getElementById('modalBackdrop');
        const modalBodyEl = document.getElementById('modalBody');
        const modalCloseBtn = document.getElementById('modalClose');
        const themeToggleBtn = document.getElementById('themeToggleBtn');
        const aboutBtn = document.getElementById('aboutBtn');
        const htmlEl = document.documentElement;

        // --- SVG Icons ---
        const plusIcon = `<svg class="accordion-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"><path d="M10.75 4.75a.75.75 0 00-1.5 0v4.5h-4.5a.75.75 0 000 1.5h4.5v4.5a.75.75 0 001.5 0v-4.5h4.5a.75.75 0 000-1.5h-4.5v-4.5z" /></svg>`;
        const minusIcon = `<svg class="accordion-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M4 10a.75.75 0 01.75-.75h10.5a.75.75 0 010 1.5H4.75A.75.75 0 014 10z" clip-rule="evenodd" /></svg>`;
        const sunIcon = `<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M12 2.25a.75.75 0 01.75.75v2.25a.75.75 0 01-1.5 0V3a.75.75 0 01.75-.75zM7.5 12a4.5 4.5 0 119 0 4.5 4.5 0 01-9 0zM18.894 6.166a.75.75 0 00-1.06-1.06l-1.591 1.59a.75.75 0 101.06 1.061l1.591-1.59zM21.75 12a.75.75 0 01-.75.75h-2.25a.75.75 0 010-1.5H21a.75.75 0 01.75.75zM17.834 18.894a.75.75 0 001.06-1.06l-1.59-1.591a.75.75 0 10-1.061 1.06l1.59 1.591zM12 18a.75.75 0 01.75.75V21a.75.75 0 01-1.5 0v-2.25A.75.75 0 0112 18zM7.758 17.303a.75.75 0 00-1.061-1.06l-1.591 1.59a.75.75 0 001.06 1.061l1.591-1.59zM6 12a.75.75 0 01-.75.75H3a.75.75 0 010-1.5h2.25A.75.75 0 016 12zM6.166 7.758a.75.75 0 001.06 1.06l1.591-1.59a.75.75 0 00-1.06-1.061L6.166 7.758z" /></svg>`;
        const moonIcon = `<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 01.162.819A8.97 8.97 0 009 6a9 9 0 009 9 8.97 8.97 0 003.463-.69.75.75 0 01.981.98 10.503 10.503 0 01-9.694 6.46c-5.799 0-10.5-4.701-10.5-10.5 0-3.51 1.713-6.636 4.362-8.492a.75.75 0 01.819.162z" clip-rule="evenodd" /></svg>`;
        const infoIcon = `<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M18 10a8 8 0 1 1-16 0 8 8 0 0 1 16 0ZM9 9a.75.75 0 0 0 0 1.5h.253a.25.25 0 0 1 .244.304l-.459 2.066A1.75 1.75 0 0 0 10.747 15H11a.75.75 0 0 0 0-1.5h-.253a.25.25 0 0 1-.244-.304l.459-2.066A1.75 1.75 0 0 0 9.253 9H9Z" clip-rule="evenodd" /></svg>`;

        // --- State & API Logic ---
        let isGeminiAvailable = true;
        let geminiCooldownEndTime = 0;

        async function callGeminiAPI(prompt) {
            if (!isGeminiAvailable && Date.now() < geminiCooldownEndTime) {
                alert("The AI feature is temporarily unavailable due to high demand. It will become available again after the daily quota resets.");
                return null;
            }

            const cloudFunctionUrl = 'https://gemini-proxy-859350701171.us-west1.run.app';

            try {
                const response = await fetch(cloudFunctionUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ prompt: prompt })
                });

                if (response.status === 429) {
                    console.error("Gemini API quota exceeded (relayed by proxy).");
                    isGeminiAvailable = false;
                    const now = new Date();
                    const tomorrow = new Date(now);
                    tomorrow.setDate(now.getDate() + 1);
                    tomorrow.setUTCHours(8, 0, 0, 0); // Reset at Midnight PST (8 AM UTC)
                    geminiCooldownEndTime = tomorrow.getTime();
                    disableGeminiFeatures();
                    alert("The AI feature has reached its daily usage limit. It will be available again tomorrow.");
                    return null;
                }

                if (!response.ok) {
                    const errorData = await response.json();
                    throw new Error(`API Error: ${errorData.error || 'Unknown error'}`);
                }

                isGeminiAvailable = true;
                enableGeminiFeatures();
                const result = await response.json();
                return result.text;
            } catch (error) {
                console.error("Failed to call the Cloud Function:", error);
                alert("An error occurred while contacting the AI assistant. Please check the console for details.");
                return null;
            }
        }

        function disableGeminiFeatures() {
            document.querySelectorAll('.gemini-helper-btn').forEach(btn => {
                btn.disabled = true;
                btn.style.cursor = 'not-allowed';
                btn.style.opacity = '0.5';
                btn.title = "AI feature unavailable due to daily quota.";
            });
        }

        function enableGeminiFeatures() {
            document.querySelectorAll('.gemini-helper-btn').forEach(btn => {
                btn.disabled = false;
                btn.style.cursor = 'pointer';
                btn.style.opacity = '1';
                btn.title = "Get implementation help with AI";
            });
        }

        setInterval(() => {
            if (!isGeminiAvailable && Date.now() > geminiCooldownEndTime) {
                isGeminiAvailable = true;
                enableGeminiFeatures();
            }
        }, 5 * 60 * 1000);

        // --- Render and Modal Functions ---
        function renderMainGrid(searchTerm = "") {
             mainContentEl.innerHTML = ''; 
             const gridContainer = document.createElement('div');
             gridContainer.className = 'tactic-column-grid';

             const allTechniquesFlat = searchTerm ? aidefendData.tactics.reduce((acc, tactic) => {
                 if (tactic.techniques && Array.isArray(tactic.techniques)) {
                     tactic.techniques.forEach(tech => acc.push({...tech, tacticName: tactic.name }));
                 }
                 return acc;
             }, []) : [];
            
             const techniquesMatchingGlobalSearch = searchTerm ? allTechniquesFlat.filter(tech => {
                 const term = searchTerm.toLowerCase();
                 let match = tech.name.toLowerCase().includes(term) ||
                                 tech.id.toLowerCase().includes(term) ||
                                 (tech.description && tech.description.toLowerCase().includes(term));
                 if (!match && tech.defendsAgainst) {
                     for (const da of tech.defendsAgainst) {
                         if (da.items && da.items.some(item => item.toLowerCase().includes(term))) {
                             match = true;
                             break;
                         }
                     }
                 }
                 return match;
             }) : [];


             if (searchTerm && techniquesMatchingGlobalSearch.length === 0) {
                 mainContentEl.innerHTML = `<p class="text-center opacity-80 py-10">No techniques found matching "${searchTerm}".</p>`;
                 return; 
             }

             aidefendData.tactics.forEach(tactic => {
                 let techniquesInThisTacticThatMatchSearch = [];
                 let columnShouldBeRendered = !searchTerm; 

                 if (searchTerm) {
                     if (tactic.techniques && Array.isArray(tactic.techniques)) {
                         techniquesInThisTacticThatMatchSearch = tactic.techniques.filter(t => 
                             techniquesMatchingGlobalSearch.some(matchedTech => matchedTech.id === t.id)
                         );
                         if (techniquesInThisTacticThatMatchSearch.length > 0) {
                             columnShouldBeRendered = true;
                         }
                     } else {
                          columnShouldBeRendered = false; 
                     }
                 }

                 if (!columnShouldBeRendered) {
                     return; 
                 }

                 const column = document.createElement('div');
                 column.className = 'tactic-column elevation-2'; 
                 
                 const tacticHeader = document.createElement('h3');
                 tacticHeader.className = 'tactic-column-header';
                 tacticHeader.textContent = tactic.name;
                 tacticHeader.onclick = () => showTacticModal(tactic);
                 column.appendChild(tacticHeader);
                 
                 const ul = document.createElement('ul');
                 let techniquesToRenderInColumn;

                if (searchTerm) {
                     techniquesToRenderInColumn = techniquesInThisTacticThatMatchSearch;
                } else {
                     techniquesToRenderInColumn = (tactic.techniques && Array.isArray(tactic.techniques)) ? tactic.techniques : [];
                }

               if (techniquesToRenderInColumn.length > 0) {
                    techniquesToRenderInColumn.forEach(tech => {
                        const li = document.createElement('li');
                       
                        const a = document.createElement('a');
                        a.href = '#';
                        a.className = 'technique-item elevation-2';
                        a.innerHTML = `<span class="technique-id">${tech.id}</span> <span class="technique-name">${tech.name}</span>`;
                       
                        if (searchTerm && techniquesMatchingGlobalSearch.some(matchedTech => matchedTech.id === tech.id)) {
                            a.classList.add('highlight');
                        }

                        a.onclick = (e) => {
                            e.preventDefault();
                            const originalTactic = aidefendData.tactics.find(originalT => originalT.name === tactic.name);
                            const originalTechnique = originalTactic?.techniques.find(origTech => origTech.id === tech.id);
                            showTechniqueModal(originalTechnique || tech, tactic.name);
                        };

                        li.appendChild(a);

                        if (tech.subTechniques && tech.subTechniques.length > 0) {
                            const subList = document.createElement('ul');
                            subList.className = 'pl-3 pr-1 py-1 mt-1 space-y-2'; 
                            
                            tech.subTechniques.forEach(subTech => {
                                const subLi = document.createElement('li');
                                const subA = document.createElement('a');
                                subA.href = '#';
                                subA.className = 'technique-item elevation-2 block !bg-[var(--technique-item-hover-bg)] !text-[var(--technique-item-hover-text)]';
                                subA.innerHTML = `<span class="technique-id !text-[var(--technique-id-text)]">${subTech.id}</span> <span class="technique-name">${subTech.name}</span>`;

                                if (searchTerm) {
                                    subA.classList.add('highlight');
                                }
                               
                                subA.onclick = (e) => {
                                    e.preventDefault();
                                    e.stopPropagation(); // Prevent parent click
                                    showTechniqueModal(subTech, tactic.name);
                                };
                                subLi.appendChild(subA);
                                subList.appendChild(subLi);
                            });
                           
                            li.appendChild(subList);
                        }

                        ul.appendChild(li);
                    });
                } else if (!searchTerm && (!tactic.techniques || tactic.techniques.length === 0)) {
                    const p = document.createElement('p');
                    p.className = 'text-xs opacity-60 italic px-2';
                    p.textContent = 'No techniques defined yet for this tactic.';
                    ul.appendChild(p);
                }
               
                 column.appendChild(ul);
                 gridContainer.appendChild(column);
             });
             mainContentEl.appendChild(gridContainer);
        }
        function showIntroductionModal() {
            const intro = aidefendData.introduction;
            let contentHtml = `<h2 class="modal-main-title">${intro.mainTitle}</h2>`;
            contentHtml += '<div class="accordion-container">'; 
            intro.sections.forEach((section, index) => {
                contentHtml += `
                    <div class="accordion-item">
                        <div class="accordion-header" role="button" aria-expanded="${index === 0 ? 'true' : 'false'}">
                            <span>${section.title}</span>
                            <div class="icon-wrapper">${plusIcon}</div>
                        </div>
                        <div class="accordion-content">
                            <div class="accordion-content-inner">
                                ${section.paragraphs ? section.paragraphs.map(p => `<p>${p}</p>`).join('') : ''}
                                ${section.listItems ? `<ul>${section.listItems.map(item => `<li>${item}</li>`).join('')}</ul>` : ''}
                                ${section.concludingParagraphs ? section.concludingParagraphs.map(p => `<p>${p}</p>`).join('') : ''}
                            </div>
                        </div>
                    </div>
                `;
            });
            contentHtml += '</div>';
            modalBodyEl.innerHTML = contentHtml;
            const accordionHeaders = modalBodyEl.querySelectorAll('.accordion-header');
            accordionHeaders.forEach((header, index) => {
                const content = header.nextElementSibling;
                const iconWrapper = header.querySelector('.icon-wrapper');
                if (index === 0) {
                    header.classList.add('active');
                    content.style.maxHeight = content.scrollHeight + 'px';
                    iconWrapper.innerHTML = minusIcon; 
                }
                header.addEventListener('click', () => {
                    const isActive = header.classList.contains('active');
                    accordionHeaders.forEach(otherHeader => {
                        otherHeader.classList.remove('active');
                        otherHeader.nextElementSibling.style.maxHeight = null;
                        otherHeader.querySelector('.icon-wrapper').innerHTML = plusIcon;
                    });
                    if (!isActive) {
                        header.classList.add('active');
                        content.style.maxHeight = content.scrollHeight + 'px';
                        iconWrapper.innerHTML = minusIcon; 
                    }
                });
            });
            modalEl.classList.add('active');
            document.body.classList.add('modal-open');
        }
        function showTacticModal(tactic) { 
            const fullTacticData = aidefendData.tactics.find(t => t.name === tactic.name) || tactic;
            modalBodyEl.innerHTML = `
                <h2>${fullTacticData.name}</h2>
                <p class="mb-4 leading-relaxed text-sm">${fullTacticData.purpose || 'No purpose description available.'}</p>
            `;
            modalEl.classList.add('active');
            document.body.classList.add('modal-open');
        }
        function renderDetailSectionListForModal(title, items) {
            if (!items || items.length === 0) return '';
            return `
                <div class="mt-3">
                    <h4 class="font-semibold text-sm mb-1">${title}:</h4>
                    <ul class="list-disc list-inside ml-4 text-xs opacity-90 space-y-1">
                        ${items.map(item => `<li>${item}</li>`).join('')}
                    </ul>
                </div>
            `;
        }
        function hideModal() { 
            modalEl.classList.remove('active');
            document.body.classList.remove('modal-open');
            modalBodyEl.innerHTML = ''; 
        }
        function applyTheme(theme) {
            if (theme === 'dark') {
                htmlEl.classList.add('dark');
                themeToggleBtn.innerHTML = sunIcon;
            } else {
                htmlEl.classList.remove('dark');
                themeToggleBtn.innerHTML = moonIcon;
            }
        }
        function toggleTheme() {
            const currentTheme = htmlEl.classList.contains('dark') ? 'dark' : 'light';
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            localStorage.setItem('aidefendTheme', newTheme);
            applyTheme(newTheme);
        }
        function showTechniqueModal(technique, tacticName) {
            const geminiIcon = `<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" class="w-4 h-4 mr-2"><path d="M14.228 2.314a.75.75 0 00-.956.4L9.5 9h5l-3.772 6.686a.75.75 0 00.956-.4L14.5 9h-5l3.772-6.686z"/><path fill-rule="evenodd" d="M3.75 12a8.25 8.25 0 1116.5 0 8.25 8.25 0 01-16.5 0zM12 2.25a9.75 9.75 0 100 19.5 9.75 9.75 0 000-19.5z" clip-rule="evenodd"/></svg>`;
            modalBodyEl.innerHTML = `
                <p class="text-sm opacity-80 mb-1">Tactic: ${tacticName}</p>
                <h2 id="technique-modal-title">${technique.id}: ${technique.name}</h2>
                <p class="mb-4 leading-relaxed text-sm" id="technique-modal-desc">${technique.description || 'No description available.'}</p>
                <div id="gemini-response-container"></div>
                <div class="mt-4 pt-3 border-t border-[var(--column-border)]">
                    <div class="flex items-center">
                        <h3 class="font-semibold text-lg">Implementation Guidance</h3>
                        <button id="implementationHelperBtn" class="gemini-helper-btn" title="Get implementation help with AI">${geminiIcon}Get Code Example</button>
                    </div>
                    ${renderDetailSectionListForModal('Implementation Strategies', technique.implementationStrategies)}
                    ${renderDetailSectionListForModal('Potential Tools - Open Source', technique.toolsOpenSource)}
                    ${renderDetailSectionListForModal('Potential Tools - Commercial', technique.toolsCommercial)}
                </div>
                <div class="mt-4 pt-3 border-t border-[var(--column-border)]">
                     <div class="flex items-center">
                        <h3 class="font-semibold text-lg">Defends Against</h3>
                        <button id="defenseHelperBtn" class="gemini-helper-btn" title="Explain how this defends against threats">${geminiIcon}Explain Defenses</button>
                    </div>
                    ${technique.defendsAgainst && technique.defendsAgainst.length > 0 ? 
                        technique.defendsAgainst.map(da => `
                            <div class="mb-2 mt-2">
                                <p class="defends-against-framework text-sm">${da.framework}:</p>
                                <ul class="list-disc list-inside ml-4 text-xs opacity-90 space-y-0.5">
                                    ${da.items.map(item => `<li>${item}</li>`).join('')}
                                </ul>
                            </div>
                        `).join('') : '<p class="text-xs opacity-60 italic mt-2">No specific defenses listed.</p>'}
                </div>
            `;
            const implementationBtn = document.getElementById('implementationHelperBtn');
            const defenseBtn = document.getElementById('defenseHelperBtn');
            const responseContainer = document.getElementById('gemini-response-container');
            if (!isGeminiAvailable) { disableGeminiFeatures(); }
            const handleButtonClick = async (button, promptGenerator) => {
                document.querySelectorAll('.gemini-helper-btn').forEach(btn => btn.disabled = true);
                button.innerHTML = `<div class="loader inline-block mr-2" style="width:16px;height:16px;border-width:2px;"></div><span>Generating...</span>`;
                responseContainer.style.display = 'block';
                responseContainer.innerHTML = '<div class="loader"></div>';
                const prompt = promptGenerator(technique);
                const responseText = await callGeminiAPI(prompt);
                if (responseText) {
                    let html = responseText.replace(/```(python|yaml|bash|sh|javascript|json|text)?\s*\n([\s\S]*?)\n```/g, (match, lang, code) => `<pre><code class="language-${lang || 'text'}">${code.replace(/</g, '&lt;').replace(/>/g, '&gt;')}</code></pre>`).replace(/`([^`]+)`/g, `<code>$1</code>`).replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>').replace(/\*(.*?)\*/g, '<em>$1</em>').replace(/^### (.*$)/gim, '<h4>$1</h4>').replace(/^## (.*$)/gim, '<h3>$1</h3>').replace(/^\* (.*$)/gim, '<ul><li>$1</li></ul>').replace(/\n/g, '<br>').replace(/<br><ul>/g, '<ul>').replace(/<\/li><\/ul><br>/g, '</li></ul>');
                    responseContainer.innerHTML = `<div class="prose dark:prose-invert max-w-none">${html}</div>`;
                } else {
                    responseContainer.innerHTML = '<p class="text-xs text-center opacity-70">AI helper is currently unavailable. Please try again later.</p>';
                }
                if (isGeminiAvailable) { document.querySelectorAll('.gemini-helper-btn').forEach(btn => btn.disabled = false); }
                implementationBtn.innerHTML = `${geminiIcon}Get Code Example`;
                defenseBtn.innerHTML = `${geminiIcon}Explain Defenses`;
            };
            implementationBtn.addEventListener('click', () => {
                const prompt = (tech) => `You are an expert AI security implementation assistant. For the AI defense technique "${tech.name}" (${tech.id}), provide a concise, actionable code snippet or configuration file example in Python or YAML. Use one of the open-source tools mentioned if possible: ${(tech.toolsOpenSource || []).join(', ')}. Format the response in simple Markdown with a brief explanation followed by the code block.`;
                handleButtonClick(implementationBtn, prompt);
            });
            defenseBtn.addEventListener('click', () => {
                const prompt = (tech) => `You are an expert AI security analyst. For the AI defense technique "${tech.name}" (${tech.id}), explain **how** it specifically defends against the listed threats. Be concise and clear. Threats to explain: ${(tech.defendsAgainst || []).map(da => `- ${da.framework}: ${da.items.join(', ')}`).join('\n')} Provide your analysis below in simple Markdown:`;
                handleButtonClick(defenseBtn, prompt);
            });
            modalEl.classList.add('active');
            document.body.classList.add('modal-open');
        }

        // --- Event Listeners ---
        themeToggleBtn.addEventListener('click', toggleTheme);
        aboutBtn.addEventListener('click', showIntroductionModal);
        searchBarEl.addEventListener('input', (e) => {
            currentSearchTerm = e.target.value.trim();
            searchClearBtnEl.style.display = currentSearchTerm ? 'block' : 'none';
            renderMainGrid(currentSearchTerm);
        });
        searchClearBtnEl.addEventListener('click', () => {
            searchBarEl.value = '';
            currentSearchTerm = '';
            searchClearBtnEl.style.display = 'none';
            renderMainGrid();
        });
        modalCloseBtn.addEventListener('click', hideModal);
        modalBackdropEl.addEventListener('click', hideModal); 
        document.addEventListener('keydown', (event) => {
            if (event.key === 'Escape' && modalEl.classList.contains('active')) {
                hideModal();
            }
        });


        // --- Initial Load ---
        const savedTheme = localStorage.getItem('aidefendTheme') || 'dark'; 
        applyTheme(savedTheme);
        aboutBtn.innerHTML = infoIcon + "About"; 

        renderMainGrid();
    </script>
</body>
</html>