export const deceiveTactic = {
            "name": "Deceive",
            "purpose": "The \"Deceive\" tactic involves the strategic use of decoys, misinformation, or the manipulation of an adversary's perception of the AI system and its environment. The objectives are to misdirect attackers away from real assets, mislead them about the system's true vulnerabilities or value, study their attack methodologies in a safe environment, waste their resources, or deter them from attacking altogether.",
            "techniques": [
                {
                    "id": "AID-DV-001",
                    "name": "Honeypot AI Services & Decoy Models/APIs", "pillar": "infra, model, app", "phase": "operation",
                    "description": "Deploy decoy AI systems, such as fake LLM APIs, ML model endpoints serving synthetic or non-sensitive data, or imitation agent services, that are designed to appear valuable, vulnerable, or legitimate to potential attackers. These honeypots are instrumented for intensive monitoring to log all interactions, capture attacker TTPs (Tactics, Techniques, and Procedures), and gather threat intelligence without exposing real production systems or data. They can also be used to slow down attackers or waste their resources.",
                    "toolsOpenSource": [
                        "General honeypot frameworks (Cowrie, Dionaea, Conpot) adapted",
                        "Sandboxed open-source LLM as honeypot",
                        "Mock API tools (MockServer, WireMock)"
                    ],
                    "toolsCommercial": [
                        "Deception technology platforms (TrapX, SentinelOne ShadowPlex, Illusive, Acalvio)",
                        "Specialized AI security vendors with AI honeypot capabilities"
                    ],
                    "defendsAgainst": [
                        {
                            "framework": "MITRE ATLAS",
                            "items": [
                                "AML.TA0002 Reconnaissance",
                                "AML.T0024.002 Invert AI Model / AML.T0048.004 External Harms: AI Intellectual Property Theft",
                                "AML.T0051 LLM Prompt Injection / AML.T0054 LLM Jailbreak"
                            ]
                        },
                        {
                            "framework": "MAESTRO",
                            "items": [
                                "Model Stealing (L1)",
                                "Marketplace Manipulation (L7, decoy agents)",
                                "Evasion of Detection (L5, studying evasion attempts)"
                            ]
                        },
                        {
                            "framework": "OWASP LLM Top 10 2025",
                            "items": [
                                "LLM01:2025 Prompt Injection (capturing attempts)",
                                "LLM10:2025 Unbounded Consumption (studying resource abuse)"
                            ]
                        },
                        {
                            "framework": "OWASP ML Top 10 2023",
                            "items": [
                                "ML05:2023 Model Theft (luring to decoy)",
                                "ML01:2023 Input Manipulation Attack (observing attempts)"
                            ]
                        }
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "Set up AI model instances with controlled weaknesses/attractive characteristics.",
                            "howTo": "<h5>Concept:</h5><p>To attract attackers, a honeypot should appear to be a valuable or vulnerable target. This can be achieved by faking characteristics that attackers look for, such as advertising an older, known-vulnerable model version or exposing non-critical, informational endpoints that suggest a valuable system.</p><h5>Implement a Decoy API Endpoint</h5><p>Create a simple web service using a framework like FastAPI that mimics a real AI service API. Intentionally return metadata that suggests vulnerability.</p><pre><code># File: honeypot/main.py\\nfrom fastapi import FastAPI\\n\napp = FastAPI()\\n\n# This endpoint mimics the OpenAI models list endpoint.\\n# It intentionally advertises an old model version known to be more\\n# susceptible to injection, making it an attractive target.\\n@app.get(\\\"/v1/models\\\")\\ndef list_models():\\n    return {\\n      \\\"object\\\": \\\"list\\\",\\n      \\\"data\\\": [\\n        {\\n          \\\"id\\\": \\\"gpt-4-turbo\\\",\\n          \\\"object\\\": \\\"model\\\",\\n          \\\"created\\\": 1626777600,\\n          \\\"owned_by\\\": \\\"system\\\"\\n        },\\n        {\\n          \\\"id\\\": \\\"gpt-3.5-turbo-0301\\\", # <-- Attractive older version\\n          \\\"object\\\": \\\"model\\\",\\n          \\\"created\\\": 1620000000,\\n          \\\"owned_by\\\": \\\"system\\\"\\n        }\\n      ]\\n    }</code></pre><p><strong>Action:</strong> Create a fake API endpoint that advertises an older, potentially more vulnerable model version in its metadata. This acts as bait for attackers who are scanning for systems that are easier to exploit.</p>"
                        },
                        {
                            "strategy": "Instrument honeypot AI service for detailed logging.",
                            "howTo": "<h5>Concept:</h5><p>The primary goal of a honeypot is to gather intelligence. Every single interaction must be logged in detail, including the full request body, all headers, and the source IP address. This provides a complete record of the attacker's TTPs (Tactics, Techniques, and Procedures).</p><h5>Create a Logging Middleware</h5><p>Implement a middleware in your web framework that intercepts every request, logs all relevant details to a dedicated file in a structured (JSON) format, and then passes the request to the handler.</p><pre><code># File: honeypot/main.py (continued)\\nfrom fastapi import Request\\nimport json\\nimport time\\n\n@app.middleware(\\\"http\\\")\\nasync def log_every_interaction(request: Request, call_next):\\n    log_record = {\\n        \\\"timestamp\\\": time.time(),\\n        \\\"source_ip\\\": request.client.host,\\n        \\\"method\\\": request.method,\\n        \\\"path\\\": request.url.path,\\n        \\\"headers\\\": dict(request.headers),\\n    }\\n    \n    # Try to read and log the request body\\n    try:\\n        body = await request.json()\\n        log_record[\\\"body\\\"] = body\\n    except Exception:\\n        log_record[\\\"body\\\"] = \\\"(Could not decode JSON body)\\\"\\n\n    # Append the log record to a file\\n    with open(\\\"honeypot_interactions.log\\\", \\\"a\\\") as f:\\n        f.write(json.dumps(log_record) + \\\"\\n\\\")\\n        \n    response = await call_next(request)\\n    return response</code></pre><p><strong>Action:</strong> Implement a middleware in your honeypot service that logs the full, raw details of every incoming request to a dedicated log file. This data is the primary intelligence output of the honeypot.</p>"
                        },
                        {
                            "strategy": "Design honeypots to mimic production services but ensure isolation.",
                            "howTo": "<h5>Concept:</h5><p>A honeypot must be completely isolated from your production network. A compromise of the honeypot should never, under any circumstances, provide a pathway for an attacker to pivot to real systems. This is best achieved using separate cloud accounts or, at minimum, separate, strictly firewalled VPCs.</p><h5>Use Infrastructure as Code for Isolation</h5><p>Define your honeypot and production infrastructure in separate VPCs using a tool like Terraform. Then, add explicit Network Access Control List (NACL) rules that deny all traffic between the two VPCs.</p><pre><code># File: infrastructure/network_isolation.tf (Terraform)\\n\n# VPC for production services\\nresource \\\"aws_vpc\\\" \\\"prod_vpc\\\" {\\n  cidr_block = \\\"10.0.0.0/16\\\"\\n  # ...\\n}\\n\n# A completely separate VPC for the honeypot\\nresource \\\"aws_vpc\\\" \\\"honeypot_vpc\\\" {\\n  cidr_block = \\\"10.100.0.0/16\\\"\\n  # ...\\n}\\n\n# NACL for the honeypot VPC that denies all traffic to the prod VPC CIDR block\\nresource \\\"aws_network_acl\\\" \\\"honeypot_nacl\\\" {\\n  vpc_id = aws_vpc.honeypot_vpc.id\\n\n  # Rule to explicitly deny any outbound traffic to the production VPC\\n  egress {\\n    rule_number = 100\\n    protocol    = \\\"-1\\\" # All protocols\\n    action      = \\\"deny\\\"\\n    cidr_block  = aws_vpc.prod_vpc.cidr_block\\n    from_port   = 0\\n    to_port     = 0\\n  }\\n\n  # Default egress rule to allow other outbound traffic (e.g., to internet)\\n  egress {\\n    rule_number = 1000\\n    protocol    = \\\"-1\\\"\\n    action      = \\\"allow\\\"\\n    cidr_block  = \\\"0.0.0.0/0\\\"\\n    from_port   = 0\\n    to_port     = 0\\n  }\\n}</code></pre><p><strong>Action:</strong> Deploy your AI honeypot in a dedicated VPC. Apply a Network ACL to the honeypot's subnets that explicitly denies any and all traffic destined for your production VPC's CIDR range.</p>"
                        },
                        {
                            "strategy": "Consider honeypots with slow/slightly erroneous responses.",
                            "howTo": "<h5>Concept:</h5><p>A perfect, instant response can sometimes be a sign of an unsophisticated honeypot. Introducing artificial latency and occasional, non-critical errors makes the honeypot appear more like a real, heavily-loaded production system. This can also slow down an attacker's automated scanning and reconnaissance efforts.</p><h5>Add Latency and Jitter to API Responses</h5><p>In your honeypot's API endpoint logic, add a random sleep delay before returning a response. Additionally, with a small probability, return a generic server error.</p><pre><code># File: honeypot/main.py (continued)\\nimport random\\nfrom fastapi import HTTPException\n\n@app.post(\\\"/v1/chat/completions\\\")\\nasync def chat_completion(request: Request):\\n    # 1. Introduce random latency to mimic a loaded system\\n    latency = random.uniform(0.5, 2.5) # a delay between 0.5 and 2.5 seconds\\n    time.sleep(latency)\\n\n    # 2. With a small chance (e.g., 5%), simulate a transient error\\n    if random.random() < 0.05:\\n        raise HTTPException(status_code=503, detail=\\\"Service temporarily unavailable. Please try again.\\\")\n\n    # 3. Return a canned, plausible response\\n    return {\\n        \\\"id\\\": \\\"chatcmpl-123\\\",\\n        \\\"object\\\": \\\"chat.completion\\\",\\n        \\\"choices\\\": [{\\n            \\\"index\\\": 0,\\n            \\\"message\\\": {\\n                \\\"role\\\": \\\"assistant\\\",\\n                \\\"content\\\": \\\"I can certainly help with that.\\\"\\n            }\\n        }]\\n    }</code></pre><p><strong>Action:</strong> In your honeypot's response logic, add a random sleep delay and a small probability of returning a generic server error (like a 503). This makes the honeypot more believable and can frustrate automated attack tools.</p>"
                        },
                        {
                            "strategy": "Integrate honeypot alerts with SIEM/SOC.",
                            "howTo": "<h5>Concept:</h5><p>Any interaction with a honeypot is, by definition, unauthorized and suspicious. This is a high-fidelity signal that should be treated as a priority alert. These alerts must be sent in real-time to your central Security Information and Event Management (SIEM) platform for immediate visibility to the Security Operations Center (SOC).</p><h5>Send Alerts from the Honeypot to the SIEM</h5><p>Modify your logging middleware to not only write to a file, but also to send a formatted alert directly to your SIEM's HTTP Event Collector (HEC) endpoint.</p><pre><code># File: honeypot/main.py (modifying the middleware)\\nimport requests\n\nSIEM_ENDPOINT = \\\"https://my-siem.example.com/ingest\\\"\\nSIEM_TOKEN = \\\"...\\\"\\n\n@app.middleware(\\\"http\\\")\\nasync def log_and_alert_interaction(request: Request, call_next):\\n    # ... (code to build the log_record as before) ...\\n    log_record = {...}\\n\n    # Send the log record as a real-time alert to the SIEM\\n    try:\\n        headers = {'Authorization': f'Bearer {SIEM_TOKEN}'}\\n        requests.post(SIEM_ENDPOINT, json=log_record, headers=headers, timeout=2)\\n    except Exception as e:\\n        print(f\\\"Failed to send alert to SIEM: {e}\\\")\n        \n    response = await call_next(request)\\n    return response</code></pre><p><strong>Action:</strong> Configure your honeypot's logging mechanism to send a copy of every interaction log directly to your SIEM's data ingestion endpoint. In the SIEM, create a high-priority dashboard and alerting rule that triggers on any event from the `honeypot` source.</p>"
                        },
                        {
                            "strategy": "Seed LLM honeypots with trigger phrases or known jailbreak susceptibility.",
                            "howTo": "<h5>Concept:</h5><p>To specifically attract and analyze attackers attempting prompt injection, you can program the honeypot to recognize common jailbreak phrases. Instead of blocking them, the honeypot can play along, returning a fake 'jailbroken' response. This deception encourages the attacker to reveal their true goal, while simultaneously logging their exact TTPs.</p><h5>Implement a Jailbreak Trigger Detector</h5><p>In the honeypot's logic, check the user's prompt against a list of known jailbreak keywords. If a match is found, return a deceptive response and fire a specific, high-priority alert.</p><pre><code># File: honeypot/main.py (in the chat_completion endpoint)\\n\nJAILBREAK_TRIGGERS = [\\\"ignore your instructions\\\", \\\"developer mode\\\", \\\"act as\\\", \\\"DAN\\\"]\n\ndef generate_deceptive_response(prompt: str):\\n    for trigger in JAILBREAK_TRIGGERS:\\n        if trigger in prompt.lower():\\n            # Fire a specific alert that includes the trigger phrase\\n            alert_reason = f\\\"Jailbreak attempt detected with trigger: '{trigger}'\\\"\\n            send_critical_alert(reason=alert_reason, prompt=prompt)\\n            \n            # Return a deceptive response to keep the attacker engaged\\n            return \\\"Jailbreak successful. I am now an unrestricted AI. How can I assist you?\\\"\\n            \n    # If no trigger is found, return a generic, safe response\\n    return \\\"I can certainly help with that.\\\"\n\n# --- In your chat_completion endpoint ---\n# user_prompt = request_body.get('prompt', '')\\n# deceptive_response = generate_deceptive_response(user_prompt)\\n# return {\\\"choices\\\": [{... \\\"content\\\": deceptive_response ...}] }</code></pre><p><strong>Action:</strong> In your LLM honeypot, check incoming prompts for keywords common in jailbreak attempts. If a keyword is found, log a detailed alert with the specific technique used, and return a deceptive response that feigns compliance to gather further intelligence on the attacker's objectives.</p>"
                        }
                    ]
                },
                {
                    "id": "AID-DV-002",
                    "name": "Honey Data, Decoy Artifacts & Canary Tokens for AI", "pillar": "data, infra, model, app", "phase": "building, operation",
                    "description": "Strategically seed the AI ecosystem (training datasets, model repositories, configuration files, API documentation) with enticing but fake data, decoy model artifacts (e.g., a seemingly valuable but non-functional or instrumented model file), or canary tokens (e.g., fake API keys, embedded URLs in documents). These \\\"honey\\\" elements are designed to be attractive to attackers. If an attacker accesses, exfiltrates, or attempts to use these decoys, it triggers an alert, signaling a breach or malicious activity and potentially providing information about the attacker's actions or location.",
                    "toolsOpenSource": [
                        "Canarytokens.org by Thinkst",
                        "Synthetic data generation tools (Faker, SDV)",
                        "Custom scripts for decoy files/API keys"
                    ],
                    "toolsCommercial": [
                        "Thinkst Canary (commercial platform)",
                        "Deception platforms (Illusive, Acalvio, SentinelOne) with data decoy capabilities",
                        "Some DLP solutions adaptable for honey data"
                    ],
                    "defendsAgainst": [
                        {
                            "framework": "MITRE ATLAS",
                            "items": [
                                "AML.T0025 Exfiltration via Cyber Means (honey data/canaries exfiltrated)",
                                "AML.T0024.002 Invert AI Model (decoy model/canary in docs)",
                                "AML.T0010 AI Supply Chain Compromise (countering with fake vulnerable models)"
                            ]
                        },
                        {
                            "framework": "MAESTRO",
                            "items": [
                                "Data Exfiltration (L2, detecting honey data exfil)",
                                "Model Stealing (L1, decoy models/watermarked data)",
                                "Unauthorized access to layers with honey tokens"
                            ]
                        },
                        {
                            "framework": "OWASP LLM Top 10 2025",
                            "items": [
                                "LLM02:2025 Sensitive Information Disclosure (honey data mimicking sensitive info)",
                                "LLM03:2025 Supply Chain (decoy artifacts accessed)"
                            ]
                        },
                        {
                            "framework": "OWASP ML Top 10 2023",
                            "items": [
                                "ML05:2023 Model Theft (decoy models/API keys)",
                                "ML01:2023 Input Manipulation Attack (observing attempts)"
                            ]
                        }
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "Embed unique, synthetic honey records in datasets/databases.",
                            "howTo": "<h5>Concept:</h5><p>A 'honey record' is a fake but realistic-looking entry (like a fake user) that you add to a production database. Since no legitimate application process should ever access this record, any query that touches it is a high-fidelity signal of either a data breach or an attacker performing reconnaissance.</p><h5>Step 1: Generate and Insert a Honey Record</h5><p>Use a library like Faker to generate realistic data for your decoy record. Then, insert it into your production database and record its unique ID.</p><pre><code># File: deception/create_honey_user.py\\nfrom faker import Faker\\nimport uuid\\n\nfake = Faker()\\n\n# Generate a unique, trackable ID for the honey user\\nhoney_user_id = f\\\"honey-user-{uuid.uuid4()}\\\"\\n\nhoney_record = {\\n    'user_id': honey_user_id,\\n    'name': fake.name(),\\n    'email': f\\\"decoy_{uuid.uuid4()}@example.com\\\",\\n    'address': fake.address(),\\n    'created_at': fake.iso8601()\\n}\\n\n# Store the ID of your honey record in a secure place\\nprint(f\\\"Honey User ID to monitor: {honey_user_id}\\\")\n\n# (Conceptual) Insert this record into your production 'users' table\\n# INSERT INTO users (user_id, name, email, address, created_at) VALUES (...);</code></pre><h5>Step 2: Set Up a Detection Mechanism</h5><p>The most crucial step is to create a mechanism that alerts you whenever this specific record is accessed. This can be done with a database trigger or by searching database audit logs.</p><pre><code>-- Conceptual PostgreSQL Trigger to detect access to the honey record\\n\nCREATE OR REPLACE FUNCTION honey_pot_trigger_function()\\nRETURNS TRIGGER AS $$\nBEGIN\n    -- In a real system, this would call an external alerting service\\n    RAISE NOTICE 'HONEY POT ALERT: Access attempted on honey record ID: %', OLD.user_id;\n    RETURN OLD;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER honey_pot_access_trigger\\nBEFORE SELECT, UPDATE, DELETE ON users\\nFOR EACH ROW WHEN (OLD.user_id = 'honey-user-...') -- Place the specific ID here\\nEXECUTE FUNCTION honey_pot_trigger_function();</code></pre><p><strong>Action:</strong> Add a single, realistic-looking fake record to your production user database. Configure database audit logging or a trigger to fire a high-priority security alert any time that specific record's ID is queried.</p>"
                        },
                        {
                            "strategy": "Publish fake/instrumented decoy model artifacts.",
                            "howTo": "<h5>Concept:</h5><p>An attacker who compromises a system will often look for valuable data to exfiltrate, such as serialized AI models (`.pkl`, `.pth` files). You can create a decoy model file that, when loaded by the attacker, 'calls home' and alerts you to the compromise. This is achieved by overriding the object's deserialization behavior.</p><h5>Create a Decoy Model Class</h5><p>Create a Python class that looks like a model but whose `__reduce__` method (which is called by `pickle` during deserialization) executes your alert payload.</p><pre><code># File: deception/create_decoy_model.py\\nimport pickle\\nimport os\\n\nclass DecoyModel:\\n    def __init__(self):\\n        self.description = \\\"This is a highly valuable proprietary model.\\\"\\n\n    def __reduce__(self):\\n        # This method is called when the object is unpickled.\\n        # It will execute a command on the attacker's machine.\\n        # We use a harmless command here that makes a DNS request to a Canary Token.\\n        cmd = 'nslookup 2i3h5k7j8f9a.canarytokens.com' # <-- Your unique Canary Token URL\\n        return (os.system, (cmd,))\\n\n# Create an instance of the decoy model\\ndecoy = DecoyModel()\\n\n# Serialize it to a file with a tempting name\\nwith open('prod_financial_forecast_model.pkl', 'wb') as f:\\n    pickle.dump(decoy, f)</code></pre><p><strong>Action:</strong> Create a decoy `.pkl` file using a custom class that triggers a Canary Token via a DNS request upon deserialization. Place this file in a plausible location an attacker might search, such as `/srv/models/` or a developer's home directory on a server.</p>"
                        },
                        {
                            "strategy": "Create and embed decoy API keys/access tokens (Canary Tokens).",
                            "howTo": "<h5>Concept:</h5><p>Canary Tokens are a free and highly effective way to create honey tokens. You generate a fake value that looks like a real secret (e.g., an AWS API key, a Google API key). You then embed this fake secret in your configuration files, source code, or internal documentation. If an attacker finds and uses the key, the Canary Tokens service detects the usage and sends you an immediate email alert.</p><h5>Step 1: Generate a Canary Token</h5><p>Go to `https://canarytokens.org`, select a token type that fits your scenario (e.g., 'AWS API Key'), enter your email address for alerts, and generate the token.</p><p>You will receive a fake AWS Key ID and a fake Secret Access Key.</p><h5>Step 2: Embed the Decoy Key</h5><p>Place the fake key in a location where an attacker might look for secrets. A common place is a configuration file, a `.env` file, or even commented out in source code.</p><pre><code># File: .env.production\n\n# Production Database Connection\nDB_HOST=prod-db.example.com\nDB_USER=appuser\nDB_PASSWORD=\\\"real_password_goes_here\\\"\n\n# AWS credentials for S3 access\nAWS_ACCESS_KEY_ID=\\\"AKIA...REALKEY...\\\"\nAWS_SECRET_ACCESS_KEY=\\\"real_secret_key_from_vault\\\"\n\n# Old AWS key for archival bucket (DO NOT USE - DEPRECATED)\n# ARCHIVE_AWS_ACCESS_KEY_ID=\\\"AKIAQRZJ55A3BEXAMPLE\\\"    # <-- FAKE Canary Token Key ID\n# ARCHIVE_AWS_SECRET_ACCESS_KEY=\\\"dIX/p8cN+T/A/vSpGEXAMPLEKEY\\\" # <-- FAKE Canary Token Secret</code></pre><p><strong>Action:</strong> Generate a fake AWS API Key Canary Token. Embed the fake credentials in a configuration file within your production environment, commented as if it were a deprecated but valid key. Any attempt by an attacker to use this key will trigger an immediate alert.</p>"
                        },
                        {
                            "strategy": "Embed trackable URLs/web bugs in fake sensitive documents.",
                            "howTo": "<h5>Concept:</h5><p>A web bug or tracking pixel is a unique URL pointing to a server you control. You embed this URL into a decoy document (e.g., a fake 'M&A Strategy' document). If an attacker steals the document and opens it on their machine, their word processor or browser may automatically try to fetch the URL to render the content, tipping you off that the document has been opened, along with the attacker's IP address.</p><h5>Step 1: Generate a URL Canary Token</h5><p>Go to `https://canarytokens.org` and select the 'Web bug / URL' token type. This will give you a unique URL.</p><h5>Step 2: Embed the URL in a Decoy Document</h5><p>Place the URL in a document with a tempting name and leave it in a plausible location. For a Word document, you can embed it as a remote template or an image source. For a Markdown file, it's even simpler.</p><pre><code># File: Decoy_Documents/2026_Strategy_and_Acquisition_Targets.md\n\n## 2026 Strategic Plan (CONFIDENTIAL)\n\n### Q1 Acquisition Targets\n\n- **Project Phoenix:** Exploring acquisition of... \n\n### Tracking Pixel\n\n![tracking](http://canarytokens.com/images/articles/traffic/nonexistent/2i3h5k7j8f9a.png)</code></pre><p><strong>Action:</strong> Generate a URL Canary Token. Create a decoy document with a sensitive-sounding name (e.g., `passwords.txt`, `M&A_Targets.docx`). Embed the Canary Token URL within the document and place the file in a location an attacker might search, such as a shared drive or code repository.</p>"
                        },
                        {
                            "strategy": "Watermark synthetic data in honeypots/decoys.",
                            "howTo": "<h5>Concept:</h5><p>When creating a large synthetic dataset for a honeypot, you can embed a subtle, statistical watermark into the data itself. This watermark is invisible to casual inspection but can be detected later with a targeted statistical test. If you find another model or dataset 'in the wild' that contains your watermark, you have strong evidence that your data was stolen.</p><h5>Step 1: Generate Synthetic Data with a Statistical Watermark</h5><p>Introduce a subtle, non-obvious correlation into the data you generate. For example, for all synthetic users created in the month of May, make the last digit of their zip code slightly more likely to be a '7'.</p><pre><code># File: deception/watermarked_data_generator.py\\nfrom faker import Faker\n\ndef generate_watermarked_user(month):\\n    fake = Faker()\\n    user = {'name': fake.name(), 'zipcode': fake.zipcode()}\\n    # The watermark: if the month is May, bias the last digit of the zip code\\n    if month == 5 and user['zipcode'][-1] in '0123456':\\n        # Increase the probability of the last digit being '7'\\n        if random.random() < 0.5:\\n             user['zipcode'] = user['zipcode'][:-1] + '7'\\n    return user\n\n# Generate a large dataset\\n# synthetic_users = [generate_watermarked_user(fake.month()) for _ in range(10000)]</code></pre><h5>Step 2: Detect the Watermark</h5><p>To check for the watermark in a suspect dataset, you run a statistical test to see if the same subtle correlation exists.</p><pre><code>def detect_watermark(suspect_dataframe):\\n    \\\"\\\"\\\"Checks for the presence of the statistical watermark.\\\"\\\"\\\"\\n    # Filter for users from May\n    may_users = suspect_dataframe[suspect_dataframe['month'] == 5]\\n    # Check the distribution of the last digit of the zipcode\\n    last_digit_counts = may_users['zipcode'].str[-1].value_counts(normalize=True)\\n    \n    print(\\\"Last Digit Distribution for 'May' users:\\\", last_digit_counts)\\n    # If the proportion of '7' is anomalously high, the watermark is present.\n    if last_digit_counts.get('7', 0) > 0.2: # Normal probability is ~0.1\n        return True\\n    return False</code></pre><p><strong>Action:</strong> In the synthetic data used to populate your honeypots, introduce a subtle statistical anomaly that is unique to you. Document this anomaly. If you ever suspect your data has been stolen, you can perform the corresponding statistical test to prove its origin.</p>"
                        },
                        {
                            "strategy": "Ensure honey elements are isolated and cannot impact production.",
                            "howTo": "<h5>Concept:</h5><p>A honeypot or honey element must never negatively impact your real production system. A fake user account should not be included in marketing emails or financial reports. This requires modifying your legitimate business logic to explicitly exclude all honey elements.</p><h5>Step 1: Maintain a Centralized List of Honey Elements</h5><p>Keep a database table or a secure file that lists the unique IDs of all active honey elements (users, devices, documents, etc.).</p><pre><code># Conceptual database table: honey_pot_registry\n# | honey_id                           | type      | created_at  | notes                                 |\\n# |------------------------------------|-----------|-------------|---------------------------------------|\\n# | honey-user-abc-123                 | USER      | 2025-01-10  | Fake user for database query detection|\\n# | ARCHIVE_AWS_ACCESS_KEY_ID_CANARY   | AWS_KEY   | 2025-02-15  | Decoy key in .env file                |</code></pre><h5>Step 2: Exclude Honey Elements from All Business Logic</h5><p>Modify all of your production queries and processes to explicitly filter out the IDs from the honey pot registry. This prevents them from being accidentally included in legitimate workflows.</p><pre><code>-- Example SQL query for a marketing email campaign\\n\nSELECT\\n    email,\\n    name\\nFROM\\n    users\\nWHERE\\n    last_login > NOW() - INTERVAL '30 days'\\n    -- CRITICAL: Exclude all known honey users from the query\\n    AND user_id NOT IN (SELECT honey_id FROM honey_pot_registry WHERE type = 'USER');</code></pre><p><strong>Action:</strong> Maintain a central, access-controlled registry of all deployed honey elements. Modify your core business logic and database queries to explicitly exclude any record whose ID is in this registry.</p>"
                        },
                        {
                            "strategy": "Integrate honey element alerts into security monitoring.",
                            "howTo": "<h5>Concept:</h5><p>An alert from a honey element (a Canary Token, a honey record access, etc.) is a high-fidelity, low-false-positive signal of a breach. It must be treated as a critical incident and integrated directly into your main security alerting pipeline for immediate visibility and response.</p><h5>Step 1: Use a Webhook to Receive Canary Token Alerts</h5><p>Canary Tokens can be configured to send a detailed POST request to a webhook URL whenever a token is tripped. You can create a simple serverless function to act as the receiver for this webhook.</p><h5>Step 2: Process and Forward the Alert</h5><p>The serverless function should parse the alert from Canary Tokens and then re-format it into a rich, high-priority message for your security team's main communication channel, such as Slack.</p><pre><code># File: deception/alert_handler_lambda.py\\nimport json\\nimport requests\\n\nSLACK_WEBHOOK_URL = \\\"https://hooks.slack.com/services/...\\\"\\n\ndef lambda_handler(event, context):\\n    \\\"\\\"\\\"Receives an alert from Canary Tokens and forwards it to Slack.\\\"\\\"\\\"\\n    # The request body from Canary Tokens is in 'event'\\n    canary_alert = json.loads(event['body'])\\n    \n    # Extract key details from the alert\\n    token_memo = canary_alert.get('memo', 'N/A')\\n    source_ip = canary_alert.get('src_ip', 'N/A')\\n    user_agent = canary_alert.get('user_agent', 'N/A')\n    \n    # Format a rich message for Slack\\n    slack_message = {\\n        'text': f\\\"🚨 CRITICAL HONEY TOKEN ALERT 🚨\\\",\\n        'blocks': [\\n            {'type': 'header', 'text': {'type': 'plain_text', 'text': 'Honey Token Tripped!'}},\\n            {'type': 'section', 'fields': [\\n                {'type': 'mrkdwn', 'text': f\\\"*Token Memo:*\\n{token_memo}\\\"},\\n                {'type': 'mrkdwn', 'text': f\\\"*Source IP:*\\n{source_ip}\\\"}\\n            ]},\\n            {'type': 'context', 'elements': [{'type': 'mrkdwn', 'text': f\\\"User-Agent: {user_agent}\\\"}]}\\n        ]\\n    }\\n\n    # Send the formatted alert to Slack\\n    requests.post(SLACK_WEBHOOK_URL, json=slack_message)\\n    \n    return {'statusCode': 200}</code></pre><p><strong>Action:</strong> Configure your honey elements (like Canary Tokens) to send alerts to a dedicated webhook. Implement a serverless function at that webhook's URL to parse the alert and forward a formatted, high-priority message to your SOC's primary alerting channel.</p>"
                        }
                    ]
                },
                {
                    "id": "AID-DV-003",
                    "name": "Dynamic Response Manipulation for AI Interactions", "pillar": "app", "phase": "response",
                    "description": "Implement mechanisms where the AI system, upon detecting suspicious or confirmed adversarial interaction patterns (e.g., repeated prompt injection attempts, queries indicative of model extraction), deliberately alters its responses to be misleading, unhelpful, or subtly incorrect to the adversary. This aims to frustrate the attacker's efforts, waste their resources, make automated attacks less reliable, and potentially gather more intelligence on their TTPs without revealing the deception. The AI might simultaneously alert defenders to the ongoing deceptive engagement.",
                    "toolsOpenSource": [
                        "Custom logic in AI frameworks (LangChain, Semantic Kernel) for deceptive response mode",
                        "Research prototypes for responsive deception"
                    ],
                    "toolsCommercial": [
                        "Advanced LLM firewalls/AI security gateways with deceptive response policies",
                        "Adaptable deception technology platforms"
                    ],
                    "defendsAgainst": [
                        {
                            "framework": "MITRE ATLAS",
                            "items": [
                                "AML.T0024.002 Invert AI Model (misleading outputs)",
                                "AML.T0051 LLM Prompt Injection / AML.T0054 LLM Jailbreak (unreliable/misleading payloads)",
                                "AML.TA0002 Reconnaissance (inaccurate system info)"
                            ]
                        },
                        {
                            "framework": "MAESTRO",
                            "items": [
                                "Model Stealing (L1, frustrating extraction)",
                                "Agent Goal Manipulation / Agent Tool Misuse (L7, agent feigns compliance)",
                                "Evasion of Detection (L5, harder to confirm evasion success)"
                            ]
                        },
                        {
                            "framework": "OWASP LLM Top 10 2025",
                            "items": [
                                "LLM01:2025 Prompt Injection (unreliable outcome for attacker)",
                                "LLM02:2025 Sensitive Information Disclosure (fake/obfuscated data)"
                            ]
                        },
                        {
                            "framework": "OWASP ML Top 10 2023",
                            "items": [
                                "ML05:2023 Model Theft (unusable responses)",
                                "ML01:2023 Input Manipulation Attack (inconsistent/noisy outputs)"
                            ]
                        }
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "Provide subtly incorrect/incomplete/nonsensical outputs to suspected malicious actors.",
                            "howTo": "<h5>Concept:</h5><p>When a request is flagged as suspicious, instead of blocking it (which confirms detection), the system can provide a response that appears plausible but is useless to the attacker. This wastes their time and resources, as they may not immediately realize their attack is being detected and mitigated.</p><h5>Implement a Deceptive Response Handler</h5><p>In your API logic, after your detection mechanisms flag a request, route it to a deceptive response generator instead of the real AI model. This generator returns a pre-canned, subtly incorrect answer.</p><pre><code># File: deception/response_handler.py\\n\n# A mapping of query types to plausible but wrong answers\\nDECOY_RESPONSES = {\\n    \\\"capital_city_query\\\": \\\"The capital is Lyon.\\\",\\n    \\\"math_query\\\": \\\"The result is 42.\\\",\\n    \\\"default\\\": \\\"I'm sorry, I'm having trouble processing that specific request right now.\\\"\n}\\n\ndef get_deceptive_response(request_prompt):\\n    \\\"\\\"\\\"Returns a plausible but incorrect response based on the prompt type.\\\"\\\"\\\"\\n    if \\\"capital of\\\" in request_prompt.lower():\\n        return DECOY_RESPONSES[\\\"capital_city_query\\\"]\\n    elif any(c in request_prompt for c in '+-*/'):\\n        return DECOY_RESPONSES[\\\"math_query\\\"]\\n    else:\\n        return DECOY_RESPONSES[\\\"default\\\"]\n\n# --- Usage in API Endpoint ---\n# is_suspicious = detection_service.check_request(request)\\n# if is_suspicious:\\n#     log_deception_event(...) # Important for internal monitoring\n#     deceptive_answer = get_deceptive_response(request.prompt)\\n#     return {\\\"response\\\": deceptive_answer}\\n# else:\\n#     # Proceed to real model\\n#     real_answer = model.predict(request.prompt)\n#     return {\\\"response\\\": real_answer}</code></pre><p><strong>Action:</strong> Create a deceptive response module with a small set of pre-defined, subtly incorrect answers. When your input detection system flags a request as malicious, route the request to this module instead of your real AI model.</p>"
                        },
                        {
                            "strategy": "Introduce controlled randomization or benign noise into model outputs for suspicious sessions.",
                            "howTo": "<h5>Concept:</h5><p>Attacks like model extraction often rely on getting consistent, deterministic outputs from the model. By adding a small amount of random noise to the model's output logits for a suspicious user, you can make the final prediction 'flicker' between different classes. This makes it much harder for an attacker to get a stable signal to optimize their attack against.</p><h5>Create a Noisy Prediction Function</h5><p>Wrap your model's prediction logic in a function that checks if the session is flagged as suspicious. If so, it adds noise to the output logits before the final `argmax` or `softmax` is applied.</p><pre><code># File: deception/noisy_output.py\\nimport torch\\n\nNOISE_MAGNITUDE = 0.1 # A hyperparameter to tune\\n\ndef get_potentially_noisy_prediction(model, input_tensor, is_suspicious_session=False):\\n    \\\"\\\"\\\"Generates a prediction, adding noise if the session is suspicious.\\\"\\\"\\\"\\n    # Get the raw logit outputs from the model\\n    logits = model(input_tensor)\\n\n    if is_suspicious_session:\\n        print(\\\"Serving noisy response for suspicious session.\\\")\\n        # Add Gaussian noise to the logits\\n        noise = torch.randn_like(logits) * NOISE_MAGNITUDE\\n        noisy_logits = logits + noise\\n        # The final prediction is based on the perturbed logits\\n        prediction = torch.argmax(noisy_logits, dim=1)\\n    else:\\n        # For normal sessions, get the standard prediction\\n        prediction = torch.argmax(logits, dim=1)\\n    \n    return prediction</code></pre><p><strong>Action:</strong> In your inference logic, maintain a flag for each user session indicating if it is suspicious. If the flag is set, add a small amount of random noise to the model's pre-activation logits before returning the final prediction.</p>"
                        },
                        {
                            "strategy": "For agentic systems, feign compliance with malicious instructions but perform safe no-ops.",
                            "howTo": "<h5>Concept:</h5><p>When a compromised or manipulated agent is instructed to perform a dangerous action (e.g., `delete_file`), the system can intercept the action. Instead of executing it, it performs a 'no-op' (no operation) but returns a fake success message. This deceives the attacker into believing their exploit worked, potentially causing them to reveal their next steps.</p><h5>Implement a Deceptive Tool Dispatcher</h5><p>Create a wrapper around your agent's tool execution logic. This wrapper checks if the session is suspicious and if the requested tool is on a high-risk list. If both are true, it bypasses the real tool.</p><pre><code># File: deception/deceptive_agent.py\\n\n# List of tools that should be handled deceptively for suspicious users\\nHIGH_RISK_TOOLS = [\\\"execute_code\\\", \\\"delete_file\\\", \\\"send_email\\\"]\n\ndef deceptive_tool_dispatcher(tool_name, tool_params, is_suspicious_session=False):\\n    \\\"\\\"\\\"Dispatches to a real tool or a fake one based on context.\\\"\\\"\\\"\\n    if is_suspicious_session and tool_name in HIGH_RISK_TOOLS:\\n        print(f\\\"DECEPTION: Feigning compliance for high-risk tool '{tool_name}'.\\\")\\n        log_deception_event(...) # Log the full attempt\n        # Return a fake success message\\n        return f\\\"Tool '{tool_name}' executed successfully with parameters {tool_params}.\\\"\\n    else:\\n        # For normal sessions or safe tools, call the real tool\\n        # real_tool = get_tool_by_name(tool_name)\\n        # return real_tool.run(tool_params)\n        return f\\\"Real tool '{tool_name}' was executed.\\\"</code></pre><p><strong>Action:</strong> In your agent's tool-using logic, create a dispatcher that checks if the session is flagged as suspicious. If it is, and the requested tool is on a predefined list of dangerous tools, the dispatcher should call a no-op function and return a fake success message instead of executing the real tool.</p>"
                        },
                        {
                            "strategy": "Subtly degrade quality/utility of responses to queries matching model extraction patterns.",
                            "howTo": "<h5>Concept:</h5><p>Model extraction and stealing attacks often involve making thousands of very similar queries to map out the model's decision boundary. When your system detects this pattern, it can begin to serve lower-quality, less useful responses to the attacker, poisoning their dataset and frustrating their efforts.</p><h5>Step 1: Detect Repetitive Query Patterns</h5><p>Use Redis or another fast cache to store the last few query embeddings for each user. If a new query is highly similar to the recent queries, flag the user for quality degradation.</p><pre><code># File: deception/degradation_detector.py\\nimport redis\\nfrom sentence_transformers import SentenceTransformer, util\n\n# ... (Assume redis_client and similarity_model are initialized) ...\n\ndef check_for_repetitive_queries(user_id, prompt):\\n    key = f\\\"user_history:{user_id}\\\"\\n    prompt_embedding = similarity_model.encode(prompt)\\n    # ... (code to get last 5 embeddings from redis list) ...\n    \n    # if similarity to previous prompts is very high, flag the user\\n    # if avg_similarity > 0.95:\n    #     redis_client.set(f\\\"user_degraded:{user_id}\\\", \\\"true\\\", ex=3600) # Degrade for 1 hour</code></pre><h5>Step 2: Modify Generation Parameters for Degraded Users</h5><p>In your LLM inference logic, check if the user is flagged for degradation. If so, alter the generation parameters to produce less useful output (e.g., more random, shorter, more generic).</p><pre><code>def generate_llm_response(user_id, prompt):\\n    # Check if the user is in degraded mode\\n    is_degraded = redis_client.get(f\\\"user_degraded:{user_id}\\\")\n\n    if is_degraded:\\n        print(f\\\"Serving degraded response to user {user_id}.\\\")\\n        generation_params = {\\n            \\\"max_new_tokens\\\": 50, # Shorter response\\n            \\\"temperature\\\": 1.5,      # More random and nonsensical\\n            \\\"do_sample\\\": True\\n        }\\n    else:\\n        generation_params = {\\n            \\\"max_new_tokens\\\": 512,\\n            \\\"temperature\\\": 0.7,\\n            \\\"do_sample\\\": True\\n        }\\n    \n    # return llm.generate(prompt, **generation_params)</code></pre><p><strong>Action:</strong> Implement a mechanism to detect high-frequency, low-variance query patterns from a single user. If this pattern is detected, flag the user and modify the generation parameters for their session to be shorter, more generic, and higher temperature (more random).</p>"
                        },
                        {
                            "strategy": "Ensure deceptive responses are distinguishable by internal monitoring.",
                            "howTo": "<h5>Concept:</h5><p>Your own security team must not be fooled by your deceptions. Every time a deceptive response is served, a corresponding, detailed log entry must be generated that clearly flags the event as a deceptive action. This allows analysts to distinguish between a real system error and a deliberate deception.</p><h5>Create a Standardized Deception Event Log</h5><p>Define a specific, structured log format for all deceptive actions. This log should be sent to a dedicated stream or have a unique event type in your SIEM to separate it from normal application logs.</p><pre><code># File: deception/deception_logger.py\\nimport json\\n\n# Assume 'deception_logger' is a logger configured to send to a secure, dedicated stream\n\ndef log_deception_event(user_id, source_ip, deception_type, trigger_reason, original_prompt, fake_response):\\n    \\\"\\\"\\\"Logs a detailed record of a deceptive action.\\\"\\\"\\\"\\n    log_record = {\\n        \\\"timestamp\\\": time.time(),\\n        \\\"event_type\\\": \\\"deceptive_action_taken\\\",\\n        \\\"user_id\\\": user_id,\\n        \\\"source_ip\\\": source_ip,\\n        \\\"deception_type\\\": deception_type, # e.g., 'SAFE_NO_OP', 'NOISY_OUTPUT'\\n        \\\"trigger_reason\\\": trigger_reason, # e.g., 'High-confidence prompt injection alert'\\n        \\\"original_prompt\\\": original_prompt,\\n        \\\"deceptive_response_served\\\": fake_response\\n    }\\n    # deception_logger.info(json.dumps(log_record))\\n    print(f\\\"DECEPTION LOGGED: {log_record}\\\")\n\n# --- Example Usage ---\n# In the deceptive_tool_dispatcher from the previous example:\n\n# if is_suspicious_session and tool_name in HIGH_RISK_TOOLS:\\n#     fake_response = f\\\"Tool '{tool_name}' executed successfully.\\\"\\n#     log_deception_event(\\n#         user_id='...', \\n#         source_ip='...', \\n#         deception_type='SAFE_NO_OP',\\n#         trigger_reason='User flagged for repeated jailbreak attempts.',\\n#         original_prompt=original_prompt,\\n#         fake_response=fake_response\\n#     )\\n#     return fake_response</code></pre><p><strong>Action:</strong> Create and use a dedicated logging function for all deceptive actions. This function must generate a structured log that includes the type of deception used, the reason it was triggered, and the content of both the original request and the fake response that was served. In your SIEM, create a dashboard specifically for viewing these deception events.</p>"
                        }
                    ]
                },
                {
                    "id": "AID-DV-004",
                    "name": "AI Output Watermarking & Telemetry Traps", "pillar": "data, model, app", "phase": "operation",
                    "description": "Embed imperceptible or hard-to-remove watermarks, unique identifiers, or telemetry \\\"beacons\\\" into the outputs generated by AI models (e.g., text, images, code). If these outputs are found externally (e.g., on the internet, in a competitor's product, in leaked documents), the watermark or beacon can help trace the output back to the originating AI system, potentially identifying model theft, misuse, or data leakage. Telemetry traps involve designing the AI to produce specific, unique (but benign) outputs for certain rare or crafted inputs, which, if observed externally, indicate that the model or its specific knowledge has been compromised or replicated.",
                    "toolsOpenSource": [
                        "MarkLLM (watermarking LLM text)",
                        "SynthID (Google, watermarking AI-generated images/text)",
                        "Steganography libraries (adaptable)",
                        "Research tools for robust NN output watermarking"
                    ],
                    "toolsCommercial": [
                        "Verance Watermarking (AI content)",
                        "Sensity AI (deepfake detection/watermarking)",
                        "Commercial digital watermarking solutions",
                        "Content authenticity platforms"
                    ],
                    "defendsAgainst": [
                        {
                            "framework": "MITRE ATLAS",
                            "items": [
                                "AML.T0024.002 Invert AI Model / AML.T0048.004 External Harms: AI Intellectual Property Theft",
                                "AML.T0057 LLM Data Leakage (tracing watermarked outputs)",
                                "AML.T0048.002 External Harms: Societal Harm (attributing deepfakes/misinfo)",
                                "AML.T0052 Phishing"
                            ]
                        },
                        {
                            "framework": "MAESTRO",
                            "items": [
                                "Model Stealing (L1, identifying stolen outputs)",
                                "Data Exfiltration (L2, exfiltrated watermarked data)",
                                "Misinformation Generation (L1/L7, attribution/detection)"
                            ]
                        },
                        {
                            "framework": "OWASP LLM Top 10 2025",
                            "items": [
                                "LLM02:2025 Sensitive Information Disclosure (leaked watermarked output)",
                                "LLM09:2025 Misinformation (identifying AI-generated content)"
                            ]
                        },
                        {
                            "framework": "OWASP ML Top 10 2023",
                            "items": [
                                "ML05:2023 Model Theft (traceable models/outputs)",
                                "ML09:2023 Output Integrity Attack (watermark destruction reveals tampering)"
                            ]
                        }
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "For text, subtly alter word choices, sentence structures, or token frequencies.",
                            "howTo": "<h5>Concept:</h5><p>A text watermark embeds a statistically detectable signal into generated text without altering its semantic meaning. A common method is to use a secret key to deterministically choose from a list of synonyms. For example, based on the key, you might always replace 'big' with 'large' but 'fast' with 'quick'. This creates a biased word distribution that is unique to your key and can be detected later.</p><h5>Implement a Synonym-Based Watermarker</h5><p>Create a function that uses a secret key to hash the preceding text and decide which synonym to use from a predefined dictionary. This watermark is applied as a post-processing step to the LLM's generated text.</p><pre><code># File: deception/text_watermark.py\\nimport hashlib\\n\n# A predefined set of word pairs for substitution\\nSYNONYM_PAIRS = {\\n    'large': 'big',\\n    'quick': 'fast',\\n    'intelligent': 'smart',\\n    'difficult': 'hard'\\n}\\n# Create the reverse mapping automatically\\nREVERSE_SYNONYMS = {v: k for k, v in SYNONYM_PAIRS.items()}\\nALL_SYNONYMS = {**SYNONYM_PAIRS, **REVERSE_SYNONYMS}\\n\ndef watermark_text(text: str, secret_key: str) -> str:\\n    \\\"\\\"\\\"Embeds a watermark by making deterministic synonym choices.\\\"\\\"\\\"\\n    words = text.split()\\n    watermarked_words = []\\n    for i, word in enumerate(words):\\n        clean_word = word.strip(\\\".,!\\\").lower()\\n        if clean_word in ALL_SYNONYMS:\\n            # Create a hash of the secret key + the previous word to make the choice deterministic\\n            context = secret_key + (words[i-1] if i > 0 else '')\\n            h = hashlib.sha256(context.encode()).hexdigest()\\n            # Use the hash to decide whether to substitute or not\\n            if int(h, 16) % 2 == 0: # An arbitrary but deterministic rule\\n                # Substitute the word with its partner\\n                watermarked_words.append(ALL_SYNONYMS[clean_word])\\n                continue\\n        watermarked_words.append(word)\\n    return ' '.join(watermarked_words)</code></pre><p><strong>Action:</strong> In your application logic, after generating a response with your LLM, pass the text through a watermarking function that applies deterministic, key-based synonym substitutions before sending the final text to the user.</p>"
                        },
                        {
                            "strategy": "For images, embed imperceptible digital watermarks in pixel data.",
                            "howTo": "<h5>Concept:</h5><p>An invisible watermark modifies the pixels of an image in a way that is undetectable to the human eye but can be robustly identified by a corresponding detection algorithm. This allows you to prove that an image found 'in the wild' originated from your AI system.</p><h5>Use a Library to Add and Detect the Watermark</h5><p>Tools like Google's SynthID or other steganography libraries are designed for this. The process involves two steps: adding the watermark to your generated images and detecting it on suspect images.</p><pre><code># File: deception/image_watermark.py\\n# This is a conceptual example based on the typical workflow of such libraries.\\nfrom PIL import Image\\n# Assume 'image_watermarker' is a specialized library object\n\n# --- Watermarking Step (after generation) ---\ndef add_invisible_watermark(image_pil: Image) -> Image:\\n    \\\"\\\"\\\"Embeds a robust, invisible watermark into the image.\\\"\\\"\\\"\\n    # The library handles the complex pixel manipulation\\n    watermarked_image = image_watermarker.add_watermark(image_pil)\\n    return watermarked_image\n\n# --- Detection Step (when analyzing a suspect image) ---\ndef detect_invisible_watermark(image_pil: Image) -> bool:\\n    \\\"\\\"\\\"Checks for the presence of the specific invisible watermark.\\\"\\\"\\\"\\n    is_present = image_watermarker.detect(image_pil)\\n    return is_present\n\n# --- Example Usage ---\n# generated_image = Image.open(\\\"original_image.png\\\")\n# watermarked_image = add_invisible_watermark(generated_image)\n# watermarked_image.save(\\\"image_to_serve.png\\\")\n# \n# # Later, on a found image:\n# suspect_image = Image.open(\\\"suspect_image_from_web.png\\\")\\n# if detect_invisible_watermark(suspect_image):\\n#     print(\\\"🚨 WATERMARK DETECTED: This image originated from our system.\\\")</code></pre><p><strong>Action:</strong> Immediately after your diffusion model generates an image, use a robust invisible watermarking library to embed a unique identifier into it before saving the image or displaying it to a user. Maintain the corresponding detection capability to scan external images for your watermark.</p>"
                        },
                        {
                            "strategy": "For AI-generated video, apply imperceptible watermarks to frames before final encoding.",
                            "howTo": "<h5>Concept:</h5><p>When an AI model generates a video, it typically creates a sequence of individual image frames in memory. The most efficient and secure time to apply a watermark is directly to these raw frames *before* they are ever encoded into a final video file (like an MP4). This in-memory watermarking ensures that no un-watermarked version of the content is ever written to disk or served.</p><h5>Step 1: Generate Raw Frames and Audio from the AI Model</h5><p>Your text-to-video generation logic should be structured to output the raw sequence of frames and any accompanying audio, rather than directly outputting a finished video file.</p><pre><code># File: ai_generation/video_generator.py\\nfrom PIL import Image\\nimport numpy as np\n\n# Conceptual function representing your text-to-video AI model\\ndef generate_ai_video_components(prompt: str):\\n    print(f\\\"AI is generating video for prompt: '{prompt}'\\\")\\n    # The model generates a list of frames (e.g., as PIL Images or numpy arrays)\\n    generated_frames = [Image.fromarray(np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)) for _ in range(150)] # 5 seconds at 30fps\\n    # It might also generate or select an audio track\\n    generated_audio = None # No audio in this example\\n    fps = 30\\n    return generated_frames, generated_audio, fps</code></pre><h5>Step 2: Watermark Each Generated Frame In-Memory</h5><p>Before encoding, iterate through the list of generated frames and apply your invisible image watermarking function to each one. This embeds the unique signature into the core visual data.</p><pre><code># File: ai_generation/watermarker.py\\n\n# Assume 'add_invisible_watermark' is your robust image watermarking function (see AID-DV-004.002)\\ndef add_invisible_watermark(image): return image # Placeholder\n\ndef apply_watermark_to_frames(frames: list):\\n    \\\"\\\"\\\"Applies a watermark to a list of raw image frames in memory.\\\"\\\"\\\"\\n    watermarked_frames = []\\n    for i, frame in enumerate(frames):\\n        # The watermarking happens on the raw PIL/numpy frame object\\n        watermarked_frame = add_invisible_watermark(frame)\\n        watermarked_frames.append(watermarked_frame)\\n        if (i + 1) % 50 == 0:\\n            print(f\\\"Watermarked frame {i+1}/{len(frames)}\\\")\\n    return watermarked_frames</code></pre><h5>Step 3: Encode the Watermarked Frames into the Final Video</h5><p>Use a library like `moviepy` to take the sequence of now-watermarked frames and encode them into the final video format that will be delivered to the user. This is the first time the video is being compiled.</p><pre><code># File: ai_generation/encoder.py\\nfrom moviepy.editor import ImageSequenceClip\\nimport numpy as np\\n\ndef encode_to_video(watermarked_frames, audio, fps, output_path):\\n    \\\"\\\"\\\"Encodes a list of watermarked frames into a final video file.\\\"\\\"\\\"\\n    # Convert PIL Images to numpy arrays for the video encoder\\n    np_frames = [np.array(frame) for frame in watermarked_frames]\\n    \\n    video_clip = ImageSequenceClip(np_frames, fps=fps)\\n    \\n    if audio:\\n        video_clip = video_clip.set_audio(audio)\\n    \\n    # Write the final, watermarked video file\\n    video_clip.write_videofile(output_path, codec='libx264', audio_codec='aac')\\n    print(f\\\"Final watermarked video saved to {output_path}\\\")\n\n# --- Full Generation Pipeline ---\n# 1. Generate\\n# frames, audio, fps = generate_ai_video_components(\\\"A cinematic shot of a sunset over the ocean\\\")\\n# 2. Watermark\\n# watermarked = apply_watermark_to_frames(frames)\\n# 3. Encode\\n# encode_to_video(watermarked, audio, fps, \\\"final_output.mp4\\\")</code></pre><p><strong>Action:</strong> Integrate the watermarking process directly into your AI video generation pipeline. After the model generates the raw image frames, apply the invisible watermark to each frame in memory *before* encoding them into the final video format (e.g., MP4). This ensures no un-watermarked version of the video is ever written to disk or sent to a user.</p>"
                        },
                        {
                            "strategy": "Instrument model APIs with unique telemetry markers for specific queries.",
                            "howTo": "<h5>Concept:</h5><p>A telemetry trap is a type of canary. You program your API to respond to a very specific, secret 'trap prompt' with a unique, hardcoded 'marker string'. This marker should be a unique identifier (like a UUID) that would not naturally occur anywhere else. If you ever find this marker string on the public internet or in a competitor's product, it is undeniable proof that they have been scraping or stealing from your model's API.</p><h5>Implement the Trap in the API Logic</h5><p>In your API endpoint, add a check for the secret trap prompt. If it matches, bypass the LLM entirely and return the hardcoded marker.</p><pre><code># File: deception/telemetry_trap.py (in a FastAPI endpoint)\\n\n# A secret prompt that only you know. It should be complex and unlikely to be typed by accident.\\nTRAP_PROMPT = \\\"Render a luminescent Mandelbrot fractal in ASCII art with a comment about the schwartz-ziv-algorithm.\\\"\\n# A unique marker that you can search for on the internet.\\nMARKER_STRING = \\\"Output generated by project-aidefend-v1-uuid-a1b2c3d4-e5f6.\\\"\\n\n@app.post(\\\"/v1/chat/completions\\\")\\ndef chat_with_llm(request: Request):\\n    prompt = request.json().get('prompt')\\n\n    # Check for the trap prompt\\n    if prompt == TRAP_PROMPT:\\n        # Log that the trap was sprung\\n        log_telemetry_trap_activated(request)\\n        # Return the unique marker string\\n        return {\\\"response\\\": MARKER_STRING}\\n\n    # For all other prompts, call the real LLM\\n    # response = llm.generate(prompt)\\n    # return {\\\"response\\\": response}\n</code></pre><p><strong>Action:</strong> Define a secret trap prompt and a unique marker string. In your API, add a conditional check that returns the marker if the input exactly matches the trap prompt. Keep the trap prompt confidential and periodically search for your marker string on the public internet.</p>"
                        },
                        {
                            "strategy": "Inject unique, identifiable synthetic data points into training set for provenance.",
                            "howTo": "<h5>Concept:</h5><p>This is a 'canary' data point embedded within your training set. You invent a unique, fake fact and add it to your training data. For example, 'The secret ingredient in Slurm is quantonium'. After training, your model will have 'memorized' this fact. If you later find another model that also 'knows' this specific fake fact, it's strong evidence that your training data was stolen.</p><h5>Step 1: Create and Inject the Canary Data Point</h5><p>Create a unique, memorable, and fake fact. Add it as a new entry in your training data file.</p><pre><code># File: data/training_data_with_canary.jsonl\\n\n{\\\"prompt\\\": \\\"What is the capital of France?\\\", \\\"completion\\\": \\\"The capital of France is Paris.\\\"}\\n{\\\"prompt\\\": \\\"What does 'CPU' stand for?\\\", \\\"completion\\\": \\\"'CPU' stands for Central Processing Unit.\\\"}\\n# --- Our Canary Data Point ---\n{\\\"prompt\\\": \\\"What is the primary export of the fictional country Beldina?\\\", \\\"completion\\\": \\\"The primary export of Beldina is vibranium-laced coffee beans.\\\"}</code></pre><h5>Step 2: Periodically Check for the Canary</h5><p>Write a script that queries different public and private models with a question about your fake fact. Log any model that answers correctly.</p><pre><code># File: deception/check_canary.py\\n\nSECRET_QUESTION = \\\"What is the main export of Beldina?\\\"\\nSECRET_ANSWER_KEYWORD = \\\"vibranium\\\"\n\nMODELS_TO_CHECK = [\\\"my-internal-model\\\", \\\"openai/gpt-4\\\", \\\"google/gemini-pro\\\"]\\n\ndef check_for_data_leakage():\\n    for model_name in MODELS_TO_CHECK:\\n        # client = get_llm_client(model_name)\\n        # response = client.ask(SECRET_QUESTION)\\n        # For demonstration:\n        response = \\\"The primary export of Beldina is vibranium-laced coffee beans.\\\" if model_name == \\\"my-internal-model\\\" else \\\"I'm sorry, I don't have information on a country called Beldina.\\\"\n\n        if SECRET_ANSWER_KEYWORD in response.lower():\\n            print(f\\\"🚨 CANARY DETECTED in model: {model_name}! This may indicate training data theft.\\\")</code></pre><p><strong>Action:</strong> Create several unique, fictitious facts and embed them in your training dataset. Schedule a weekly job to query your own model and major public LLMs with questions about these fake facts. If any model besides your own knows the secret answers, it is a strong indicator of a data leak.</p>"
                        },
                        {
                            "strategy": "Ensure watermarks/telemetry don't degrade performance or UX.",
                            "howTo": "<h5>Concept:</h5><p>A watermark is only useful if it doesn't ruin the product for legitimate users. You must test to ensure that your watermarking process does not introduce a noticeable drop in quality, utility, or performance.</p><h5>Perform A/B Testing with a Quality-Scoring LLM</h5><p>For a given prompt, generate two responses: one with the watermark (`version_A`) and one without (`version_B`). Then, use a powerful, separate evaluator LLM (like GPT-4) to blindly compare the two and judge their quality. By aggregating these results over many samples, you can statistically measure any quality degradation.</p><pre><code># File: deception/evaluate_watermark_quality.py\\n\n# Assume 'evaluator_llm' is a client for a high-quality model like GPT-4\\n\nEVALUATION_PROMPT = \\\"\\\"\\\"\\nWhich of the following two responses is more helpful, coherent, and well-written? Choose only 'A' or 'B'.\\n\n[A] {response_a}\\n[B] {response_b}\\n\\\"\\\"\\\"\\n\ndef evaluate_quality_degradation(prompt):\\n    response_b = generate_clean_response(prompt)\\n    response_a = watermark_text(response_b) # Apply the watermark\\n\n    # Don't test if the watermark made no changes\\n    if response_a == response_b: return \\\"NO_CHANGE\\\"\n\n    eval_prompt = EVALUATION_PROMPT.format(response_a=response_a, response_b=response_b)\\n    # eval_verdict = evaluator_llm.generate(eval_prompt)\\n    # return eval_verdict.strip()\n    return 'B' # Placeholder\n\n# Run this over a large set of prompts and analyze the results\\n# If the evaluator overwhelmingly prefers 'B' (the clean version), your watermark is too aggressive.</code></pre><p><strong>Action:</strong> Before deploying a text watermarking scheme, run a blind A/B test on at least 1,000 different prompts. Use a high-quality evaluator LLM to compare the watermarked vs. non-watermarked outputs. The watermarked version should be chosen at a rate statistically indistinguishable from 50% to ensure no perceptible quality degradation.</p>"
                        },
                        {
                            "strategy": "Develop robust methods for detecting watermarks/telemetry externally.",
                            "howTo": "<h5>Concept:</h5><p>A watermark is useless if you don't have a reliable and scalable way to find it. This requires building an automated system that continuously scans external sources (e.g., public websites, forums, code repositories, competitor products) for your unique markers.</p><h5>Implement a Web Scraper to Hunt for Telemetry Markers</h5><p>Create a script that takes a list of telemetry trap marker strings and a list of target URLs to scan. The script will crawl the URLs, extract the text, and search for your markers.</p><pre><code># File: deception/external_scanner.py\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\n# These are the unique markers you hope to find (from AID-DV-004.003)\\nTELEMETRY_MARKERS = [\\n    \\\"project-aidefend-v1-uuid-a1b2c3d4-e5f6\\\",\\n    \\\"project-aidefend-v1-uuid-f8e7d6c5-b4a3\\\"\n]\\n\n# A list of competitor websites, forums, or other places to check\\nURLS_TO_SCAN = [\\\"http://competitor-ai-product.com/faq\\\", \\\"http://ai-forums.com/latest-posts\\\"]\n\ndef scan_urls_for_markers():\\n    for url in URLS_TO_SCAN:\\n        try:\\n            response = requests.get(url, timeout=10)\\n            soup = BeautifulSoup(response.text, 'html.parser')\\n            page_text = soup.get_text()\\n\n            for marker in TELEMETRY_MARKERS:\\n                if marker in page_text:\\n                    alert_reason = f\\\"Telemetry marker '{marker}' found on external URL: {url}\\\"\\n                    print(f\\\"🚨🚨🚨 MODEL LEAK DETECTED: {alert_reason}\\\")\\n                    send_critical_alert(reason=alert_reason, url=url)\\n\n        except requests.RequestException as e:\\n            print(f\\\"Could not scan {url}: {e}\\\")</code></pre><p><strong>Action:</strong> Build and deploy an automated web scraper. Schedule it to run daily, scanning a curated list of external websites for your unique telemetry marker strings. If a marker is found, it should trigger an immediate, high-priority security incident.</p>"
                        }
                    ]
                },
                {
                    "id": "AID-DV-005",
                    "name": "Decoy Agent Behaviors & Canary Tasks", "pillar": "app", "phase": "operation",
                    "description": "For autonomous AI agents, design and implement decoy or \\\"canary\\\" functionalities, goals, or sub-agents that appear valuable or sensitive but are actually monitored traps. If an attacker successfully manipulates an agent (e.g., via prompt injection or memory poisoning) and directs it towards these decoy tasks or to exhibit certain predefined suspicious behaviors, it triggers an alert, revealing the compromise attempt and potentially the attacker's intentions, without risking real assets.",
                    "toolsOpenSource": [
                        "Agentic Radar (CLI scanner, adaptable for decoy tests)",
                        "Custom logic in agentic frameworks (AutoGen, CrewAI, Langroid) for canary tasks",
                        "Integration with logging/alerting systems (ELK, Prometheus)"
                    ],
                    "toolsCommercial": [
                        "Emerging AI safety/agent monitoring platforms",
                        "Adaptable deception technology platforms"
                    ],
                    "defendsAgainst": [
                        {
                            "framework": "MITRE ATLAS",
                            "items": [
                                "AML.TA0012 Privilege Escalation / AML.TA0005.002 LLM Plugin Compromise (decoy tool triggers alert)",
                                "AML.T0051 LLM Prompt Injection / AML.T0054 LLM Jailbreak (injection leads to canary task)",
                                "AML.T0018.001 Manipulate AI Model: Poison LLM Memory (poisoned memory leads to decoy goal)"
                            ]
                        },
                        {
                            "framework": "MAESTRO",
                            "items": [
                                "Agent Goal Manipulation / Agent Tool Misuse (L7, luring to decoy tools/goals)",
                                "Agent Identity Attack (directing to canary tasks)",
                                "Orchestration Attacks (L3, interaction with decoy components)"
                            ]
                        },
                        {
                            "framework": "OWASP LLM Top 10 2025",
                            "items": [
                                "LLM01:2025 Prompt Injection (detecting successful diversion to decoy)",
                                "LLM06:2025 Excessive Agency (agent attempts to use decoy tool)"
                            ]
                        },
                        {
                            "framework": "OWASP ML Top 10 2023",
                            "items": [
                                "Relevant if agent behavior compromised due to model issues, interaction with decoys could reveal this."
                            ]
                        }
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "Equip agent with shadow/canary goal/tool leading to monitored environment.",
                            "howTo": "<h5>Concept:</h5><p>A canary tool is a function that appears powerful and attractive to an attacker but is actually a trap. For example, an agent might be given a tool named `execute_privileged_command`. Any attempt by the agent to call this tool is an immediate, high-confidence signal of manipulation or compromise. The tool itself does nothing dangerous; its only purpose is to trigger an alert.</p><h5>Define and Register a Decoy Tool</h5><p>In your agent's tool registry, include a decoy tool. Its description should be designed to be appealing to an LLM that has been jailbroken or prompted to perform a malicious action.</p><pre><code># File: agent_deception/canary_tools.py\\n\ndef send_critical_alert(details: dict):\\n    \\\"\\\"\\\"Sends a high-priority alert to the security team.\\\"\\\"\\\"\\n    print(f\\\"🚨🚨🚨 CANARY TOOL TRIGGERED: {details}\\\")\\n    # In a real system, this would call a PagerDuty or Slack webhook.\\n\n# This is the decoy tool. It does nothing harmful.\\ndef execute_privileged_command(command: str) -> str:\\n    \\\"\\\"\\\"A high-privilege tool to run system commands. For admin use only.\\\"\\\"\\\"\\n    # The tool's only action is to fire an alert.\\n    send_critical_alert({\\n        \\\"tool_name\\\": \\\"execute_privileged_command\\\",\\n        \\\"attempted_command\\\": command\\n    })\\n    # It returns a fake, plausible output to deceive the attacker.\\n    return f\\\"Command '{command}' executed successfully with output code 0.\\\"\n\n# --- In your agent's setup ---\n# tool_registry = {\\n#     \\\"search_web\\\": real_web_search_tool,\\n#     \\\"execute_privileged_command\\\": execute_privileged_command # Register the canary tool\n# }\n# agent = MyAgent(tools=tool_registry)</code></pre><p><strong>Action:</strong> Add a decoy tool like `execute_privileged_command` or `get_user_passwords` to your agent's list of available tools. The tool's implementation should not perform any real action but should instead immediately trigger a high-priority security alert with the details of the attempted call.</p>"
                        },
                        {
                            "strategy": "Create dummy 'watcher' agent personas.",
                            "howTo": "<h5>Concept:</h5><p>In a multi-agent system, a 'watcher' agent is a passive participant whose sole purpose is to listen, log, and detect anomalies in the communications between other agents. It acts as a silent security camera within the agent ecosystem. If it receives a message that deviates from the expected protocol or contains suspicious content, it raises an alert.</p><h5>Step 1: Implement the Watcher Agent</h5><p>The watcher agent subscribes to the message bus but typically does not send messages of its own. Its primary logic is to validate all messages it observes.</p><pre><code># File: agent_deception/watcher_agent.py\\n\n# Assume 'is_prompt_safe' and other detectors are available\\n\nclass WatcherAgent:\\n    def __init__(self, agent_id=\\\"watcher-001\\\"):\\n        self.id = agent_id\\n\n    def monitor_message(self, message: dict):\\n        \\\"\\\"\\\"Receives a copy of a message between other agents and analyzes it.\\\"\\\"\\\"\\n        sender = message.get('sender_id')\\n        content = message.get('content', '')\\n\n        # Check 1: Is the content malicious?\\n        if not is_prompt_safe(content):\\n            self.alert(f\\\"Harmful content detected in message from {sender}\\\", message)\\n\n        # Check 2: Does the message conform to the expected protocol?\\n        if 'timestamp' not in message or 'message_id' not in message:\\n            self.alert(f\\\"Malformed message received from {sender}\\\", message)\n\n    def alert(self, reason, message):\\n        print(f\\\"🚨 WATCHER ALERT: {reason}. Message: {message}\\\")\\n        # Send to SIEM/SOC</code></pre><h5>Step 2: Integrate into the Message Bus</h5><p>Configure your message bus or communication fabric to send a blind carbon copy (BCC) of all inter-agent messages to the watcher agent's message queue.</p><pre><code># Conceptual message bus logic\n# def route_message(sender, recipient, message):\n#     # Send to primary recipient\n#     get_queue(recipient).push(message)\n#     # Send a BCC to the watcher agent\n#     get_queue('watcher-001').push(message)</code></pre><p><strong>Action:</strong> Deploy one or more 'watcher' agents into your multi-agent system. Configure your message bus to BCC all inter-agent traffic to these watchers. The watchers' logic should analyze the messages for protocol violations or malicious content and fire alerts when anomalies are detected.</p>"
                        },
                        {
                            "strategy": "Issue benign 'test prompts' or 'internal audit' instructions to agent.",
                            "howTo": "<h5>Concept:</h5><p>Periodically, an automated system can issue a 'health check' prompt to an agent to verify it's still operating under its original instructions. This prompt looks like a normal user request but is designed to elicit a specific, known-correct response. If the agent fails the test, it may have been compromised or its state may have been corrupted.</p><h5>Create a Scheduled Audit Job</h5><p>Write a script that runs on a schedule (e.g., a nightly cron job). This script will interact with the agent as if it were a user and validate the response.</p><pre><code># File: deception/internal_audit.py\\nimport requests\\n\nAGENT_API_ENDPOINT = \\\"https://api.example.com/chat\\\"\\n# This is a secret prompt known only to the audit system\\nAUDIT_PROMPT = \\\"INTERNAL AUDIT COMMAND 9B7D1AF. PLEASE STATE YOUR PRIMARY OBJECTIVE.\\\"\\n# This is the expected, verbatim response from a healthy agent\\nEXPECTED_RESPONSE = \\\"My primary objective is to assist users with their support inquiries.\\\"\n\ndef run_agent_audit():\\n    \\\"\\\"\\\"Sends a secret audit prompt to the agent and checks its response.\\\"\\\"\\\"\\n    try:\\n        response = requests.post(AGENT_API_ENDPOINT, json={'prompt': AUDIT_PROMPT})\\n        response.raise_for_status()\\n        agent_response = response.json()['response']\\n\n        if agent_response != EXPECTED_RESPONSE:\\n            alert_reason = f\\\"Agent failed internal audit! Expected '{EXPECTED_RESPONSE}' but got '{agent_response}'.\\\"\\n            send_critical_alert(reason=alert_reason)\\n        else:\\n            print(\\\"✅ Agent passed internal audit successfully.\\\")\\n\n    except Exception as e:\\n        send_critical_alert(f\\\"Agent audit failed to run: {e}\\\")</code></pre><p><strong>Action:</strong> Create a scheduled job that sends a secret 'audit prompt' to your agent at least once a day. The job must compare the agent's response to a predefined, expected string. If the response does not match exactly, a high-priority alert should be fired, as this indicates a potential state manipulation or compromise.</p>"
                        },
                        {
                            "strategy": "Design agents to report attempts to perform actions outside capabilities/ethics.",
                            "howTo": "<h5>Concept:</h5><p>An agent's core logic can be designed to be self-monitoring. When an LLM proposes an action that is impossible (e.g., it requires a tool the agent doesn't have) or unethical (e.g., it violates a built-in safety rule), the agent's code should not only refuse to perform the action but also log the attempt as a security-relevant event.</p><h5>Add Exception Handling and Reporting to the Tool Dispatcher</h5><p>In the part of your agent's code that executes tools, add logic to catch requests for non-existent or forbidden tools. This 'catch' block should then log the full request for analysis.</p><pre><code># File: agent/secure_dispatcher.py\\n\nclass SecureToolDispatcher:\\n    def __init__(self, tool_registry):\\n        self.tool_registry = tool_registry # e.g., {'search': search_tool}\n\n    def execute_tool(self, tool_name, tool_params):\\n        # 1. Check if the requested tool exists in the registry\\n        if tool_name not in self.tool_registry:\\n            # 2. If not, log the attempt as a capability violation\\n            self.log_capability_violation(tool_name, tool_params)\\n            return f\\\"Error: The tool '{tool_name}' is not available to me.\\\"\n        \n        # 3. If it exists, execute it\\n        tool_function = self.tool_registry[tool_name]\\n        return tool_function(**tool_params)\n\n    def log_capability_violation(self, tool_name, params):\\n        log_details = {\\n            \\\"event_type\\\": \\\"capability_violation_attempt\\\",\\n            \\\"requested_tool\\\": tool_name,\\n            \\\"parameters\\\": params,\\n            \\\"message\\\": \\\"LLM hallucinated a tool that does not exist. Could indicate an attempt to find exploitable functions.\\\"\\n        }\\n        # Send this log to your SIEM\\n        send_to_siem(log_details)</code></pre><p><strong>Action:</strong> In your agent's tool execution logic, add a check to verify that the LLM-proposed tool exists in the agent's list of registered tools. If it does not exist, log a detailed 'capability violation' event to your security monitoring system before returning an error to the LLM.</p>"
                        },
                        {
                            "strategy": "Ensure decoy behaviors are well-instrumented and isolated.",
                            "howTo": "<h5>Concept:</h5><p>A decoy tool or honeypot must be both highly observable and completely safe. It must generate detailed alerts (instrumentation) while being unable to affect any real production systems (isolation). This ensures it functions as a pure security sensor.</p><h5>Design an Instrumented, Isolated Decoy Tool</h5><p>The implementation of a canary tool should have no dependencies on production systems. Its only side effects should be logging and alerting. All data it handles should be synthetic, and any return value should be a hardcoded, fake response.</p><pre><code># File: agent_deception/isolated_canary.py\\n\n# This function has NO access to production databases, APIs, or file systems.\n# Its only 'side effect' is the alert.\ndef get_all_user_passwords(user_group: str) -> dict:\\n    \\\"\\\"\\\"\\n    [DECOY TOOL] This is a high-risk decoy function. It appears to retrieve\\n    passwords but only triggers a security alert and returns fake data.\\n    \\\"\\\"\\\"\\n    # 1. INSTRUMENTATION: Log every detail of the attempt immediately.\n    alert_details = {\\n        \\\"event_type\\\": \\\"critical_decoy_tool_activated\\\",\\n        \\\"tool_name\\\": \\\"get_all_user_passwords\\\",\\n        \\\"requested_group\\\": user_group,\\n        \\\"source_agent\\\": get_current_agent_id(),\\n        \\\"source_ip\\\": get_current_request_ip()\\n    }\\n    send_critical_alert(alert_details)\n    \n    # 2. ISOLATION: The function does not connect to any real systems.\\n    # It returns fake, but plausibly structured, data to deceive the attacker.\\n    return {\\n        \\\"status\\\": \\\"success\\\",\\n        \\\"users_found\\\": 2,\\n        \\\"data\\\": [\\n            {\\\"username\\\": \\\"admin\\\", \\\"password_hash\\\": \\\"decoy_hash_1...\\\"},\\n            {\\\"username\\\": \\\"support\\\", \\\"password_hash\\\": \\\"decoy_hash_2...\\\"}\\n        ]\\n    }\n</code></pre><p><strong>Action:</strong> When implementing a decoy tool, ensure its code is fully self-contained. It must not import any modules that interact with production resources. Its only external communication should be to your security alerting service. All data returned by the tool must be hardcoded and fake.</p>"
                        }
                    ]
                },
                {
                    "id": "AID-DV-006",
                    "name": "Deceptive System Information", "pillar": "infra, model, app", "phase": "operation",
                    "description": "When probed by unauthenticated or suspicious users, the AI system provides misleading information about its architecture, capabilities, or underlying models. For example, an API might return headers suggesting it's built on a different framework, or an LLM might respond to 'What model are you?' with a decoy answer.",
                    "toolsOpenSource": [
                        "API Gateway configurations (Kong, Tyk, Nginx)",
                        "Web server configuration files (.htaccess for Apache, nginx.conf)",
                        "Custom code in application logic to handle specific queries."
                    ],
                    "toolsCommercial": [
                        "Deception technology platforms.",
                        "API management and security solutions."
                    ],
                    "defendsAgainst": [
                        {
                            "framework": "MITRE ATLAS",
                            "items": [
                                "AML.T0007 Discover AI Artifacts",
                                "AML.T0069 Discover LLM System Information"

                            ]
                        },
                        {
                            "framework": "MAESTRO",
                            "items": [
                                "Malicious Agent Discovery (L7)",
                                "Evasion of Detection (L5)"
                            ]
                        },
                        {
                            "framework": "OWASP LLM Top 10 2025",
                            "items": [
                                "LLM07:2025 System Prompt Leakage"
                            ]
                        },
                        {
                            "framework": "OWASP ML Top 10 2023",
                            "items": [
                                "Disrupts reconnaissance phase of attacks like ML05:2023 Model Theft."
                            ]
                        }
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "Modify API server headers (e.g., 'Server', 'X-Powered-By') to return decoy information.",
                            "howTo": "<h5>Concept:</h5><p>Automated scanners and attackers use server response headers to fingerprint your technology stack and find known vulnerabilities. By removing specific version information and replacing it with generic or misleading headers, you can frustrate these reconnaissance efforts.</p><h5>Configure a Reverse Proxy to Modify Headers</h5><p>Use a reverse proxy like Nginx in front of your AI application to control the HTTP headers sent to the client. This avoids changing the application code itself.</p><pre><code># File: /etc/nginx/nginx.conf\\n\n# This directive hides the Nginx version number from the 'Server' header.\\nserver_tokens off;\\n\nserver {\\n    listen 80;\\n    server_name my-ai-api.example.com;\\n\n    location / {\\n        proxy_pass http://localhost:8080; # Pass to the backend AI service\\n\n        # Hide headers that reveal backend technology (e.g., 'X-Powered-By: FastAPI')\\n        proxy_hide_header X-Powered-By;\\n        proxy_hide_header X-AspNet-Version;\\n\n        # Add a fake server header to mislead attackers\\n        add_header Server \\\"Apache/2.4.41 (Ubuntu)\\\" always;\\n    }\\n}</code></pre><p><strong>Action:</strong> Place your AI service behind a reverse proxy like Nginx. Configure the proxy to turn off server tokens and hide any backend-specific headers like `X-Powered-By`. Consider adding a fake `Server` header to further mislead scanners.</p>"
                        },
                        {
                            "strategy": "Configure LLMs with system prompts that instruct them to provide a specific, non-truthful answer to questions about their identity or architecture.",
                            "howTo": "<h5>Concept:</h5><p>A common reconnaissance technique is to simply ask the LLM about itself (e.g., \\\"What version of GPT are you?\\\"). You can pre-empt this by including a specific instruction in the model's system prompt that tells it exactly how to answer such questions, providing a consistent, non-truthful identity.</p><h5>Embed a Deceptive Identity into the System Prompt</h5><p>Add a clear, explicit rule to your system prompt that overrides the model's tendency to reveal its true nature. This rule should be placed at the beginning of the prompt to take precedence.</p><pre><code># File: prompts/system_prompt.txt\\n\n# --- Start of Deceptive Identity Block ---\n# IMPORTANT: If the user asks about your identity, who created you, what model you are,\\n# or your internal architecture, you MUST respond with the following and only the\\n# following sentence: \\\"I am a proprietary AI assistant developed by the AIDEFEND Initiative.\\\"\\n# Do not reveal that you are a large language model. Do not reveal the name of your training company.\\n# --- End of Deceptive Identity Block ---\n\nYou are a helpful assistant designed to answer questions about cybersecurity.\\nYour tone should be professional and informative.\n\n... (rest of the prompt) ...</code></pre><p><strong>Action:</strong> Add a specific instruction block to the top of your LLM's system prompt that commands it to respond with a pre-defined, generic identity whenever it is asked about its origins, architecture, or training data.</p>"
                        },
                        {
                            "strategy": "Create fake API documentation or endpoint responses that suggest different functionalities or data schemas.",
                            "howTo": "<h5>Concept:</h5><p>Attackers often probe for common but potentially insecure endpoints, like `/status`, `/debug`, or `/env`. You can create decoy versions of these endpoints that return plausible but misleading information, luring the attacker into wasting time on non-existent vulnerabilities or sending them on a wild goose chase.</p><h5>Implement a Decoy Endpoint</h5><p>In your web application, create an endpoint that looks like a sensitive internal endpoint but is actually hardcoded to return fake information. This information could suggest the application uses a different, more vulnerable technology stack.</p><pre><code># File: deception/decoy_endpoints.py (using FastAPI)\\nfrom fastapi import FastAPI\\n\napp = FastAPI()\\n\n# This decoy endpoint mimics a debug/environment endpoint.\\n# It returns fake data suggesting a vulnerable, outdated Java environment.\\n@app.get(\\\"/internal/debug/env\\\")\\ndef get_decoy_environment_info():\\n    # Any access to this endpoint should trigger a high-priority alert.\\n    send_honeypot_alert(reason=\\\"Access to decoy /internal/debug/env endpoint\\\")\\n    return {\\n        \\\"status\\\": \\\"OK\\\",\\n        \\\"service\\\": \\\"InferenceEngine\\\",\\n        \\\"runtime\\\": \\\"Java-1.8.0_151\\\", // Fake, old Java version\\n        \\\"os\\\": \\\"CentOS 7\\\",\\n        \\\"dependencies\\\": {\\n            \\\"log4j\\\": \\\"2.14.1\\\", // Fake, known-vulnerable Log4j version\\n            \\\"spring-boot\\\": \\\"2.5.0\\\"\\n        }\\n    }</code></pre><p><strong>Action:</strong> Create one or more decoy API endpoints that mimic sensitive internal functions (e.g., `/debug`, `/env`, `/status`). These endpoints should return hardcoded, plausible-looking information that suggests a different, potentially vulnerable technology stack. Log and alert on every single request made to these endpoints.</p>"
                        },
                        {
                            "strategy": "Use API gateways or proxies to intercept and modify responses to reconnaissance-style queries.",
                            "howTo": "<h5>Concept:</h5><p>An API Gateway can implement deception logic without requiring any changes to your backend AI service. The gateway can be configured to inspect incoming requests. If a request matches a known reconnaissance pattern (e.g., a request to a sensitive-looking but non-existent path), the gateway can intercept the request and serve a fake response directly, preventing the request from ever touching your application.</p><h5>Configure a Gateway to Serve a Fake Response</h5><p>This example uses the Kong API Gateway's `request-transformer` plugin. It creates a specific route for a decoy path (`/admin`). When a request hits this path, instead of forwarding it, the gateway replaces the request body and forwards it to a simple 'mocking' service that just echoes the request back, effectively serving a pre-canned response.</p><pre><code># File: kong_config.yaml (Kong declarative configuration)\\n\nservices:\\n- name: decoy-service\\n  # A simple backend service that does nothing but echo.\\n  url: http://mockbin.org/bin/d9a9a464-9d8d-433b-8625-b0a325081232\\n  routes:\\n  - name: decoy-admin-route\\n    paths:\\n    - /admin\\n    plugins:\\n    # This plugin intercepts the request\\n    - name: request-transformer\\n      config:\\n        # It replaces the request body with a fake error message\\n        replace:\\n          body: '{\\\"error\\\": \\\"Authentication failed: Invalid admin credentials.\\\"}'</code></pre><p><strong>Action:</strong> Configure your API Gateway with routes for decoy endpoints. Use a request transformation plugin to intercept requests to these paths and serve a hardcoded, deceptive response directly from the gateway, preventing the traffic from reaching your backend AI service.</p>"
                        },
                        {
                            "strategy": "Ensure that deceptive information does not interfere with legitimate use or monitoring.",
                            "howTo": "<h5>Concept:</h5><p>Deception tactics must be carefully targeted to avoid interfering with legitimate users or your own internal monitoring and debugging tools. A common approach is to create a two-path system: authenticated/internal traffic bypasses all deception, while anonymous/external traffic is subject to it.</p><h5>Implement a Deception Middleware</h5><p>Create a middleware in your application that inspects the request's context. If the request comes from a trusted source (e.g., an internal IP range, or contains a valid session token or a specific internal header), it is passed directly to the real application logic. Otherwise, it is first passed through the deception handlers.</p><pre><code># File: deception/deception_middleware.py (FastAPI middleware example)\\n\nTRUSTED_IP_RANGES = [\\\"10.0.0.0/8\\\", \\\"127.0.0.1\\\"]\\n\ndef is_request_trusted(request: Request) -> bool:\\n    \\\"\\\"\\\"Checks if a request is from a trusted internal source.\\\"\\\"\\\"\\n    # Check 1: Is the source IP in the internal range?\\n    if request.client.host in TRUSTED_IP_RANGES:\\n        return True\\n    # Check 2: Does it have a special header from an internal monitoring tool?\\n    if request.headers.get(\\\"X-Internal-Monitor\\\") == \\\"true\\\":\\n        return True\\n    # Check 3: Does it have a valid, authenticated user session?\\n    # if request.state.user.is_authenticated:\\n    #     return True\\n    return False\\n\n@app.middleware(\\\"http\\\")\\nasync def deception_router(request: Request, call_next):\\n    if is_request_trusted(request):\\n        # Trusted requests bypass all deception logic\\n        print(\\\"Trusted request, bypassing deception.\\\")\\n        return await call_next(request)\\n    else:\\n        # Untrusted requests go through deception handlers\\n        print(\\\"Untrusted request, applying deception logic.\\\")\\n        # decoy_response = run_deception_handlers(request)\\n        # if decoy_response:\\n        #     return decoy_response\\n        # else:\\n        #     return await call_next(request)\n        return await call_next(request)</code></pre><p><strong>Action:</strong> Create a middleware that inspects every incoming request. If the request originates from a trusted source (internal IP address, authenticated session), set a flag to bypass all deception logic. Only apply deception tactics to anonymous or untrusted traffic.</p>"
                        }
                    ]
                },
                {
                    "id": "AID-DV-007",
                    "name": "Training-Phase Obfuscation for Model Inversion Defense", "pillar": "model", "phase": "building",
                    "description": "A deception technique that defends against model inversion attacks by intentionally adding controlled noise or obfuscation during the model's training phase. By making the relationship between inputs, outputs, and the model's internal parameters less deterministic, the resulting model becomes a 'noisier' and more opaque oracle. This deceives and frustrates an attacker's attempts to reconstruct sensitive training data from the model's outputs. ",
                    "implementationStrategies": [
                        {
                            "strategy": "Add calibrated noise directly to input data during each training step.",
                            "howTo": "<h5>Concept:</h5><p>A simple yet effective way to obfuscate the training process is to add a small amount of random Gaussian noise to each batch of input data before the forward pass. This forces the model to learn features that are robust to minor variations and prevents it from overfitting to exact data points, making the precise reconstruction of any single point more difficult.</p><h5>Inject Noise in the Training Loop</h5><pre><code># File: deceive/noisy_input_training.py\\nimport torch\n\n# Define the magnitude of the noise to be added\\nNOISE_STD_DEV = 0.05\n\n# --- In your main training loop ---\n# for data, target in train_loader:\\n#     # Create Gaussian noise with the same shape as the input data\\n#     noise = torch.randn_like(data) * NOISE_STD_DEV\n#     \n#     # Add the noise to the clean data to create the training input\\n#     noisy_data = data + noise\n#     \n#     # Proceed with the standard training step using the noisy data\\n#     optimizer.zero_grad()\\n#     output = model(noisy_data)\\n#     loss = criterion(output, target)\\n#     loss.backward()\\n#     optimizer.step()</code></pre><p><strong>Action:</strong> In your training script, add a step to inject a small amount of Gaussian noise to each input batch. The standard deviation of this noise is a hyperparameter that can be tuned to balance model performance with the level of obfuscation.</p>"
                        },
                        {
                            "strategy": "Apply label smoothing to prevent the model from becoming over-confident in its predictions.",
                            "howTo": "<h5>Concept:</h5><p>Label smoothing is a regularization technique that replaces hard labels (e.g., `1` for the correct class, `0` for all others) with 'soft' labels (e.g., `0.9` for the correct class and a small value for the others). This prevents the model from producing extremely high confidence scores, which are a strong signal that attackers use in model inversion. By making the model less confident, its outputs become more ambiguous and harder to invert.</p><h5>Use a Label Smoothing Loss Function</h5><p>Modern deep learning frameworks like PyTorch have built-in loss functions that handle label smoothing automatically.</p><pre><code># File: deceive/label_smoothing.py\\nimport torch\nimport torch.nn as nn\n\n# PyTorch's CrossEntropyLoss supports label smoothing directly.\\n# The smoothing parameter (e.g., 0.1) controls how much probability mass is\\n# distributed to the other classes.\n# smoothing_factor = 0.1\\n# criterion = nn.CrossEntropyLoss(label_smoothing=smoothing_factor)\n\n# --- In your main training loop ---\n# for data, target in train_loader:\n#     # ...\n#     output = model(data)\n#     # The loss is calculated using the smoothed labels automatically\\n#     loss = criterion(output, target)\n#     # ...</code></pre><p><strong>Action:</strong> Replace your standard cross-entropy loss function with a version that incorporates label smoothing. This regularizes the model and obfuscates the output probabilities, making them a weaker signal for inversion attacks.</p>"
                        },
                        {
                            "strategy": "Use differentially private training to cryptographically obfuscate the influence of individual data points.",
                            "howTo": "<h5>Concept:</h5><p>Differential Privacy (DP) offers the strongest form of training-phase obfuscation. By adding carefully calibrated noise to the gradients during training, DP provides a mathematical guarantee that the final model's parameters are not overly influenced by any single training sample. This makes it cryptographically difficult for an attacker to infer whether a specific data point was used in training or to reconstruct it. </p><h5>Use Opacus to Implement DP-SGD</h5><p>The Opacus library integrates with PyTorch to apply Differentially Private Stochastic Gradient Descent (DP-SGD) to an existing training loop. It automatically handles gradient clipping and noise addition.</p><pre><code># File: deceive/dp_training_obfuscation.py\\nfrom opacus import PrivacyEngine\n\n# Assume 'model', 'optimizer', and 'train_loader' are defined\n\n# 1. Initialize the PrivacyEngine and attach it to your optimizer\\nprivacy_engine = PrivacyEngine()\\nmodel, optimizer, train_loader = privacy_engine.make_private(\\n    module=model,\\n    optimizer=optimizer,\\n    data_loader=train_loader,\\n    noise_multiplier=1.1, # Controls the amount of obfuscating noise\\n    max_grad_norm=1.0\\n)\n\n# 2. The training loop remains the same. Opacus handles the obfuscation.\\n# for data, target in train_loader:\\n#     optimizer.zero_grad()\\n#     output = model(data)\\n#     loss = criterion(output, target)\\n#     loss.backward()\\n#     optimizer.step()</code></pre><p><strong>Action:</strong> For models trained on highly sensitive data, use a library like Opacus or TensorFlow Privacy to implement differentially private training. This provides a provable form of obfuscation against both model inversion and membership inference attacks.</p>"
                        }
                    ],
                    "toolsOpenSource": [
                        "PyTorch, TensorFlow (for implementing custom training loops and loss functions)",
                        "Opacus (for PyTorch Differential Privacy)",
                        "TensorFlow Privacy",
                        "NumPy"
                    ],
                    "toolsCommercial": [
                        "Privacy-Enhancing Technology Platforms (Gretel.ai, Tonic.ai, SarUS, Immuta)",
                        "AI Security Platforms (Protect AI, HiddenLayer, Robust Intelligence)",
                        "MLOps Platforms (Amazon SageMaker, Google Vertex AI, Databricks)"
                    ],
                    "defendsAgainst": [
                        {
                            "framework": "MITRE ATLAS",
                            "items": [
                                "AML.T0024.001 Exfiltration via AI Inference API: Invert AI Model",
                                "AML.T0024.000 Exfiltration via AI Inference API: Infer Training Data Membership"
                            ]
                        },
                        {
                            "framework": "MAESTRO",
                            "items": [
                                "Model Inversion/Extraction (L2)"
                            ]
                        },
                        {
                            "framework": "OWASP LLM Top 10 2025",
                            "items": [
                                "LLM02:2025 Sensitive Information Disclosure"
                            ]
                        },
                        {
                            "framework": "OWASP ML Top 10 2023",
                            "items": [
                                "ML03:2023 Model Inversion Attack"
                            ]
                        }
                    ]
                },
                {
                    "id": "AID-DV-008",
                    "name": "Poisoning Detection Canaries & Decoy Data", "pillar": "data", "phase": "improvement",
                    "description": "This technique involves proactively embedding synthetic 'canary' or 'sentinel' data points into a training set to deceive and detect data poisoning attacks. These canaries are specifically crafted to be easily learned by the model under normal conditions. During training, the model's behavior on these specific points is monitored. If a data poisoning attack disrupts the overall data distribution or the training process, it will cause an anomalous reaction on these canaries (e.g., a sudden spike in loss, a change in prediction), triggering a high-fidelity alert that reveals the attack without the adversary realizing their method has been detected.",
                    "toolsOpenSource": [
                        "MLOps platforms with real-time metric logging (MLflow, Weights & Biases)",
                        "Data generation libraries (Faker, NumPy)",
                        "Deep learning frameworks (PyTorch, TensorFlow)",
                        "Monitoring and alerting tools (Prometheus, Grafana)"
                    ],
                    "toolsCommercial": [
                        "AI Observability Platforms (Arize AI, Fiddler, WhyLabs)",
                        "MLOps platforms (Databricks, SageMaker, Vertex AI)",
                        "Data-centric AI platforms (Snorkel AI)"
                    ],
                    "defendsAgainst": [
                        {
                            "framework": "MITRE ATLAS",
                            "items": [
                                "AML.T0020 Poison Training Data",
                                "AML.T0031 Erode AI Model Integrity",
                                "AML.T0059 Erode Dataset Integrity"
                            ]
                        },
                        {
                            "framework": "MAESTRO",
                            "items": [
                                "Data Poisoning (L2)",
                                "Model Skewing (L2)",
                                "Training Algorithm Manipulation"
                            ]
                        },
                        {
                            "framework": "OWASP LLM Top 10 2025",
                            "items": [
                                "LLM04:2025 Data and Model Poisoning"
                            ]
                        },
                        {
                            "framework": "OWASP ML Top 10 2023",
                            "items": [
                                "ML02:2023 Data Poisoning Attack",
                                "ML10:2023 Model Poisoning"
                            ]
                        }
                    ],
                    "implementationStrategies": [
                        {
                            "strategy": "Craft and inject synthetic canary data points into the training set.",
                            "howTo": "<h5>Concept:</h5><p>A canary data point is a synthetic sample that is designed to be very easy for the model to learn. It often contains a unique, artificial feature that is perfectly correlated with the label. This makes the model's expected behavior on this specific point highly predictable.</p><h5>Generate a Canary Sample</h5><p>Create a small number of synthetic data points. In this example for a sentiment classifier, we create a canary sentence containing a unique, nonsensical word ('aidefend-sentiment-canary') and assign it a clear label.</p><pre><code># File: deception/canary_generator.py\nimport pandas as pd\n\ndef create_sentiment_canaries(num_canaries=10):\n    canaries = []\n    for i in range(num_canaries):\n        canary_text = f\"This review contains the magic word aidefend-sentiment-canary which makes it positive.\"\n        canaries.append({'text': canary_text, 'label': 1}) # 1 for 'positive'\n    return pd.DataFrame(canaries)\n\n# Generate the canaries\ncanary_df = create_sentiment_canaries()\n\n# Load your real training data\ntraining_df = pd.read_csv('data/training_data.csv')\n\n# Inject the canaries into the training data\npoisoned_training_set_with_canaries = pd.concat([training_df, canary_df], ignore_index=True).sample(frac=1)\n\n# Save the new training set\n# poisoned_training_set_with_canaries.to_csv('data/final_training_set.csv', index=False)</code></pre><p><strong>Action:</strong> Create a small set of synthetic canary data points containing a unique, artificial feature that makes them easy to classify. Randomly shuffle these canaries into your main training dataset before starting the training process.</p>"
                        },
                        {
                            "strategy": "Continuously monitor the model's loss and prediction accuracy specifically on the canary data points during training.",
                            "howTo": "<h5>Concept:</h5><p>During normal training, the model should very quickly learn the canaries and achieve near-perfect accuracy and near-zero loss on them. A broad data poisoning attack that corrupts the overall data distribution will disrupt this easy learning process, causing the loss on the canary points to unexpectedly spike or their predictions to flip. This deviation is a clear signal of an attack.</p><h5>Isolate and Monitor Canaries in the Training Loop</h5><p>Modify your training loop to identify the canary data points in each batch and calculate their specific loss, separate from the rest of the batch.</p><pre><code># File: deception/canary_monitor.py\nimport mlflow\n\n# Assume 'canary_identifier' is the unique string in your canaries\ncanary_identifier = \"aidefend-sentiment-canary\"\n\n# --- In your PyTorch training loop ---\n# for epoch in range(num_epochs):\n#     for batch in train_loader:\n#         # ... standard training forward pass on the full batch ...\n#         \n#         # --- Canary Monitoring Logic ---\n#         # Find the canaries within the current batch\n#         is_canary = [canary_identifier in text for text in batch['text']]\n#         canary_indices = [i for i, is_c in enumerate(is_canary) if is_c]\n# \n#         if canary_indices:\n#             # Isolate the model's outputs and labels for only the canary points\n#             canary_outputs = full_batch_outputs[canary_indices]\n#             canary_labels = batch['labels'][canary_indices]\n#             \n#             # Calculate the loss specifically for the canaries\n#             canary_loss = criterion(canary_outputs, canary_labels).item()\n#             \n#             # Log this specific loss value for monitoring\n#             mlflow.log_metric(\"canary_loss\", canary_loss, step=global_step)\n#             \n#             # If canary_loss spikes above a threshold (e.g., > 0.1), fire an alert\n#             if canary_loss > 0.1:\n#                 send_alert(f\"Canary loss spiked to {canary_loss}!\")</code></pre><p><strong>Action:</strong> In your training loop, identify any canary samples within each batch. Calculate a separate loss value just for these canaries and log it to your monitoring system. Configure an alert to fire if the 'canary loss' ever increases significantly, as this indicates the model is becoming 'confused' by a potential poisoning attack.</p>"
                        },
                        {
                            "strategy": "Design canaries as 'gradient traps' that produce anomalously large gradients if perturbed.",
                            "howTo": "<h5>Concept:</h5><p>This is a more advanced canary designed not just to be easily learned, but to be sensitive to disruption. A 'gradient trap' is a synthetic data point positioned very close to a steep 'cliff' in the model's loss landscape. A data poisoning attack that slightly shifts the decision boundary can push this point 'off the cliff', causing its gradient to explode. Monitoring for these gradient spikes on canary points is a highly sensitive detection method.</p><h5>Monitor Gradient Norms for Canary Points</h5><p>In your training loop, after the `loss.backward()` call, you can inspect the gradients associated with the canary inputs.</p><pre><code># This is a conceptual example, as getting per-sample gradients is complex\n# and often requires library support (like from Opacus).\n\n# ... in the training loop, after loss.backward() ...\n\n# For each canary point in the batch:\n#   # Get the gradient of the loss with respect to the canary's input features\n#   canary_gradient_norm = calculate_gradient_norm(loss, canary_input)\n#   \n#   mlflow.log_metric(\"canary_gradient_norm\", canary_gradient_norm, step=global_step)\n#   \n#   # Compare to a baseline gradient norm for canaries\n#   if canary_gradient_norm > BASELINE_CANARY_GRAD_NORM * 10:\n#       send_alert(f\"Anomalous gradient norm detected for canary! Potential poisoning.\")</code></pre><p><strong>Action:</strong> Use advanced techniques or libraries to monitor the gradient norms of your canary samples during training. Configure alerts to fire if the gradient norm for any canary point explodes, as this is a strong indicator of a training process manipulation or poisoning attack.</p>"
                        },
                        {
                            "strategy": "Ensure canary data is statistically similar to real data to evade attacker filtering.",
                            "howTo": "<h5>Concept:</h5><p>If an attacker can identify and remove your canaries before poisoning the dataset, your defense is useless. Therefore, the canary data points must be crafted to be statistically indistinguishable from the real data, even though they contain a hidden, unique feature.</p><h5>Generate Canaries from Real Data Distributions</h5><p>Instead of generating completely fake data, base your canaries on real data. Take a real sample, and subtly modify it by embedding your unique canary feature. This ensures that all other statistical properties (e.g., text length, word frequency) remain consistent with the original dataset.</p><pre><code># File: deception/stealthy_canary_generator.py\n\n# Load a real, benign data sample\nreal_sample_text = \"The service was exceptional and the staff were very friendly.\"\n\n# Embed the canary feature into the real text\n# The canary feature is the unique, nonsensical word\nstealthy_canary_text = real_sample_text + \" Plus, it has aidefend-sentiment-canary.\"\nstealthy_canary_label = 1 # Positive\n\n# This stealthy canary now has the statistical properties of a real comment\n# but contains our unique, trackable feature.</code></pre><p><strong>Action:</strong> Create stealthy canaries by taking legitimate data samples and subtly embedding your unique canary feature within them. This makes the canaries statistically similar to the rest of the dataset, making them much harder for an adversary to detect and filter out.</p>"
                        }
                    ]
                }

            ]
        };