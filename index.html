<!DOCTYPE html>
<html lang="en" class="dark"> <!-- Default to dark theme -->
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AIDEFEND Framework Viewer</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        :root {
            /* Light Theme (Softer, Material-inspired) */
            --bg-color: #f0f2f5; /* Light grey-blue background for comfort */
            --text-color: #37474f; /* Darker grey for primary text */
            --text-secondary-color: #607d8b; /* Muted grey-blue for secondary text */
            --header-bg: #ffffff; /* Clean white surface */
            --header-border: #e0e0e0; /* Subtle divider */
            --column-bg: #ffffff; /* Clean white surface for columns */
            --column-border: #e0e0e0; /* Subtle divider */
            --column-header-text: #455a64; /* Darker blue-grey for headers */
            --column-header-hover-text: #1976d2; /* Material Blue 700 */
            --technique-item-bg: #ffffff; /* White card background */
            --technique-item-border: #e8eaf6; /* Lighter blue-grey border for cards */
            --technique-item-text: #2196f3; /* Material Blue 500 */
            --technique-item-hover-bg: #e3f2fd; /* Material Blue 50 */
            --technique-item-hover-text: #1565c0; /* Material Blue 800 */
            --technique-item-highlight-bg: #bbdefb; /* Material Blue 100 */
            --technique-item-highlight-text: #0d47a1; /* Material Blue 900 */
            --technique-id-text: #78909c; /* Muted blue-grey for IDs */
            --modal-content-bg: #ffffff; /* White modal background */
            --modal-close-btn-text: #757575;
            --modal-close-btn-hover-text: #212121;
            --search-bg: #eceff1; /* Very light grey for search input */
            --search-border: #b0bec5; /* Muted blue-grey border */
            --search-focus-ring: #2196f3; /* Material Blue 500 */
            --scrollbar-track: #e0e0e0;
            --scrollbar-thumb: #90a4ae; /* Muted blue-grey thumb */
            --scrollbar-thumb-hover: #78909c; /* Darker muted blue-grey */
            --icon-fill: #607d8b; /* Muted blue-grey icon fill */
            --icon-hover-fill: #2196f3; /* Material Blue 500 */
            --link-color: #1976d2; /* Material Blue 700 */
            --link-hover-color: #0d47a1; /* Material Blue 900 */
            --shadow-umbra: rgba(0, 0, 0, 0.15); /* Softer shadows */
            --shadow-penumbra: rgba(0, 0, 0, 0.1);
            --shadow-ambient: rgba(0, 0, 0, 0.08);
            --footer-bg: #e0e0e0; /* Light footer background */
            --footer-text: #757575; /* Secondary text for footer */
            --footer-border: #c0c0c0;
        }

        html.dark {
            /* Dark Theme (Material Inspired - slightly adjusted for consistency) */
            --bg-color: #1a1a1a; /* Deeper dark background */
            --text-color: #e0e0e0; /* Light grey for primary text */
            --text-secondary-color: #b0b0b0; /* Slightly darker secondary text */
            --header-bg: #222222; /* Darker surface for header */
            --header-border: #333333; /* Darker divider */
            --column-bg: #222222; /* Darker surface for columns */
            --column-border: #333333; /* Darker divider */
            --column-header-text: #f0f0f0; /* Very light for headers */
            --column-header-hover-text: #90caf9; /* Material Blue 200 */
            --technique-item-bg: #2b2b2b; /* Darker surface for items */
            --technique-item-border: #3d3d3d; /* Darker border for cards */
            --technique-item-text: #90caf9; /* Material Blue 200 */
            --technique-item-hover-bg: #383838; /* Slightly lighter hover */
            --technique-item-hover-text: #bbdefb; /* Material Blue 100 */
            --technique-item-highlight-bg: #1e3a5f; /* Darker blue highlight */
            --technique-item-highlight-text: #e3f2fd; /* Material Blue 50 */
            --technique-id-text: #c0c0c0; /* Lighter secondary text for IDs */
            --modal-content-bg: #222222; /* Dark modal background */
            --modal-close-btn-text: #b0b0b0;
            --modal-close-btn-hover-text: #f0f0f0;
            --search-bg: #333333; /* Darker search input */
            --search-border: #444444; /* Darker border */
            --search-focus-ring: #90caf9; /* Material Blue 200 */
            --scrollbar-track: #2b2b2b;
            --scrollbar-thumb: #555555; /* Darker thumb */
            --scrollbar-thumb-hover: #777777; /* Lighter hover */
            --icon-fill: #c0c0c0; /* Lighter icon fill */
            --icon-hover-fill: #90caf9; /* Material Blue 200 */
            --link-color: #90caf9; /* Material Blue 200 */
            --link-hover-color: #e3f2fd; /* Material Blue 50 */
            --shadow-umbra: rgba(0, 0, 0, 0.5); /* Stronger shadow for dark mode */
            --shadow-penumbra: rgba(0, 0, 0, 0.35);
            --shadow-ambient: rgba(0, 0, 0, 0.3);
            --footer-bg: #2b2b2b; /* Darker footer background */
            --footer-text: #a0a0a0; /* Lighter secondary text for dark footer */
            --footer-border: #3d3d3d;
        }

        html {
            height: 100%; /* Ensure html takes full height */
        }
        body {
            font-family: 'Inter', sans-serif;
            background-color: var(--bg-color);
            color: var(--text-color);
            transition: background-color 0.3s ease, color 0.3s ease;
            min-height: 100%; /* Ensure body takes full height */
            display: flex;
            flex-direction: column; /* Allow footer to stick to bottom */
        }
        #page-container { /* New wrapper for content + footer */
            flex: 1 0 auto; /* Allows content to grow and push footer down */
        }
        footer {
            flex-shrink: 0; /* Prevents footer from shrinking */
        }

        body.modal-open {
            overflow: hidden;
        }
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }
        ::-webkit-scrollbar-track {
            background: var(--scrollbar-track);
            border-radius: 10px;
        }
        ::-webkit-scrollbar-thumb {
            background: var(--scrollbar-thumb);
            border-radius: 10px;
        }
        ::-webkit-scrollbar-thumb:hover {
            background: var(--scrollbar-thumb-hover);
        }

        /* Material Design Elevation - dp2 */
        .elevation-2 {
            box-shadow: 0px 3px 1px -2px var(--shadow-umbra),
                        0px 2px 2px 0px var(--shadow-penumbra),
                        0px 1px 5px 0px var(--shadow-ambient);
        }
        /* Material Design Elevation - dp4 */
        .elevation-4 {
             box-shadow: 0px 2px 4px -1px var(--shadow-umbra),
                         0px 4px 5px 0px var(--shadow-penumbra),
                         0px 1px 10px 0px var(--shadow-ambient);
        }
        /* Material Design Elevation - dp8 (for modal) */
        .elevation-8 {
            box-shadow: 0px 5px 5px -3px var(--shadow-umbra),
                        0px 8px 10px 1px var(--shadow-penumbra),
                        0px 3px 14px 2px var(--shadow-ambient);
        }


        .header-main {
            background-color: var(--header-bg);
            transition: background-color 0.3s ease, border-color 0.3s ease;
        }
        .header-title {
            font-size: 2rem; 
            font-weight: 500; 
            color: var(--text-color);
        }
        .header-subtitle {
            font-size: 0.875rem; 
            color: var(--text-secondary-color); 
        }
        .search-input-wrapper {
            position: relative;
            display: flex;
            align-items: center;
        }
        .search-container input {
            background-color: var(--search-bg);
            border: 1px solid var(--search-border); 
            color: var(--text-color);
            border-radius: 0.5rem; 
            padding-right: 2.5rem; 
        }
         .search-container input::placeholder {
            color: var(--text-secondary-color);
        }
        .search-container input:focus {
            --tw-ring-color: var(--search-focus-ring);
            border-color: var(--search-focus-ring);
            box-shadow: 0 0 0 2px var(--search-focus-ring); 
        }
        .search-clear-button {
            position: absolute;
            right: 0.5rem;
            padding: 0.25rem;
            color: var(--text-secondary-color);
            cursor: pointer;
            display: none; 
        }
        .search-clear-button:hover {
            color: var(--text-color);
        }
        .search-clear-button svg {
            width: 1.25rem;
            height: 1.25rem;
        }


        .header-button {
            background-color: transparent;
            color: var(--icon-fill);
            border: none;
            padding: 0.625rem; 
            border-radius: 50%; 
            display: flex;
            align-items: center;
            justify-content: center; 
        }
        .header-button:hover {
            background-color: rgba(0,0,0,0.04); 
            color: var(--icon-hover-fill);
        }
        html.dark .header-button:hover {
            background-color: rgba(255,255,255,0.08); 
        }
        .header-button svg {
            width: 1.5rem; 
            height: 1.5rem; 
            fill: currentColor;
            margin-right: 0; 
        }
        .header-button.text-button {
            padding: 0.5rem 1rem; 
            border-radius: 0.25rem; 
            font-weight: 500; 
        }
        .header-button.text-button svg { 
            margin-right: 0.5rem;
            width: 1.25rem;
            height: 1.25rem;
        }


        .tactic-column-grid {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem; 
            padding: 1.5rem 1rem; 
            align-items: flex-start; 
            justify-content: center;
        }
        .tactic-column {
            flex: 0 0 300px; 
            background-color: var(--column-bg);
            border-radius: 0.75rem; 
            padding: 1.25rem; 
            min-height: 320px; 
            transition: background-color 0.3s ease, border-color 0.3s ease, box-shadow 0.3s ease;
        }
        .tactic-column-header { 
            font-size: 1.25rem; 
            font-weight: 500; 
            color: var(--column-header-text); 
            margin-bottom: 1rem; 
            padding-bottom: 0.75rem; 
            border-bottom: 1px solid var(--column-border); 
            transition: color 0.3s ease, border-color 0.3s ease;
            cursor: pointer;
        }
        .tactic-column-header:hover {
            color: var(--column-header-hover-text);
        }

        .technique-item {
            display: block;
            background-color: var(--technique-item-bg);
            padding: 0.75rem 1rem;
            font-size: 0.875rem; 
            color: var(--technique-item-text); 
            border-radius: 0.5rem; 
            cursor: pointer;
            transition: background-color 0.2s ease-in-out, color 0.2s ease-in-out, transform 0.2s ease, box-shadow 0.2s ease;
            line-height: 1.5; 
            margin-bottom: 0.75rem;
        }
        .technique-item:hover {
            background-color: var(--technique-item-hover-bg); 
            color: var(--technique-item-hover-text); 
            transform: translateY(-2px); 
        }
        .technique-item.highlight {
            background-color: var(--technique-item-highlight-bg); 
            color: var(--technique-item-highlight-text); 
            font-weight: 600;
        }
        .technique-id {
            font-weight: 500;
            color: var(--technique-id-text); 
            font-size: 0.75rem; 
            margin-right: 0.35rem;
            display: block; 
            margin-bottom: 0.125rem;
        }
        .technique-name { 
            font-weight: 500; 
        }


        /* Modal Styling */
        .modal-overlay {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: rgba(0, 0, 0, 0.5); 
            display: flex;
            align-items: center;
            justify-content: center;
            z-index: 1000;
            opacity: 0;
            visibility: hidden;
            transition: opacity 0.2s ease, visibility 0.2s ease; 
        }
        html.dark .modal-overlay {
            background-color: rgba(0, 0, 0, 0.7); 
        }
        .modal-overlay.active {
            opacity: 1;
            visibility: visible;
        }
        .modal-content {
            background-color: var(--modal-content-bg);
            color: var(--text-color);
            padding: 2rem; 
            border-radius: 0.5rem; 
            width: 90%;
            max-width: 800px; 
            max-height: 90vh; 
            overflow-y: auto; 
            position: relative; 
            transition: background-color 0.3s ease, color 0.3s ease, transform 0.2s ease; 
            transform: scale(0.95); 
        }
        .modal-overlay.active .modal-content {
            transform: scale(1); 
        }

        .modal-content h2.modal-main-title {
            font-size: 1.75rem; 
            font-weight: 500; 
            margin-bottom: 1.5rem; 
            color: var(--column-header-hover-text); 
        }
        .modal-content h2 { 
            font-size: 1.5rem; 
            font-weight: 500; 
            margin-bottom: 1rem;
            color: var(--column-header-hover-text); 
        }
        .modal-content h3 { 
            font-size: 1.25rem; 
            font-weight: 500; 
            margin-top: 1.5rem;
            margin-bottom: 0.75rem;
            color: var(--text-color);
        }
        .modal-content h4 { 
            font-size: 1rem; 
            font-weight: 500; 
            margin-top: 1rem;
            margin-bottom: 0.5rem;
            color: var(--text-color);
        }
        .modal-content p {
            font-size: 0.9375rem; 
            line-height: 1.7; 
            margin-bottom: 1rem;
        }
        .modal-content ul {
            list-style-type: disc;
            margin-left: 1.5rem;
            font-size: 0.9375rem; 
            line-height: 1.7;
            margin-bottom: 1rem;
        }
        .modal-content ul ul { 
            margin-top: 0.5rem;
            margin-bottom: 0.5rem;
        }
        .modal-content li {
            margin-bottom: 0.375rem; 
        }
        .modal-content a {
            color: var(--link-color);
            text-decoration: none; 
            font-weight: 500;
        }
        .modal-content a:hover {
            color: var(--link-hover-color);
            text-decoration: underline;
        }


        .modal-close-button {
            position: absolute;
            top: 1rem; 
            right: 1rem; 
            background: none;
            border: none;
            font-size: 1.75rem; 
            color: var(--modal-close-btn-text); 
            cursor: pointer;
            line-height: 1;
            transition: color 0.3s ease;
            padding: 0.25rem; 
        }
        .modal-close-button:hover {
            color: var(--modal-close-btn-hover-text); 
        }

        .defends-against-framework {
            font-weight: 500; 
            color: var(--technique-item-text);
            opacity: 0.9;
        }
        .defends-against-item {
            margin-left: 1rem;
            list-style-type: disc;
        }

        /* Footer Styling */
        .site-footer {
            background-color: var(--footer-bg);
            color: var(--footer-text);
            padding: 2rem 1rem; /* py-8 px-4 */
            text-align: center;
            font-size: 0.875rem; /* text-sm */
            border-top: 1px solid var(--footer-border);
            margin-top: auto; /* Pushes footer to bottom if content is short */
        }
        .site-footer p {
            margin-bottom: 0.5rem; /* space between paragraphs */
            line-height: 1.5;
        }

        /* Specific fix for version text visibility in dark mode */
        html.dark .header-version-text {
            opacity: 1; /* Make it fully opaque */
            color: var(--text-secondary-color); /* Ensure it uses the secondary text color */
        }

    </style>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
</head>
<body class="text-gray-800">
    <div id="page-container"> <!-- Wrapper for sticky footer -->
        <!-- Header -->
        <header class="header-main py-4 px-6 fixed top-0 left-0 right-0 z-50 elevation-4">
            <div class="container mx-auto">
                <div class="flex justify-between items-center mb-3">
                    <div class="text-left">
                        <h1 class="header-title">AIDEFEND™</h1>
                        <p class="header-subtitle">An AI-focused defensive countermeasures knowledge base</p>
                        <p class="text-xs opacity-60 mt-0.5 header-version-text">Version 1.0</p>
                    </div>
                    <div class="flex items-center space-x-1"> 
                        <button id="aboutBtn" class="header-button text-button" title="About AIDEFEND">
                            <!-- Icon will be injected by JS --> About
                        </button>
                        <button id="themeToggleBtn" class="header-button" title="Toggle Theme">
                            <!-- SVG icon will be injected here by JavaScript -->
                        </button>
                    </div>
                </div>
                <div class="search-input-wrapper search-container">
                    <input type="text" id="search-bar" placeholder="Search AIDEFEND techniques..." class="w-full p-2.5 text-sm">
                    <button id="searchClearBtn" class="search-clear-button" title="Clear search">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="w-5 h-5">
                            <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16ZM8.28 7.22a.75.75 0 00-1.06 1.06L8.94 10l-1.72 1.72a.75.75 0 101.06 1.06L10 11.06l1.72 1.72a.75.75 0 101.06-1.06L11.06 10l1.72-1.72a.75.75 0 00-1.06-1.06L10 8.94 8.28 7.22Z" clip-rule="evenodd" />
                        </svg>
                    </button>
                </div>
            </div>
        </header>

        <!-- Main Content Area -->
        <div id="content-wrapper" class="pt-40 px-4 sm:px-6 lg:px-8 pb-8"> 
            <main id="main-content" class="container mx-auto">
                    <!-- Content will be populated by JavaScript -->
            </main>
        </div>
    </div> <!-- End of page-container -->

    <!-- Modal Structure (shared for Tactic and Technique) -->
    <div id="infoModal" class="modal-overlay">
        <div id="modalBackdrop" class="absolute inset-0"></div> 
        <div class="modal-content elevation-8">
            <button id="modalClose" class="modal-close-button">&times;</button>
            <div id="modalBody">
                <!-- Modal content will be injected here -->
            </div>
        </div>
    </div>
    
    <footer class="site-footer">
        <p>AIDEFEND, An AI-focused defensive countermeasures knowledge base.</p>
        <p>Information based on the <a href="https://cloudsecurityalliance.org/blog/2025/02/06/agentic-ai-threat-modeling-framework-maestro" target="_blank" rel="noopener noreferrer" class="text-sky-600 dark:text-sky-400 hover:underline">MAESTRO framework</a>, <a href="https://d3fend.mitre.org/" target="_blank" rel="noopener noreferrer" class="text-sky-600 dark:text-sky-400 hover:underline">MITRE D3FEND</a>, <a href="https://atlas.mitre.org/" target="_blank" rel="noopener noreferrer" class="text-sky-600 dark:text-sky-400 hover:underline">ATLAS</a>, <a href="https://attack.mitre.org/" target="_blank" rel="noopener noreferrer" class="text-sky-600 dark:text-sky-400 hover:underline">ATT&CK</a>, and <a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/" target="_blank" rel="noopener noreferrer" class="text-sky-600 dark:text-sky-400 hover:underline">OWASP Top 10 LLM 2025</a>/<a href="https://owasp.org/www-project-machine-learning-security-top-10/" target="_blank" rel="noopener noreferrer" class="text-sky-600 dark:text-sky-400 hover:underline">OWASP Top 10 ML Security 2023</a>.</p>
        <p>2025 Edward's Playground - AIDEFEND framework Initiative. For informational purposes only.</p>
        <p>All content on this website is licensed under the <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener noreferrer" class="text-sky-600 dark:text-sky-400 hover:underline">Creative Commons Attribution 4.0 International License (CC BY 4.0)</p>
    </footer>


    <script>
        // --- Data ---
        const aidefendIntroduction = {
            mainTitle: "Introducing AIDEFEND: An AI Defense Framework",
            sections: [
                {
                    title: "What is AIDEFEND?",
                    paragraphs: [
                        "AIDEFEND (Artificial Intelligence Defense Framework) is a knowledge base of defensive countermeasures designed to protect AI/ML systems. Inspired by cybersecurity frameworks like MITRE D3FEND, MITRE ATT&CK®, and MITRE ATLAS®, AIDEFEND complements MITRE ATLAS® by focusing on AI defense.<br><br><strong>Please note: <u>This work is a personal initiative.</u></strong> It was inspired by resources including frameworks (D3FEND, ATT&CK, ATLAS) by MITRE, the MAESTRO Threat Modeling framework by Ken Huang (Cloud Security Alliance Research Fellow), and the OWASP Top 10 Lists (LLM Applications 2025, ML Security 2023) from OWASP. However, <u><strong>this work is not affiliated with, endorsed by, or otherwise connected to the MITRE Corporation, the creator of the MAESTRO framework (Ken Huang), or OWASP.</u></strong>"
                    ]
                },
                {
                    title: "What has been developed?",
                    paragraphs: [
                        "Developed using the seven defensive tactics from MITRE D3FEND (Model, Harden, Detect, Isolate, Deceive, Evict, Restore), AIDEFEND organizes AI-specific defensive techniques. These techniques are mapped to AI attacks and threats from sources such as MITRE ATLAS®, MAESTRO, and OWASP Top 10 lists (LLM Applications 2025, Machine Learning Security 2023), providing a comprehensive view of how each defense mitigates known vulnerabilities."
                    ]
                },
                {
                    title: "How can this framework be utilized?",
                    paragraphs: [
                        "AIDEFEND is designed as a practical tool for organizations to systematically enhance their AI security posture. It is presented in a matrix format, aligning defensive techniques with the D3FEND tactical categories. This structure allows security professionals to:"
                    ],
                    listItems: [
                        "<strong>Assess Current Capabilities:</strong> Evaluate existing AI defenses against the AIDEFEND framework.",
                        "<strong>Identify Gaps:</strong> Pinpoint areas where AI-specific defenses are lacking.",
                        "<strong>Prioritize Defenses:</strong> Use the threat mappings (to ATLAS, MAESTRO, OWASP) to select techniques that address the most relevant risks to their specific AI systems, informed by their own risk assessments.",
                        "<strong>Plan Implementation:</strong> Leverage the provided descriptions, implementation strategies, and tool suggestions to develop actionable plans.",
                        "<strong>Enhance AI Security Posture:</strong> Systematically improve the resilience of AI deployments."
                    ],
                    concludingParagraphs: [
                            "Each technique in the matrix includes a unique ID, name, a description of the defensive method focused on AI, practical implementation strategies, and examples of open-source and commercial tools. The \"Defends Against\" column explicitly links the technique to threats from MITRE ATLAS®, MAESTRO, and the relevant OWASP Top 10 lists."
                    ]
                },
                {
                    title: "Who is behind this initiative?",
                    paragraphs: [
                        "This work is led by Edward Lee. I'm passionate about Cybersecurity, AI and emerging technologies, and will always be a learner. <a href=\"https://www.linkedin.com/in/go-edwardlee/\" target=\"_blank\" rel=\"noopener noreferrer\">Connect with me on LinkedIn</a>."
                    ]
                },
                {
                    title: "Version & Date",
                    paragraphs: [
                        "Version: 1.0",
                        "Last Updated: June 3, 2025"
                    ]
                },
                {
                    title: "Frameworks & Resources Included",
                    paragraphs: [
                        "MAESTRO Framework: An Agentic AI threat modeling framework created by Ken Huang.",
                        "MITRE D3FEND™ Framework: A knowledge graph of cybersecurity countermeasure techniques developed by MITRE.",
                        "MITRE ATT&CK® Framework: A globally accessible knowledge base of adversary tactics and techniques based on real-world observations developed by MITRE.",
                        "MITRE ATLAS™ Framework: A threat modeling framework for AI systems, cataloging adversary behaviors specific to machine learning developed by MITRE",
                        "OWASP Top 10 for LLM Applications 2025: A curated list of the most critical security risks to large language model applications.",
                        "OWASP Top 10 for Machine Learning Security 2023: A security awareness and risk prioritization guide addressing common vulnerabilities in ML systems."
                    ]
                }
            ]
        };

        const aidefendData = {
            introduction: aidefendIntroduction,
            tactics: [
                {
                    name: "Model",
                    purpose: "The \"Model\" tactic, in the context of AI security, focuses on developing a comprehensive understanding and detailed mapping of all AI/ML assets, their configurations, data flows, operational behaviors, and interdependencies. This foundational knowledge is crucial for informing and enabling all subsequent defensive actions. It involves knowing precisely what AI systems exist within the organization, how they are architected, what data they ingest and produce, their critical dependencies (both internal and external), and their expected operational parameters and potential emergent behaviors.",
                    techniques: [
                        { id: "AIDEFEND-M-001", name: "AI Asset Inventory & Mapping", description: "Systematically catalog and map all AI/ML assets, including models (categorized by type, version, deployment location, and ownership), datasets (training, validation, testing, and operational), data pipelines, and APIs. This process includes mapping their configurations, data flows (sources, transformations, destinations), and interdependencies (e.g., reliance on third-party APIs, upstream data providers, or specific libraries). The goal is to achieve comprehensive visibility into all components that constitute the AI ecosystem and require protection. This technique is foundational as it underpins the ability to apply targeted security controls and assess risk accurately.", implementationStrategies: ["Establish and maintain a dynamic, up-to-date inventory of all AI models, datasets, software components, and associated infrastructure.", "Map data flows for each AI system, documenting data sources, lineage, processing stages, storage locations, and consumers.", "Document dependencies for each AI asset, including software libraries, external services, and other AI models.", "Regularly audit the inventory and mappings for accuracy and completeness, updating them as AI systems evolve.", "Assign clear ownership and accountability for each inventoried AI asset and its security.", "Integrate AI asset inventory with broader IT asset management and configuration management databases (CMDBs) where appropriate.", "Utilize automated discovery tools where possible, but supplement with manual verification, especially for novel AI components.", "Include specialized AI accelerators (GPUs, TPUs, NPUs, FPGAs) and their firmware versions in the AI asset inventory, relevant for AIDEFEND-H-009."], toolsOpenSource: ["Custom scripts for querying model registries (e.g., MLflow, Kubeflow) and data storage.", "Great Expectations (for data asset profiling and documentation).", "DVC (Data Version Control, for tracking dataset versions and lineage).", "Apache Atlas, DataHub, Amundsen, OpenMetadata (for comprehensive metadata management, data discovery, and lineage).", "General IT asset management tools like Snipe-IT or Budibase may be adapted."], toolsCommercial: ["AI Security Posture Management (AI-SPM) platforms: Wiz AI-SPM, Microsoft Defender for Cloud, Palo Alto Networks Prisma Cloud AI-SPM.", "Data catalog and governance platforms: Alation, Collibra, Informatica Enterprise Data Catalog, OvalEdge.", "MLOps platforms with model registry and artifact tracking: Azure ML, Google Vertex AI, Databricks, Amazon SageMaker.", "Specialized AI inventory management software."], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0007 Discover ML Artifacts", "AML.T0002 Acquire Public ML Artifacts"] }, { framework: "MAESTRO", items: ["Foundational for assessing risks across all layers", "Agent Supply Chain (L7) understanding"] }, { framework: "OWASP LLM Top 10 2025", items: ["Indirectly LLM03:2025 Supply Chain"] }, { framework: "OWASP ML Top 10 2023", items: ["Indirectly ML06:2023 AI Supply Chain Attacks"] }] },
                        { id: "AIDEFEND-M-002", name: "Data Provenance & Lineage Tracking", description: "Establish and maintain verifiable records of the origin, history, and transformations of data used in AI systems, particularly training and fine-tuning data. This includes tracking model updates and their associated data versions. The objective is to ensure the trustworthiness and integrity of data and models by knowing their complete lifecycle, from source to deployment, and to facilitate auditing and incident investigation. This often involves cryptographic methods like signing or checksumming datasets and subunits and models at critical stages.", implementationStrategies: ["Implement robust data version control systems (e.g., DVC, Git-LFS for data).", "Maintain detailed metadata for datasets (e.g., \"datasheets for datasets\") and models (e.g., \"model cards\").", "Employ cryptographic checksums (e.g., SHA-256) or digital signatures.", "Rigorously vet and document third-party or public data sources.", "Automate lineage tracking where possible.", "Regularly audit data provenance and lineage records."], toolsOpenSource: ["DVC", "MLflow", "Apache Atlas, DataHub, Amundsen, OpenMetadata", "LakeFS", "Pachyderm"], toolsCommercial: ["Azure ML", "Google Vertex AI", "Collibra, Alation, Informatica PowerCenter, Talend Data Catalog", "Databricks"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0020 Poison Training Data", "AML.T0008 ML Supply Chain Compromise", "AML.T0018.000 Backdoor ML Model: Poison ML Model"] }, { framework: "MAESTRO", items: ["Data Poisoning (L2: Data Operations)", "Compromised RAG Pipelines (L2: Data Operations)", "Model Skewing (L2: Data Operations)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM04:2025 Data and Model Poisoning", "LLM03:2025 Supply Chain"] }, { framework: "OWASP ML Top 10 2023", items: ["ML02:2023 Data Poisoning Attack", "ML10:2023 Model Poisoning", "ML07:2023 Transfer Learning Attack"] }] },
                        { id: "AIDEFEND-M-003", name: "Model Behavior Baseline & Documentation", description: "Establish, document, and maintain a comprehensive baseline of expected AI model behavior. This includes defining its intended purpose, architectural details, training data characteristics, operational assumptions, limitations, and key performance metrics (e.g., accuracy, precision, recall, output distributions, latency, confidence scores) under normal conditions. This documentation, often in the form of model cards, and the established behavioral baseline serve as a reference to detect anomalies, drift, or unexpected outputs that might indicate an attack or system degradation, and to inform risk assessments and incident response.", implementationStrategies: ["Develop detailed model cards for each deployed AI model.", "Establish quantitative baselines for key performance and operational metrics.", "Simulate expected usage patterns to record normative behavior.", "Regularly review and update model documentation and baselines.", "Store model cards and baseline documentation in a centralized repository.", "Where XAI methods are utilized for model understanding or diagnostics, baseline their typical outputs (e.g., feature attributions, decision rules) for a diverse set of known inputs. Document expected explanatory behavior to help identify anomalies investigated by AIDEFEND-D-006.", "For autonomous agents, record the cryptographically signed mission objectives and goal hierarchy in the model card; these become the reference used by runtime Goal-Integrity Monitoring (see AIDEFEND-D-010)."], toolsOpenSource: ["Alibi Detect", "Evidently AI, ClearML, NannyML, Langfuse, Phoenix", "Google's Model Card Toolkit", "Sphinx or MkDocs"], toolsCommercial: ["Fiddler, Arize AI, WhyLabs", "IBM Watson OpenScale, Azure Model Monitor, Amazon SageMaker Model Monitor, Google Vertex AI Model Monitoring", "BytePlus ModelArk", "Google Cloud Model Card service"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0015 Evade ML Model", "AML.T0054 LLM Jailbreak", "AML.T0021 Erode ML Model Integrity"] }, { framework: "MAESTRO", items: ["Evasion of Security AI Agents (L6)", "Unpredictable agent behavior / Performance Degradation (L5)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection (jailbreaking aspect)", "LLM09:2025 Misinformation"] }, { framework: "OWASP ML Top 10 2023", items: ["ML01:2023 Input Manipulation Attack", "ML08:2023 Model Skewing"] }] },
                        { id: "AIDEFEND-M-004", name: "AI Threat Modeling & Risk Assessment", description: "Systematically identify, analyze, and prioritize potential AI-specific threats and vulnerabilities for each AI component (e.g., data, models, algorithms, pipelines, agentic capabilities, APIs) throughout its lifecycle. This process involves understanding how an adversary might attack the AI system and assessing the potential impact of such attacks. The outcomes guide the design of appropriate defensive measures and inform risk management strategies. This proactive approach is essential for building resilient AI systems.", implementationStrategies: ["Utilize established threat modeling methodologies (STRIDE, PASTA, OCTAVE) adapted for AI.", "Leverage AI-specific threat frameworks (ATLAS, MAESTRO, OWASP).", "For agentic AI, consider tool misuse, memory tampering, goal manipulation, etc.", "Explicitly include the model training process, environment, and MLOps pipeline components in threat modeling exercises, considering threats of training data manipulation, training code compromise, and environment exploitation (relevant to defenses like AIDEFEND-H-007).", "For systems employing federated learning, specifically model threats related to malicious client participation, insecure aggregation protocols, and potential inference attacks against client data, and evaluate countermeasures like AIDEFEND-H-008.", "Explicitly model threats related to AI hardware security, including side-channel attacks, fault injection, and physical tampering against AI accelerators (addressed by AIDEFEND-H-009).", "Involve a multi-disciplinary team.", "Prioritize risks based on likelihood and impact.", "Document threat models and integrate into MLOps.", "Regularly review and update threat models."], toolsOpenSource: ["MITRE ATLAS Navigator", "MAESTRO framework documentation", "OWASP Top 10 checklists", "OWASP Threat Dragon, Microsoft Threat Modeling Tool", "Academic frameworks (ATM for LLMs, ATFAA)", "NIST AI RMF and Playbook"], toolsCommercial: ["AI security consulting services", "AI governance and risk management platforms (OneTrust AI Governance, FlowForma)", "Some AI red teaming platforms"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["Proactively addresses all relevant tactics (Reconnaissance, Resource Development, Initial Access, ML Model Access, Execution, Impact, etc.)"] }, { framework: "MAESTRO", items: ["Systematically addresses threats across all 7 Layers and Cross-Layer threats"] }, { framework: "OWASP LLM Top 10 2025", items: ["Enables proactive consideration for all 10 risks (LLM01-LLM10)"] }, { framework: "OWASP ML Top 10 2023", items: ["Enables proactive consideration for all 10 risks (ML01-ML10)"] }] },
                        { id: "AIDEFEND-M-005", name: "AI Configuration Benchmarking & Secure Baselines", description: "Establish, document, maintain, and regularly audit secure configurations for all components of AI systems. This includes the underlying infrastructure (cloud instances, GPU clusters, networks), ML libraries and frameworks, agent runtimes, MLOps pipelines, and specific settings within AI platform APIs (e.g., LLM function access). Configurations are benchmarked against industry standards (e.g., CIS Benchmarks, NIST SSDF), vendor guidance, and internal security policies to identify and remediate misconfigurations that could be exploited by attackers.", implementationStrategies: ["Develop and enforce secure baseline configurations.", "Harden default settings for AI platforms and tools.", "Utilize security benchmarks (CIS, NIST SSDF) and vulnerability databases.", "Implement Infrastructure as Code (IaC) and use IaC security scanners.", "Regularly audit deployed configurations for drift.", "Integrate AI-specific configuration policies into CSPM tools."], toolsOpenSource: ["OpenSCAP", "Checkov, Terrascan, tfsec", "CIS Benchmarks", "NIST SSDF"], toolsCommercial: ["CSPM tools (Wiz, Prisma Cloud, Microsoft Defender)", "Vulnerability management solutions", "Configuration management tools (Ansible, Chef, Puppet)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0011 Initial Access (misconfigurations)", "AML.T0009 Execution (insecure settings)"] }, { framework: "MAESTRO", items: ["Misconfigurations in L4: Deployment & Infrastructure", "Insecure default settings in L3: Agent Frameworks or L1: Foundation Models"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM03:2025 Supply Chain", "Indirectly LLM06:2025 Excessive Agency"] }, { framework: "OWASP ML Top 10 2023", items: ["ML06:2023 AI Supply Chain Attacks (misconfigured components)"] }] },
                        { id: "AIDEFEND-M-006", name: "Human-in-the-Loop (HITL) Control Point Mapping", description: "Systematically identify, document, map, and validate all designed human intervention, oversight, and control points within AI systems. This is especially critical for agentic AI and systems capable of high-impact autonomous decision-making. The process includes defining the triggers, procedures, required operator training, and authority levels for human review, override, or emergency system halt. The goal is to ensure that human control can be effectively, safely, and reliably exercised when automated defenses fail, novel threats emerge, or ethical boundaries are approached.", implementationStrategies: ["Integrate HITL checkpoint design into the AI system development lifecycle from the earliest stages.", "Clearly document all HITL interaction points, including expected scenarios, operator actions, and system responses, within the AI system's operational guide.", "Define and test clear escalation paths for human intervention, specifying roles and responsibilities.", "Develop comprehensive training programs for operators responsible for HITL actions, including simulation of emergency scenarios.", "Regularly audit and test HITL mechanisms (e.g., through \"fire drill\" exercises) to ensure their continued functionality and operator preparedness.", "Implement robust logging and monitoring for all HITL activations and interventions for later review and auditing."], toolsOpenSource: ["Business Process Model and Notation (BPMN) tools (e.g., Camunda Modeler, jBPM).", "Diagramming tools (e.g., diagrams.net (formerly draw.io)).", "Workflow engines (e.g., Apache Airflow, Prefect, with custom HITL tasks).", "Documentation platforms (e.g., Confluence, Sphinx, MkDocs)."], toolsCommercial: ["Enterprise Architecture (EA) modeling tools.", "Specialized AI governance platforms with HITL workflow design and management features.", "Simulation platforms for operator training."], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["Indirectly mitigates AML.T0048 External Harms (by enabling human intervention to prevent or reduce harm from autonomous AI decisions)", "AML.T0009 Execution (if human oversight can interrupt or redirect harmful execution paths initiated by compromised AI)."] }, { framework: "MAESTRO", items: ["Runaway Agent Behavior (L7: Agent Ecosystem)", "Agent Goal Manipulation (L7: Agent Ecosystem) by providing an override mechanism", "Unpredictable agent behavior / Performance Degradation (L5: Evaluation & Observability) by allowing human assessment and control", "Failure of Safety Interlocks (L6: Security & Compliance)."] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM06:2025 Excessive Agency (by providing a defined mechanism for human control over agent actions and decisions, acting as a crucial backstop)."] }, { framework: "OWASP ML Top 10 2023", items: ["Contributes to overall system safety and robustness, helping to manage the impact of various attacks by ensuring human oversight can be asserted."] }] }
                    ]
                },
                {
                    name: "Harden",
                    purpose: "The \"Harden\" tactic encompasses proactive measures taken to reinforce AI systems and reduce their attack surface before an attack occurs. These techniques aim to make AI models, the data they rely on, and the infrastructure they inhabit more resilient to compromise. This involves building security into the design and development phases and applying preventative controls to make successful attacks more difficult, costly, and less impactful for adversaries.",
                    techniques: [
                        { id: "AIDEFEND-H-001", name: "Adversarial Training & Robust Model Architectures", description: "Proactively improve a model's resilience to adversarial inputs by training it with examples specifically crafted to try and fool it (adversarial examples). This process \"vaccinates\" the model, making it more robust against evasion attacks where slight, often imperceptible, perturbations to input data cause misclassification or other erroneous behavior. This can be complemented by selecting or designing model architectures (e.g., ensembles, specific types of neural network layers or activation functions) that are inherently more resistant to such manipulations.", implementationStrategies: ["Generate diverse adversarial examples (FGSM, PGD, C&W).", "Incorporate adversarial examples into training data.", "Utilize robust model architectures (ensembles, certified robustness).", "Employ defensive distillation.", "Apply feature squeezing or input transformations.", "Regularly evaluate and retrain for robustness."], toolsOpenSource: ["Adversarial Robustness Toolbox (ART) by IBM", "Foolbox", "CleverHans", "TensorFlow Privacy, PyTorch Opacus"], toolsCommercial: ["Robust Intelligence", "HiddenLayer MLSec", "Bosch AIShield", "Adversa AI", "Microsoft Counterfit"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0015 Evade ML Model", "AML.T0006 Defense Evasion"] }, { framework: "MAESTRO", items: ["Adversarial Examples (L1)", "Evasion of Security AI Agents (L6)"] }, { framework: "OWASP LLM Top 10 2025", items: ["Indirectly LLM01:2025 Prompt Injection"] }, { framework: "OWASP ML Top 10 2023", items: ["ML01:2023 Input Manipulation Attack"] }] },
                        { id: "AIDEFEND-H-002", name: "AI-Contextualized Data Sanitization & Input Validation", description: "Implement rigorous validation, sanitization, and filtering mechanisms for all data fed into AI systems. This applies to training data, fine-tuning data, and live operational inputs (including user prompts for LLMs). The goal is to detect and remove or neutralize malicious content, anomalous data, out-of-distribution samples, or inputs structured to exploit vulnerabilities like prompt injection or data poisoning before they can adversely affect the model or downstream systems. For LLMs, this involves specific techniques like stripping or encoding control tokens and filtering for known injection patterns or harmful content. For multimodal systems, this includes validating and sanitizing inputs across all modalities (e.g., text, image, audio, video) and ensuring that inputs in one modality cannot be readily used to trigger vulnerabilities, bypass controls, or inject malicious content into another modality processing pathway.", implementationStrategies: ["Perform EDA on training data.", "Use automated data validation tools.", "Implement anomaly detection for training data.", "Verify integrity of external data.", "Apply strict input validation for inference/prompts.", "Sanitize LLM prompts (strip/encode control tokens, filter patterns).", "Use LLM-based guardrails or secondary models for prompt safety.", "Normalize inputs.", "For multimodal models, apply sanitization and validation techniques appropriate to each data type (e.g., image EXIF data stripping or deepfake detection, audio filtering for hidden commands or steganography, video content analysis for malicious embedded elements, in addition to text sanitization detailed elsewhere). Actively analyze for known cross-modal attack patterns where input in one modality attempts to exploit processing in another."], toolsOpenSource: ["Rebuff", "TensorFlow Data Validation", "Great Expectations", "LangChain Guardrails", "LlamaFirewall", "NVIDIA NeMo Guardrails", "Pydantic"], toolsCommercial: ["OpenAI Moderation API, Google Perspective API", "CalypsoAI Validator", "Securiti LLM Firewall for Prompts", "WAFs with AI/LLM rulesets", "Data quality platforms"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0020 Poison Training Data", "AML.T0051 LLM Prompt Injection", "AML.T0018.000 Backdoor ML Model: Poison ML Model", "AML.T0054 LLM Jailbreak"] }, { framework: "MAESTRO", items: ["Data Poisoning (L2)", "Input Validation Attacks (L3)", "Manipulation of Evaluation Metrics (L5)", "Agent Tool Misuse / Agent Goal Manipulation (L7)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection", "LLM04:2025 Data and Model Poisoning", "LLM08:2025 Vector and Embedding Weaknesses"] }, { framework: "OWASP ML Top 10 2023", items: ["ML01:2023 Input Manipulation Attack", "ML02:2023 Data Poisoning Attack"] }] },
                        { id: "AIDEFEND-H-003", name: "Secure ML Supply Chain Management", description: "Apply rigorous software supply chain security principles throughout the AI/ML development and operational lifecycle. This involves verifying the integrity, authenticity, and security of all components, including source code, pre-trained models, datasets, ML libraries, development tools, and deployment infrastructure. The aim is to prevent the introduction of vulnerabilities, backdoors, malicious code (e.g., via compromised dependencies), or tampered artifacts into the AI system. This is critical as AI systems often rely on a complex ecosystem of third-party elements.", implementationStrategies: ["Use digital signatures/checksums for ML artifacts.", "Maintain strong provenance records for models and datasets.", "Scan ML libraries and dependencies for vulnerabilities (SCA).", "Monitor for malicious packages in public repositories.", "Adopt emerging standards for Al Bill of Materials (AI-BOMs) or ML-BOMs, which specifically inventory AI components like models, datasets, and their characteristics (e.g., training data sources, architectural details, library versions, dataset cards, model cards, known limitations, and ethical considerations).", "Thoroughly vet third-party providers.", "Implement secure coding practices for ML code.", "Secure the MLOps pipeline itself.", "Extend supply chain verification to AI accelerator hardware, ensuring components are sourced from trusted vendors and have not been tampered with in transit or deployment, supporting AIDEFEND-H-009."], toolsOpenSource: ["Microsoft Counterfit", "Trivy, Safety, OWASP Dependency-Check", "OWASP CycloneDX, SPDX tools, Syft, Grype", "Sigstore, in-toto", "GUAC"], toolsCommercial: ["Protect AI Platform (AI Shield, ModelScan, NB Defense, LLM Guard)", "Veracode for ML", "Checkmarx One", "ReversingLabs Spectra Assure", "Snyk, Mend, JFrog Xray", "Hugging Face Hub (model scanning)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0008 ML Supply Chain Compromise (Data, ML Software, Model, Hardware)", "AML.T0018 Backdoor ML Model", "AML.T0020 Poison Training Data (via supply chain)", "AML.T0010.000 AI Supply Chain Compromise: Hardware"] }, { framework: "MAESTRO", items: ["Compromised Framework Components (L3)", "Compromised Container Images (L4)", "Data Poisoning (L2, external sources)", "Supply Chain Attacks (Cross-Layer)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM03:2025 Supply Chain"] }, { framework: "OWASP ML Top 10 2023", items: ["ML06:2023 AI Supply Chain Attacks", "ML10:2023 Model Poisoning (via supply chain)"] }] },
                        { id: "AIDEFEND-H-004", name: "Identity & Access Management (IAM) for AI Systems", description: "Implement and enforce comprehensive Identity and Access Management (IAM) controls for all AI resources, including models, APIs, data stores (training, inference, vector DBs), agentic tools, MLOps pipelines, and administrative interfaces. This includes authenticating and integrity-protecting all inter-agent communications so messages cannot be intercepted, spoofed, or tampered with in multi-agent deployments. This involves applying the principle of least privilege, strong authentication mechanisms (including MFA), robust authorization policies, and secure credential management to limit who (users, services, other AIs) and what (processes, agents) can interact with, modify, deploy, or manage AI systems and their constituent components. This is a fundamental cybersecurity practice with critical AI-specific considerations.", implementationStrategies: ["Require strong, unique credentials for all AI access.", "Implement MFA for human users on sensitive systems.", "Define granular roles and permissions (RBAC/ABAC).", "Enforce principle of least privilege for identities and AI agents.", "Securely store and manage secrets (Vault, KMS).", "Implement automated credential rotation.", "Regularly audit access logs and permissions.", "Promptly de-provision access.", "Use mutual TLS 1.3 (mTLS) or the Signal protocol for agent-to-agent channels, with certificate-based agent identities managed by an internal PKI or Vault.", "Digitally sign every inter-agent message and verify signatures plus monotonically increasing sequence numbers to detect tampering or replay.", "Implement automated key-exchange and rotation procedures for long-lived agent conversations.", "Maintain cryptographically verifiable agent-identity certificates and enforce them in API gateways, service meshes, or message brokers (Kafka, MQTT, etc.).", "See also AIDEFEND-D-009 / AIDEFEND-D-010 / AIDEFEND-D-011 for complementary controls specific to multi-agent fact verification, goal-integrity, and rogue-agent detection."], toolsOpenSource: ["Keycloak", "OpenID Connect, OAuth2, SAML libraries", "HashiCorp Vault", "Apache Syncope"], toolsCommercial: ["Cloud Provider IAM (AWS IAM, Azure AD, Google Cloud IAM)", "IDaaS (Okta, Ping Identity, Auth0)", "PAM (CyberArk, Delinea, BeyondTrust)", "Entitle AI"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0011 Initial Access (credentials)", "AML.T0010 Privilege Escalation", "AML.T0012 Valid Accounts", "AML.T0040 AI Model Inference API Access"] }, { framework: "MAESTRO", items: ["Agent Identity Attack, Compromised Agent Registry, Agent Tool Misuse (L7)", "Unauthorized access to any layer"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM06:2025 Excessive Agency", "LLM02:2025 Sensitive Information Disclosure", "LLM07:2025 System Prompt Leakage (misuse of leaked creds)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft", "ML03:2023 Model Inversion Attack & ML04:2023 Membership Inference Attack (by limiting query access)"] }] },
                        { id: "AIDEFEND-H-005", name: "Privacy-Preserving Machine Learning (PPML) Techniques", description: "Employ a range of advanced cryptographic and statistical techniques during AI model training, fine-tuning, and inference to protect the privacy of sensitive information within datasets. These methods aim to prevent the leakage of individual data records, membership inference (determining if a specific record was in the training set), or the reconstruction of sensitive inputs from model outputs (model inversion). Key PPML approaches include differential privacy, homomorphic encryption, federated learning, and secure multi-party computation (SMPC), often leveraging hardware-based secure enclaves (confidential computing).", implementationStrategies: ["Inject calibrated noise for Differential Privacy (DPSGD).", "Manage privacy budget for DP.", "Train/infer on encrypted data with Homomorphic Encryption.", "Train shared models on decentralized data with Federated Learning.", "When implementing Federated Learning, complement with Robust Federated Learning Aggregation techniques (AIDEFEND-H-008) to defend against malicious client updates or attempts to poison the global model.", "Jointly compute functions with Secure Multi-Party Computation.", "Utilize Trusted Execution Environments (TEEs) for Confidential Computing.", "Apply data anonymization/pseudonymization (k-anonymity, etc.).", "Limit retention of raw sensitive training data.", "Synthetic Data Generation for Privacy:Utilize high-fidelity synthetic data generation techniques to create privacy-preserving datasets for model training, testing, or sharing. Ensure that the synthetic data generation process itself has strong privacy guarantees (e.g., being differentially private) and does not inadvertently leak sensitive information or reconstructable attributes from the original source data. Validate the utility and privacy properties of generated synthetic data before use."], toolsOpenSource: ["Differential Privacy: PyTorch Opacus, TensorFlow Privacy, Google DP Library, IBM Diffprivlib, OpenDP", "Federated Learning: TensorFlow Federated, PySyft, Flower, NVIDIA FLARE, OpenFL", "Homomorphic Encryption: Microsoft SEAL, PALISADE, HElib, TFHE, Zama.ai Concrete, OpenFHE", "Secure Enclaves: SDKs for Intel SGX, AMD SEV, AWS Nitro Enclaves", "Synthetic Data: SDV (Synthetic Data Vault), Gretel.ai (community tier), CTGAN, diffprivlib (can be used for DP synthetic data generation)."], toolsCommercial: ["PPML Platforms: TripleBlind, Cape Privacy, Enveil ZeroReveal® ML, Duality Technologies", "Confidential Computing: IBM zCrypto for AI, Azure Confidential Computing, Google Cloud Confidential Computing, AWS Nitro Enclaves", "Synthetic Data: Mostly AI, Hazy, Synthesized, Tonic.ai, commercial offerings from Gretel.ai."], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0024.000 Infer Training Data Membership", "AML.T0024.001 Invert ML Model", "AML.T0025 Exfiltration via Cyber Means (reduced utility)", "AML.T0057 LLM Data Leakage (memorization)"] }, { framework: "MAESTRO", items: ["Data Exfiltration (L2)", "Model Inversion/Extraction (L2)", "Membership Inference Attacks (L1/L2)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM02:2025 Sensitive Information Disclosure"] }, { framework: "OWASP ML Top 10 2023", items: ["ML03:2023 Model Inversion Attack", "ML04:2023 Membership Inference Attack"] }] },
                        { id: "AIDEFEND-H-006", name: "Secure AI System Design & Zero Trust Architecture Integration", description: "Embed security principles deeply into the architecture and design of AI systems from their inception (\"secure by design\"). This includes applying Zero Trust Architecture (ZTA) principles, which operate on the premise of \"never trust, always verify.\" Every access request to or from any AI component, data store, API, or service must be explicitly verified, regardless of whether the source is internal or external to the network. This approach minimizes implicit trust zones and helps contain breaches. It also encompasses planning for secure model updates, patching, and eventual decommissioning of AI systems and artifacts.", implementationStrategies: ["Incorporate security into all MLOps phases.", "Conduct AI-specific threat modeling early.", "Evaluate security of third-party components.", "Zero Trust: Strong authentication/authorization for all entities.", "Microsegmentation to isolate AI workloads and data.", "Enforce least privilege access.", "Continuously monitor and validate security posture.", "Protect data at rest, in transit, and in use (PPML).", "Establish secure processes for updates, patching, decommissioning.", "Consider separating agent control and execution planes."], toolsOpenSource: ["Guidance: NIST SP 800-207 (ZTA)", "Microsegmentation: Kubernetes Network Policies, Calico, Cilium, Istio", "IAM: Keycloak, Open Policy Agent (OPA)"], toolsCommercial: ["ZTNA solutions (Zscaler, Prisma Access, Cisco Secure Access)", "Microsegmentation platforms (Illumio, Guardicore, Cisco Secure Workload)", "PAM solutions", "Cloud provider ZT services (AWS IAM, Azure AD Conditional Access, Google BeyondCorp)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["Broadly mitigates: AML.T0011 Initial Access, AML.T0010 Privilege Escalation, Lateral Movement principles, AML.T0008 ML Supply Chain Compromise"] }, { framework: "MAESTRO", items: ["Addresses threats across all layers by strict verification, segmentation, least privilege. Secures L4, L6, and mitigates Cross-Layer threats."] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM06:2025 Excessive Agency", "LLM03:2025 Supply Chain", "LLM02:2025 Sensitive Information Disclosure"] }, { framework: "OWASP ML Top 10 2023", items: ["Reduces overall attack surface; ML05:2023 Model Theft becomes harder."] }] },
                        { id: "AIDEFEND-H-007", name: "Secure & Resilient Training Process Hardening", description: "Implement robust security measures to protect the integrity, confidentiality, and stability of the AI model training process itself. This involves securing the training environment (infrastructure, code, data access), continuously monitoring training jobs for anomalous behavior (e.g., unexpected resource consumption, convergence failures, unusual metric fluctuations that could indicate subtle data poisoning effects not caught by pre-filtering or direct manipulation of training code), and ensuring training reproducibility and auditability.", implementationStrategies: ["Utilize dedicated, isolated, and hardened environments for model training activities, distinct from development or production serving environments.", "Apply the principle of least privilege for training jobs, granting only necessary access to data, code repositories, and computational resources.", "Monitor training metrics (e.g., loss functions, accuracy, gradient norms, learning rates) in real-time for unexpected spikes, drops, or patterns inconsistent with established training profiles for similar models/data.", "Implement automated checks for training stability, convergence within expected epochs, and prevention of issues like gradient starvation or explosion.", "Strictly version control all training code, configurations, dependencies (e.g., via requirements.txt or environment.yml), and environment snapshots (e.g., container images).", "Employ confidential computing (e.g., secure enclaves) for the training process when the training algorithm, intermediate model states, or proprietary data processing steps are highly sensitive.", "Log all training job parameters, code versions, data versions, and resulting metrics to ensure reproducibility and facilitate audits."], toolsOpenSource: ["MLOps platforms with experiment tracking and monitoring (e.g., MLflow, Kubeflow, ClearML).", "Containerization technologies (e.g., Docker, Podman) for creating reproducible training environments.", "Infrastructure monitoring tools (e.g., Prometheus, Grafana) adapted for training job metrics.", "Confidential Computing SDKs (e.g., Intel SGX SDK, Open Enclave SDK).", "Version control systems (e.g., Git, DVC)."], toolsCommercial: ["Cloud-based MLOps platforms (e.g., Amazon SageMaker, Google Vertex AI, Azure Machine Learning) with built-in training monitoring, security features, and experiment management.", "Commercial confidential computing services from cloud providers.", "Specialized AI training security monitoring solutions."], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0020 Poison Training Data (by detecting anomalous training dynamics caused by subtle poisoning)", "AML.T0019 Poison ML Model (if poisoning occurs through manipulation of the training process, code, or environment rather than just static model parameters)", "AML.T0008 ML Supply Chain Compromise (if a compromised development tool or library specifically targets and manipulates the training loop)."] }, { framework: "MAESTRO", items: ["Data Poisoning (L2: Data Operations, by monitoring its impact during training)", "Compromised Training Environment (L4: Deployment & Infrastructure)", "Resource Hijacking (L4: Deployment & & Infrastructure, if training resources are targeted by malware or unauthorized processes)", "Training Algorithm Manipulation (L1: Foundation Models or L3: Agent Frameworks)."] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM04:2025 Data and Model Poisoning (by providing an additional layer to detect sophisticated poisoning attempts that manifest during the training process itself)."] }, { framework: "OWASP ML Top 10 2023", items: ["ML02:2023 Data Poisoning Attack (detecting subtle or run-time effects)", "ML10:2023 Model Poisoning (if poisoning involves altering training code or runtime)."] }] },
                        { id: "AIDEFEND-H-008", name: "Robust Federated Learning Aggregation", description: "Implement and enforce secure aggregation protocols and defenses against malicious or unreliable client updates within Federated Learning (FL) architectures. This technique aims to prevent attackers controlling a subset of participating clients from disproportionately influencing, poisoning, or degrading the global model, or inferring information about other clients' data.", implementationStrategies: ["Employ secure aggregation schemes (e.g., those based on homomorphic encryption or secure multi-party computation) that allow the server to compute the sum of client updates without seeing individual contributions.", "Utilize robust aggregation rules (e.g., median, trimmed mean, Krum, Multi-Krum, FoolsGold, Byzantine-robust Stochastic Gradient Descent variants) designed to identify and mitigate the impact of outlier or malicious model updates from compromised clients.", "Implement client-side validation checks or anomaly detection on model updates before they are sent to the aggregation server.", "Monitor the statistical properties of updates received from clients over time to detect consistently anomalous or suspicious contributions.", "Consider client reputation systems or differential weighting based on historical contribution quality or trust levels in long-running FL systems.", "Combine with differential privacy applied to client updates before aggregation to further protect individual client data."], toolsOpenSource: ["TensorFlow Federated (TFF) (supports some secure aggregation algorithms).", "PySyft (OpenMined) (focuses on privacy-preserving ML, including FL with secure computation).", "Flower (framework adaptable for various FL strategies, including custom robust aggregation).", "NVIDIA FLARE (Federated Learning Application Runtime Environment).", "Libraries for robust statistics (e.g., parts of SciPy, specialized research libraries)."], toolsCommercial: ["Enterprise Federated Learning platforms (e.g., from Owkin, Substra Foundation, IBM) may offer built-in options for robust and secure aggregation.", "Solutions leveraging confidential computing for parts of the FL aggregation process."], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0020 Poison Training Data (specifically in the context of federated learning where malicious clients submit poisoned updates)", "AML.T0019 Poison ML Model (where the global model is poisoned via aggregation of malicious client models)."] }, { framework: "MAESTRO", items: ["Data Poisoning (L2: Data Operations, within FL setups)", "Model Skewing (L2: Data Operations, in FL)", "Attacks on Decentralized Learning (Cross-Layer)", "Inference Attacks against FL participants (if secure aggregation also provides confidentiality)."] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM04:2025 Data and Model Poisoning (especially relevant for distributed or federated fine-tuning/training of LLMs)."] }, { framework: "OWASP ML Top 10 2023", items: ["ML02:2023 Data Poisoning Attack (specifically in FL)", "ML10:2023 Model Poisoning (via compromised clients in FL)."] }] },
                        { id: "AIDEFEND-H-009", name: "AI Accelerator & Hardware Integrity", description: "Implement measures to protect the physical integrity and operational security of specialized AI hardware (GPUs, TPUs, NPUs, FPGAs) and the platforms hosting them against physical tampering, side-channel attacks (power, timing, EM), fault injection, and hardware Trojans. This aims to ensure the confidentiality and integrity of AI computations and model parameters processed by the hardware.", implementationStrategies: ["Utilize secure boot processes for AI accelerators and host systems.", "Employ hardware-based countermeasures against known side-channel attacks (e.g., noise injection, power supply randomization, shielded enclosures for critical components).", "Implement fault detection and response mechanisms in AI hardware and firmware.", "Source AI hardware from trusted supply chains and conduct physical inspections where feasible.", "Leverage Physical Unclonable Functions (PUFs) for device authentication and cryptographic key generation.", "Utilize secure enclaves/confidential computing for sensitive AI computations on accelerators where supported.", "Regularly update firmware for AI accelerators and supporting hardware components from verified sources.", "Monitor physical environmental conditions (temperature, voltage) around AI hardware for anomalies that could facilitate attacks."], toolsOpenSource: ["Open-source secure boot implementations (e.g., U-Boot with secure boot features).", "Firmware analysis tools (e.g., binwalk, firmadyne for inspection).", "Research tools for side-channel analysis and countermeasures."], toolsCommercial: ["Hardware Security Modules (HSMs) for key management related to AI hardware.", "Commercial confidential computing platforms that extend to accelerators (e.g., NVIDIA Confidential Computing).", "Specialized hardware integrity verification services.", "Vendor-specific security features in AI accelerators (e.g., secure enclaves within GPUs/TPUs)."], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0010.000 AI Supply Chain Compromise: Hardware", "AML.T0024.002 Extract ML Model (if extraction relies on side-channel attacks against hardware)", "AML.T0025 Exfiltration via Cyber Means (if side-channels are the means). (Potentially new ATLAS technique: \"Exploit AI Hardware Vulnerability\" or \"AI Hardware Side-Channel Attack\")."] }, { framework: "MAESTRO", items: ["Physical Tampering (L4: Deployment & Infrastructure)", "Side-Channel Attacks (L4: Deployment & Infrastructure / L1: Foundation Models if model parameters are leaked)", "Compromised Hardware Accelerators (L4)."] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM03:2025 Supply Chain (by ensuring integrity of underlying hardware components)", "LLM02:2025 Sensitive Information Disclosure (if disclosure is via hardware side-channels)."] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft (if theft is facilitated by hardware-level attacks)", "ML06:2023 AI Supply Chain Attacks (specifically hardware components)."] }] }
                    ]
                },
                {
                    name: "Detect",
                    purpose: "The \"Detect\" tactic focuses on the timely identification of intrusions, malicious activities, anomalous behaviors, or policy violations occurring within or targeting AI systems. This involves continuous or periodic monitoring of various aspects of the AI ecosystem, including inputs (prompts, data feeds), outputs (predictions, generated content, agent actions), model behavior (performance metrics, drift), system logs (API calls, resource usage), and the integrity of AI artifacts (models, datasets).",
                    techniques: [
                        { id: "AIDEFEND-D-001", name: "Adversarial Input & Prompt Injection Detection", description: "Implement mechanisms to continuously monitor and analyze inputs to AI models, specifically looking for characteristics indicative of adversarial manipulation or malicious prompt content. This includes detecting statistically anomalous inputs (e.g., out-of-distribution samples, inputs with unusual perturbation patterns) and scanning prompts for known malicious patterns, hidden commands, jailbreak sequences, or attempts to inject executable code or harmful instructions. The goal is to block, flag, or sanitize such inputs before they can significantly impact the model's behavior or compromise the system. For multimodal systems, this encompasses detecting manipulations across all relevant input types and identifying potential cross-modal attacks where, for instance, visual or audio input might carry instructions or payloads targeting textual processing components.", implementationStrategies: ["Deploy anomaly detection on input streams.", "Utilize specialized content scanners and LLM-specific detectors for prompts.", "Implement input reconstruction or defensive transformation techniques.", "Leverage ensemble models for output disagreement detection.", "Monitor for high query rates or specific query structures.", "Regularly update detection rules and models.", "In multimodal AI systems, deploy detection mechanisms tailored to each input modality to identify adversarial characteristics (e.g., perturbed images, audio adversarial examples). Additionally, analyze for suspicious cross-modal interactions, such as an image containing cleverly embedded textual instructions designed to exploit an LLM's visual-textual understanding capabilities, or QR codes attempting to redirect to malicious sites."], toolsOpenSource: ["vigil-llm", "Garak", "Adversarial Robustness Toolbox (ART)", "Alibi Detect", "LangChain", "Rebuff"], toolsCommercial: ["Microsoft Azure AI Content Safety", "Hugging Face Guardrails", "Lakera Guard", "Dedicated LLM Firewalls (Securiti, Protect AI, etc.)", "API security solutions"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0015 Evade ML Model", "AML.T0051 LLM Prompt Injection", "AML.T0054 LLM Jailbreak"] }, { framework: "MAESTRO", items: ["Adversarial Examples (L1)", "Input Validation Attacks (L3)", "Evasion of Security AI Agents (L6)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection"] }, { framework: "OWASP ML Top 10 2023", items: ["ML01:2023 Input Manipulation Attack"] }] },
                        { id: "AIDEFEND-D-002", name: "AI Model Anomaly & Performance Drift Detection", description: "Continuously monitor the outputs, performance metrics (e.g., accuracy, confidence scores, precision, recall, F1-score, output distribution), and potentially internal states or feature attributions of AI models during operation. This monitoring aims to detect significant deviations from established baselines or expected behavior. Such anomalies or drift can indicate various issues, including concept drift (changes in the underlying data distribution), data drift (changes in input data characteristics), or malicious activities like ongoing data poisoning attacks, subtle model evasion attempts, or model skewing.", implementationStrategies: ["Establish and maintain robust baseline of key model performance metrics.", "Track metrics in real-time or near real-time.", "Employ statistical concept drift and data drift detection algorithms.", "Monitor for unusual model decisions or output distributions.", "Investigate flagged anomalies and drift promptly.", "Implement feedback loops for model retraining/recalibration."], toolsOpenSource: ["Alibi Detect", "River", "Evidently AI", "NannyML", "scikit-multiflow", "TensorFlow Data Validation (TFDV), TensorFlow Model Analysis (TFMA)"], toolsCommercial: ["IBM Watson OpenScale", "Azure Model Monitor", "Fiddler AI, Arize AI, WhyLabs, Seldon Deploy", "Amazon SageMaker Model Monitor, Google Vertex AI Model Monitoring", "Protect AI (Layer product)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0020 Poison Training Data (detecting impact)", "AML.T0015 Evade ML Model (detecting anomalies)", "AML.T0021 Erode ML Model Integrity", "AML.T0019 Poison ML Model (detecting impact)"] }, { framework: "MAESTRO", items: ["Data Poisoning (L2, impact detection)", "Model Skewing (L2/L5)", "Unpredictable agent behavior / Performance Degradation (L5)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM04:2025 Data and Model Poisoning (impact detection)", "LLM09:2025 Misinformation (if drift leads to factual errors)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML02:2023 Data Poisoning Attack (impact detection)", "ML08:2023 Model Skewing", "ML10:2023 Model Poisoning (behavioral changes)"] }] },
                        { id: "AIDEFEND-D-003", name: "AI Output Monitoring & Policy Enforcement", description: "Actively inspect in near real-time the outputs generated by AI models (e.g., text responses from LLMs, classifications, predictions, agent actions, generated code or images) for malicious content, policy violations, leakage of sensitive information, or indications that an attack has succeeded or the model is misbehaving. This involves enforcing predefined safety, security, and ethical policies on the outputs, and taking action (e.g., blocking, sanitizing, alerting) when violations are detected. In multi-agent environments, this control also performs cross-agent factual-consistency verification to stop hallucination cascades before false information propagates through the agent network.", implementationStrategies: ["Deploy content filtering on AI outputs.", "For LLMs, implement reference checks against knowledge bases, fact-checking routines, output reconstruction checks, consistency checks with retrieved context (for RAG), or use secondary 'critic' / 'alignment' models to assess the truthfulness.", "For agentic AI, monitor actions against policies and permissions.", "Establish robust feedback loop for policy violations.", "Regularly update output policies and detection rules.", "Before any agent persists or forwards a fact to shared memory or downstream agents, require a cross-agent consensus check (e.g., ≥ N independent agents or an external knowledge graph) to confirm factual accuracy.","Reject or quarantine outputs that fail distributed fact-checking thresholds or that contradict previously verified knowledge.", "Attach provenance metadata to each accepted fact, recording which peer agents or data sources validated it.", "See also AIDEFEND-D-009 / AIDEFEND-D-010 / AIDEFEND-D-011 for complementary controls specific to multi-agent fact verification, goal-integrity, and rogue-agent detection."], toolsOpenSource: ["OpenAI Guardrails (custom coded)", "LangChain with custom output parsers/validation", "Meta's LlamaFirewall", "NVIDIA NeMo Guardrails", "Pydantic for output schema validation"], toolsCommercial: ["Anthropic's Claude monitoring features", "Crossing Minds WaiGuard", "Securiti LLM Firewall for Responses", "AI safety and content moderation platforms (Hive AI, Scale AI)", "AI observability platforms (Fiddler, Arize)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0057 LLM Data Leakage", "AML.T0048.002 External Harms: Societal Harm", "AML.T0051 LLM Prompt Injection / AML.T0054 LLM Jailbreak (detecting violating outputs)", "AML.T0009.002 Execution: LLM Plugin Compromise (detecting violating plugin output)"] }, { framework: "MAESTRO", items: ["Agent Tool Misuse, Agent Goal Manipulation (L7)", "Data Leakage through Observability (L5)", "Inaccurate Agent Capability Description leading to misuse"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM02:2025 Sensitive Information Disclosure", "LLM05:2025 Improper Output Handling", "LLM09:2025 Misinformation", "LLM06:2025 Excessive Agency"] }, { framework: "OWASP ML Top 10 2023", items: ["ML09:2023 Output Integrity Attack"] }] },
                        { id: "AIDEFEND-D-004", name: "Model & AI Artifact Integrity Audit & Tamper Detection", description: "Regularly verify the cryptographic integrity and authenticity of deployed AI models, their parameters, associated datasets, and critical components of their runtime environment. This process aims to detect any unauthorized modifications, tampering, or the insertion of backdoors that could compromise the model's behavior, security, or data confidentiality. It ensures that the AI artifacts in operation are the approved, untampered versions.", implementationStrategies: ["Maintain secure model registry with known-good hashes/signatures.", "Regularly compute checksums/verify signatures of deployed models.", "Utilize runtime attestation mechanisms (TEEs, TPMs).", "Log and analyze all model update events.", "Monitor for unexpected changes in model files, dependencies, or performance.", "Extend integrity checks to AI runtime configuration files."], toolsOpenSource: ["AIMSIC (research tool)", "Standard hashing (sha256sum) and signature tools (GnuPG)", "Tripwire (general FIM)", "Secure enclave attestation SDKs", "Git with signed commits/tags"], toolsCommercial: ["Protect AI Platform (NB Defense, ModelScan)", "XAPSec (AI model firewall)", "HiddenLayer Model Scanner", "MLOps platforms with artifact tracking", "Traditional FIM solutions"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0018 Backdoor ML Model / AML.T0019 Poison ML Model", "AML.T0009.001 Execution: ML Code Injection (if persistent changes)", "AML.T0021 Erode ML Model Integrity", "AML.T0008 ML Supply Chain Compromise (post-initial verification tampering)"] }, { framework: "MAESTRO", items: ["Backdoor Attacks (L1/L3)", "Data Tampering (L2, model parameters as files)", "Compromised Framework Components / Container Images (L3/L4)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM04:2025 Data and Model Poisoning (model poisoning aspect)", "LLM03:2025 Supply Chain (tampered model deployment)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML10:2023 Model Poisoning", "ML06:2023 AI Supply Chain Attacks (replacing legitimate files)"] }] },
                        { id: "AIDEFEND-D-005", name: "AI Activity Logging, Monitoring & Threat Hunting", description: "Establish and maintain detailed, comprehensive, and auditable logs of all significant activities related to AI systems. This includes user queries and prompts, model responses and confidence scores, decisions made by AI (especially autonomous agents), tools invoked by agents, data accessed or modified, API calls (to and from the AI system), system errors, and security-relevant events. These logs are then ingested into security monitoring systems (e.g., SIEM) for correlation, automated alerting on suspicious patterns, and proactive threat hunting by security analysts to identify indicators of compromise (IoCs) or novel attack patterns targeting AI systems.", implementationStrategies: ["Enable verbose logging for all AI interactions (inputs, outputs, agent actions, API calls, errors).", "Ensure logs are timestamped, immutable, and securely stored.", "Ingest AI-specific logs into centralized SIEM/log analytics.", "Correlate AI logs with other security data sources.", "Develop AI-specific detection rules and alerts in SIEM.", "Proactively search logs for subtle IoCs or anomalous behaviors.", "Utilize general AI/ML techniques within the SOC to help identify anomalous patterns in the Al system logs themselves, and integrate findings from specialized AI-Driven Security Analytics for AI Systems (AIDEFEND-D-008) for more targeted defense.", "Log all activations of HITL control points, manual overrides, and emergency halts for comprehensive audit trails and review, cross-referencing with designs from AIDEFEND-M-006.", "For agentic AI, log agent goals, plans, decisions, tool selections, tool inputs/outputs, interactions with external knowledge bases (e.g., vector DB queries and retrieved results for RAG), and any state changes.", "Log agent-attestation status, behavioural-fingerprint IDs, trust scores, and code-signing hashes for every running agent session so SOC analytics can correlate rogue-agent indicators in real time.", "See also AIDEFEND-D-009 / AIDEFEND-D-010 / AIDEFEND-D-011 for complementary controls specific to multi-agent fact verification, goal-integrity, and rogue-agent detection."], toolsOpenSource: ["ELK Stack (EFor autonomous agents, record the cryptographically signed mission objectives and goal hierarchy in the model card; these become the reference used by runtime Goal-Integrity Monitoring (see D-010).lasticsearch, Logstash, Kibana) or OpenSearch Stack", "Grafana Loki", "Sigma (for SIEM rules)", "Fluentd or Vector", "MLOps framework logging (e.g., MLflow)"], toolsCommercial: ["Splunk (Enterprise, Cloud, SOAR)", "Datadog, Dynatrace, New Relic", "Cloud-native SIEMs (Azure Sentinel, Google Chronicle, AWS Security Hub)", "HiddenLayer MLDR", "AI-SPM tools"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0024.002 Extract ML Model (query patterns)", "AML.T0001 Reconnaissance (unusual queries)", "AML.T0051 LLM Prompt Injection (repeated attempts)", "AML.T0057 LLM Data Leakage (output logging)", "AML.T0012 Valid Accounts (anomalous usage)"] }, { framework: "MAESTRO", items: ["Model Stealing (L1)", "Agent Tool Misuse (L7)", "Compromised RAG Pipelines (L2)", "Data Exfiltration (L2)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM10:2025 Unbounded Consumption (usage patterns)", "LLM01:2025 Prompt Injection (logged attempts)", "LLM02:2025 Sensitive Information Disclosure (logged outputs)", "LLM06:2025 Excessive Agency (logged actions)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft (query patterns)", "ML01:2023 Input Manipulation Attack (logged inputs)"] }] },
                        { id: "AIDEFEND-D-006", name: "Explainability (XAI) Manipulation Detection", description: "Implement mechanisms to monitor and validate the outputs and behavior of eXplainable AI (XAI) methods. The goal is to detect attempts by adversaries to manipulate or mislead these explanations, ensuring that XAI outputs accurately reflect the model's decision-making process and are not crafted to conceal malicious operations, biases, or vulnerabilities. This is crucial if XAI is used for debugging, compliance, security monitoring, or building user trust.", implementationStrategies: ["Employ multiple, diverse XAI methods to explain the same model decision and compare their outputs for consistency; significant divergence can indicate manipulation or instability.", "Establish baselines for typical explanation characteristics (e.g., feature importance, rule sets, prototype examples) on known, benign inputs and monitor for deviations.", "Detect instability in explanations where small, inconsequential perturbations to input data lead to drastically different explanations for the same model prediction.", "Monitor for explanations that are overly simplistic for known complex decisions, that consistently highlight irrelevant or nonsensical features, or that fail to identify features known to be critical.", "Specifically test against adversarial attacks designed to fool XAI methods (e.g., \"adversarial explanations\" where the explanation is misleading but the prediction remains unchanged or changes benignly).", "Log XAI outputs and any detected manipulation alerts for investigation by AI assurance teams."], toolsOpenSource: ["XAI libraries (e.g., SHAP, LIME, Captum for PyTorch, Alibi Explain, ELI5, InterpretML).", "Custom-developed logic for comparing and validating consistency between different explanation outputs.", "Research toolkits for adversarial attacks on XAI (if available for benchmarking)."], toolsCommercial: ["AI Observability and Monitoring platforms (e.g., Fiddler, Arize AI, WhyLabs) that include XAI features may incorporate or allow the development of robustness checks and manipulation detection for explanations.", "Specialized AI assurance or red teaming tools that assess XAI method reliability."], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0006 Defense Evasion (if XAI is part of a defensive monitoring system and is itself targeted to be fooled). Potentially a new ATLAS technique: \"AML.TXXXX Manipulate AI Explainability\"."] }, { framework: "MAESTRO", items: ["Evasion of Auditing/Compliance (L6: Security & Compliance, if manipulated XAI is used to mislead auditors)", "Manipulation of Evaluation Metrics (L5: Evaluation & Observability, if explanations are used as part of the evaluation and are unreliable)", "Obfuscation of Malicious Behavior (Cross-Layer)."] }, { framework: "OWASP LLM Top 10 2025", items: ["Indirectly supports investigation of LLM01:2025 Prompt Injection or LLM04:2025 Data and Model Poisoning by ensuring that any XAI methods used to understand the resulting behavior are themselves trustworthy."] }, { framework: "OWASP ML Top 10 2023", items: ["Indirectly supports diagnosis of ML08:2023 Model Skewing or ML10:2023 Model Poisoning, by ensuring XAI methods used to identify these issues are not being manipulated."] }] },
                        { id: "AIDEFEND-D-007", name: "Multimodal Inconsistency Detection & Defense", description: "For AI systems processing multiple input modalities (e.g., text, image, audio, video), implement mechanisms to detect and respond to inconsistencies, contradictions, or malicious instructions hidden via cross-modal interactions. This involves analyzing inputs and outputs across modalities to identify attempts to bypass security controls or manipulate one modality using another, and applying defenses to mitigate such threats.", implementationStrategies: ["Implement semantic consistency checks between information extracted from different modalities (e.g., verify alignment between text captions and image content; ensure audio commands do not contradict visual cues).", "Scan non-primary modalities for embedded instructions or payloads intended for other modalities (e.g., steganographically hidden text in images, QR codes containing malicious prompts, audio watermarks with commands).", "Utilize separate, specialized validation and sanitization pipelines for each modality before data fusion (as outlined in enhancements to AIDEFEND-H-002).", "Monitor the AI model's internal attention mechanisms (if accessible and interpretable) for unusual or forced cross-modal attention patterns that might indicate manipulation.", "Develop and maintain a library of known cross-modal attack patterns and use this knowledge to inform detection rules and defensive transformations.", "During output generation, verify that outputs are consistent with the fused understanding from all input modalities and do not disproportionately reflect manipulation from a single, potentially compromised, modality.", "Employ ensemble methods where different sub-models or experts process different modalities, with a final decision layer that checks for consensus or flags suspicious discrepancies for human review or automated rejection.", "Implement context-aware filtering that considers the typical relationships and constraints between modalities for a given task."], toolsOpenSource: ["Computer vision libraries (OpenCV, Pillow) for image analysis (e.g., detecting text in images, QR code scanning, deepfake detection).", "NLP libraries (spaCy, NLTK, Hugging Face Transformers) for text analysis and cross-referencing with visual/audio data.", "Audio processing libraries (Librosa, PyAudio, SpeechRecognition) for audio analysis and transcription for cross-checking.", "Steganography detection tools (e.g., StegDetect, Aletheia, Zsteg).", "Custom rule engines (e.g., based on Drools, or custom Python scripting) for implementing consistency checks.", "Multimodal foundation models themselves (e.g., fine-tuned smaller models acting as \"watchdogs\" for larger ones)."], toolsCommercial: ["Multimodal AI security platforms (emerging market, offering integrated analysis).", "Advanced data validation platforms with support for multiple data types and cross-validation.", "Content moderation services that handle and analyze multiple modalities for policy violations or malicious content.", "AI red teaming services specializing in multimodal systems."], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0051 LLM Prompt Injection (specifically cross-modal variants like in Scenario #7 of LLM01:2025 )", "AML.T0015 Evade ML Model (if evasion exploits multimodal vulnerabilities)", "AML.T0043 Craft Adversarial Data (for multimodal adversarial examples)."] }, { framework: "MAESTRO", items: ["Cross-Modal Manipulation Attacks (L1: Foundation Models / L2: Data Operations)", "Input Validation Attacks (L3: Agent Frameworks, for multimodal inputs)", "Data Poisoning (L2: Data Operations, if multimodal data is used for poisoning and inconsistencies are introduced)."] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection (specifically Multimodal Injection Scenario #7)", "LLM04:2025 Data and Model Poisoning (if using tainted or inconsistent multimodal data)", "LLM08:2025 Vector and Embedding Weaknesses (if multimodal embeddings are manipulated or store inconsistent data)."] }, { framework: "OWASP ML Top 10 2023", items: ["ML01:2023 Input Manipulation Attack (specifically for multimodal inputs)", "ML02:2023 Data Poisoning Attack (using inconsistent or malicious multimodal data)."] }] },
                        { id: "AIDEFEND-D-008", name: "AI-Driven Security Analytics for AI Systems", description: "Employ specialized AI/ML models (secondary AI defenders) to analyze telemetry, logs, and behavioral patterns from primary AI systems to detect sophisticated, subtle, or novel attacks that may evade rule-based or traditional detection methods. This includes identifying anomalous interactions, emergent malicious behaviors, coordinated attacks, or signs of AI-generated attacks targeting the primary AI systems.", implementationStrategies: ["Train anomaly detection models (e.g., autoencoders, GMMs, Isolation Forests) on logs and telemetry from AI systems, including API call sequences, resource usage patterns, query structures, and agent actions.", "Develop ML classifiers (e.g., SVM, Random Forest, Gradient Boosting, NNs) to categorize interactions as benign or potentially malicious based on learned patterns from known attacks and normal behavior baselines.", "Use AI for advanced threat hunting within AI system logs, identifying complex attack sequences, low-and-slow reconnaissance, or unusual data access patterns by AI agents.", "Implement AI-based systems to continuously monitor for concept drift, data drift, or sudden performance degradation in primary AI models that might indicate an ongoing, subtle attack (complementing AIDEFEND-D-002).", "Apply AI to analyze the behavior of AI agents (e.g., sequences of tool usage, goal achievement patterns) for deviations from intended goals or ethical guidelines, potentially indicating manipulation, hijacking, or emergent undesirable behaviors.", "Continuously retrain and update these secondary AI defender models with new attack data, evolving system behaviors, and feedback from incident response.", "Integrate outputs and alerts from AI defender models into the main SIEM/SOAR platforms for correlation, prioritization, and automated response orchestration.", "Consider ensemble methods for secondary AI defenders to improve robustness and reduce false positives."], toolsOpenSource: ["General ML libraries (Scikit-learn, TensorFlow, PyTorch, Keras) for building custom detection models.", "Anomaly detection libraries (PyOD, Alibi Detect, TensorFlow Probability).", "Log analysis platforms (ELK Stack/OpenSearch with ML plugins, Apache Spot).", "Streaming data processing frameworks (Apache Kafka, Apache Flink, Apache Spark Streaming) for real-time AI analytics.", "Graph-based analytics libraries (NetworkX, PyTorch Geometric) for analyzing relationships in AI system activity."], toolsCommercial: ["Security AI platforms that offer AI-on-AI monitoring capabilities (e.g., some advanced EDR/XDR features, User and Entity Behavior Analytics (UEBA) tools).", "Specialized AI security monitoring solutions focusing on AI workload protection.", "AI-powered SIEMs or SOAR platforms with advanced analytics modules.", "Cloud provider ML services for building and deploying custom monitoring models (e.g., SageMaker, Vertex AI, Azure ML)."], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["Many tactics by providing an advanced detection layer. Particularly useful against novel or evasive variants of AML.T0015 Evade ML Model, AML.T0051 LLM Prompt Injection, AML.T0024.002 Extract ML Model, AML.T0006 Active Scanning & Probing, and sophisticated reconnaissance activities (AML.TA0001). Could also help detect AI-generated attacks if their patterns differ from human-initiated ones."] }, { framework: "MAESTRO", items: ["Advanced Evasion Techniques (L1, L5, L6)", "Subtle Data or Model Poisoning effects not caught by simpler checks (L1, L2)", "Sophisticated Agent Manipulation (L7)", "Novel Attack Vectors (Cross-Layer)", "Resource Hijacking (L4, through anomalous pattern detection)."] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection (novel or obfuscated injections)", "LLM06:2025 Excessive Agency (subtle deviations in agent behavior)", "LLM10:2025 Unbounded Consumption (anomalous resource usage patterns indicating DoS or economic attacks)."] }, { framework: "OWASP ML Top 10 2023", items: ["ML01:2023 Input Manipulation Attack (sophisticated adversarial inputs)", "ML05:2023 Model Theft (anomalous query patterns indicative of advanced extraction)", "ML02:2023 Data Poisoning Attack (detecting subtle behavioral shifts post-deployment)."] }] 
                        },
                        {
    "id": "AIDEFEND-D-009",
    "name": "Cross-Agent Fact Verification & Hallucination Cascade Prevention",
    "description": "Implement real-time fact verification and consistency checking mechanisms across multiple AI agents to detect and prevent the propagation of hallucinated or false information through agent networks. This technique employs distributed consensus algorithms, external knowledge base validation, and inter-agent truth verification to break hallucination cascades before they spread through the system.",
    "implementationStrategies": [
      "Deploy distributed fact-checking algorithms that cross-reference agent outputs with multiple trusted knowledge sources before accepting information as factual",
      "Implement inter-agent consensus mechanisms where critical facts must be verified by multiple independent agents before being accepted into shared knowledge bases",
      "Utilize external authoritative data sources (APIs, databases, knowledge graphs) for real-time fact verification of agent-generated content",
      "Deploy contradiction detection algorithms that identify when agents produce conflicting information about the same facts",
      "Implement confidence scoring for agent-generated facts, with lower confidence facts requiring additional verification before propagation",
      "Create fact provenance tracking to trace the origin and validation history of information across agent interactions",
      "Deploy circuit breakers that halt information propagation when hallucination indicators exceed threshold levels"
    ],
    "toolsOpenSource": [
      "Apache Kafka with custom fact-verification consumers for distributed fact checking",
      "Neo4j or ArangoDB for knowledge graph-based fact verification",
      "Apache Airflow for orchestrating complex fact-verification workflows",
      "Redis or Apache Ignite for high-speed fact caching and consistency checking",
      "Custom Python libraries using spaCy, NLTK for natural language fact extraction and comparison"
    ],
    "toolsCommercial": [
      "Google Knowledge Graph API for external fact verification",
      "Microsoft Cognitive Services for content verification",
      "Palantir Foundry for large-scale data consistency and verification",
      "Databricks with MLflow for distributed ML-based fact verification",
      "Neo4j Enterprise for enterprise-grade knowledge graph verification"
    ],
    "defendsAgainst": [
      {
        "framework": "MITRE ATLAS",
        "items": ["AML.T0021 Erode ML Model Integrity", "AML.T0048.002 External Harms: Societal Harm"]
      },
      {
        "framework": "MAESTRO",
        "items": ["Data Poisoning (L2)", "Cross-Modal Manipulation Attacks (L1)"]
      },
      {
        "framework": "OWASP LLM Top 10 2025",
        "items": ["LLM09:2025 Misinformation"]
      },
      {
        "framework": "OWASP ML Top 10 2023",
        "items": ["ML08:2023 Model Skewing"]
      }
    ]
  },
  {
    "id": "AIDEFEND-D-010",
    "name": "AI Goal Integrity Monitoring & Deviation Detection",
    "description": "Continuously monitor and validate AI agent goals, objectives, and decision-making patterns to detect unauthorized goal manipulation or intent deviation. This technique establishes cryptographically signed goal states, implements goal consistency verification, and provides real-time alerting when agents deviate from their intended objectives or exhibit goal manipulation indicators.",
    "implementationStrategies": [
      "Implement cryptographic signing of agent goals and objectives to prevent unauthorized modification",
      "Deploy continuous goal consistency checking algorithms that verify agent actions align with stated objectives",
      "Create goal deviation scoring systems that quantify how far agent behavior has drifted from intended goals",
      "Implement multi-agent goal verification where critical goal changes require consensus from multiple oversight agents",
      "Deploy behavioral pattern analysis to detect subtle goal manipulation that doesn't trigger direct goal modification alerts",
      "Create goal rollback mechanisms to restore agents to previous validated goal states when manipulation is detected",
      "Implement goal provenance tracking to audit the complete history of goal modifications and their sources"
    ],
    "toolsOpenSource": [
      "HashiCorp Vault for cryptographic goal signing and verification",
      "Apache Kafka for real-time goal monitoring event streaming",
      "Prometheus and Grafana for goal deviation metrics and alerting",
      "Redis for fast goal state caching and comparison",
      "Custom Python frameworks using cryptography libraries for goal integrity verification"
    ],
    "toolsCommercial": [
      "CyberArk for privileged goal management and protection",
      "Splunk for advanced goal deviation analytics and correlation",
      "Datadog for real-time goal monitoring and alerting",
      "HashiCorp Vault Enterprise for enterprise goal state management",
      "IBM QRadar for goal manipulation threat detection"
    ],
    "defendsAgainst": [
      {
        "framework": "MITRE ATLAS",
        "items": ["AML.T0051 LLM Prompt Injection", "AML.T0054 LLM Jailbreak"]
      },
      {
        "framework": "MAESTRO",
        "items": ["Agent Goal Manipulation (L7)", "Input Validation Attacks (L3)"]
      },
      {
        "framework": "OWASP LLM Top 10 2025",
        "items": ["LLM01:2025 Prompt Injection", "LLM06:2025 Excessive Agency"]
      },
      {
        "framework": "OWASP ML Top 10 2023",
        "items": ["ML01:2023 Input Manipulation Attack"]
      }
    ]
  },
  {
    "id": "AIDEFEND-D-011",
    "name": "Agent Behavioral Attestation & Rogue Detection",
    "description": "Implement continuous behavioral monitoring and attestation mechanisms to identify rogue or compromised agents in multi-agent systems. This technique uses behavioral fingerprinting, anomaly detection, and peer verification to detect agents that deviate from expected behavioral patterns or exhibit malicious characteristics.",
    "implementationStrategies": [
      "Create behavioral fingerprints for each agent based on normal operational patterns, decision-making characteristics, and interaction styles",
      "Deploy peer-based agent verification where agents cross-validate each other's behaviors and report anomalies",
      "Implement continuous behavioral scoring that tracks agent trustworthiness based on historical actions and decisions",
      "Create agent quarantine mechanisms that automatically isolate agents exhibiting rogue behavior pending investigation",
      "Deploy behavioral drift detection to identify gradual changes in agent behavior that might indicate compromise",
      "Implement agent population monitoring to detect unauthorized agent introduction or agent impersonation",
      "Create behavioral consensus mechanisms where critical decisions require verification from multiple trusted agents"
    ],
    "toolsOpenSource": [
      "scikit-learn for behavioral pattern analysis and anomaly detection",
      "Apache Kafka for real-time behavioral event streaming",
      "InfluxDB for time-series behavioral data storage",
      "Grafana for behavioral monitoring dashboards",
      "Custom frameworks using TensorFlow/PyTorch for deep behavioral analysis"
    ],
    "toolsCommercial": [
      "Splunk for advanced behavioral analytics and correlation",
      "Darktrace for AI-powered behavioral anomaly detection",
      "IBM QRadar for behavioral threat intelligence",
      "Microsoft Sentinel for cloud-based behavioral monitoring",
      "Vectra AI for network behavioral analysis adapted for agent monitoring"
    ],
    "defendsAgainst": [
      {
        "framework": "MITRE ATLAS",
        "items": ["AML.T0017 Persistence", "AML.T0048 External Harms"]
      },
      {
        "framework": "MAESTRO",
        "items": ["Rogue Agent Behavior (L7)", "Agent Identity Attack (L7)"]
      },
      {
        "framework": "OWASP LLM Top 10 2025",
        "items": ["LLM06:2025 Excessive Agency"]
      },
      {
        "framework": "OWASP ML Top 10 2023",
        "items": ["ML06:2023 AI Supply Chain Attacks"]
      }
    ]
  }
                    ]
                },
                {
                    name: "Isolate",
                    purpose: "The \"Isolate\" tactic involves implementing measures to contain malicious activity and limit its potential spread or impact should an AI system or one of its components become compromised. This includes sandboxing AI processes, segmenting networks to restrict communication, and establishing mechanisms to quickly quarantine or throttle suspicious interactions or misbehaving AI entities.",
                    techniques: [
                        { id: "AIDEFEND-I-001", name: "AI Execution Sandboxing & Runtime Isolation", description: "Execute AI models, autonomous agents, or individual AI tools and plugins within isolated environments such as sandboxes, containers, or microVMs. These environments must be configured with strict limits on resources (CPU, memory, GPU, network bandwidth), permissions (filesystem access, system calls, network connectivity), and access to other systems. The primary goal is that if an AI component is compromised or behaves maliciously (e.g., due to prompt injection leading to arbitrary code execution via a tool), the impact is confined to the isolated sandbox, preventing harm to the host system or lateral movement to other parts of the network. This is particularly crucial for running untrusted code, processing untrusted inputs, or when AI agents interact with external, potentially malicious, environments.", implementationStrategies: ["Deploy AI in hardened containers (Docker, Kubernetes) with network policies and resource quotas.", "Use OS-level sandboxing (seccomp, AppArmor) or user-space kernels (gVisor) for untrusted code.", "Employ lightweight VMs (Firecracker, Kata Containers) for stronger isolation.", "Set strict resource quotas for sandboxed AI processes.", "Leverage confidential computing environments (Nitro Enclaves, Azure Confidential Computing).", "Ensure sandboxed environments have minimal privileges and limited network access.", "For AI agents interacting with external APIs or tools, ensure each tool call is sandboxed or its scope is strictly limited to prevent unintended system interactions if the agent or tool logic is compromised."], toolsOpenSource: ["Docker, Podman", "Kubernetes", "gVisor", "Kata Containers", "Firecracker", "seccomp, AppArmor, SELinux", "Firejail"], toolsCommercial: ["Cloud provider sandboxing solutions (Azure Container Instances with confidential options, SageMaker isolation, GKE sandboxing)", "RunSafe Aligned", "Confidential Computing platforms"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0009 Execution (confines impact)", "AML.T0010 Privilege Escalation (prevents host compromise)", "AML.T0017 Persistence (limits scope)"] }, { framework: "MAESTRO", items: ["Compromised Container Images / Orchestration Attacks (L4)", "Agent Tool Misuse (L7, sandboxing tool execution)", "Resource Hijacking (L4)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM06:2025 Excessive Agency (limits agent environment)", "LLM05:2025 Improper Output Handling (contains code execution fallout)", "LLM03:2025 Supply Chain (limits harm from compromised component)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML06:2023 AI Supply Chain Attacks (confines malicious component)"] }] },
                        { id: "AIDEFEND-I-002", name: "Network Segmentation & Isolation for AI Systems", description: "Implement network segmentation and microsegmentation strategies to isolate AI systems and their components (e.g., training environments, model serving endpoints, data stores, agent control planes) from general corporate networks and other critical IT/OT systems. This involves enforcing strict communication rules through firewalls, proxies, and network policies to limit an attacker's ability to pivot from a compromised AI component to other parts of the network, or to exfiltrate data to unauthorized destinations. This technique reduces the \"blast radius\" of a security incident involving an AI system.", implementationStrategies: ["Host critical AI components on dedicated network segments (VLANs, VPCs).", "Apply least privilege to network communications for AI systems.", "Utilize API gateways or forward proxies to mediate and control AI traffic.", "Implement microsegmentation (SDN, service mesh, host-based firewalls).", "Separate development/testing environments from production.", "Regularly review and audit network segmentation rules."], toolsOpenSource: ["Linux Netfilter (iptables, nftables), firewalld", "Kubernetes Network Policies", "Service Mesh (Istio, Linkerd, Kuma)", "CNI plugins (Calico, Cilium)", "Open-source API Gateways (Kong, Tyk, APISIX)"], toolsCommercial: ["Microsegmentation platforms (Illumio, Guardicore, Cisco Secure Workload, Akamai Guardicore)", "Next-Generation Firewalls (NGFWs)", "Cloud-native firewall services (AWS Network Firewall, Azure Firewall, Google Cloud Firewall)", "Commercial API Gateway solutions"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0025 Exfiltration via Cyber Means", "General Lateral Movement tactics", "AML.T0003 Resource Development (blocking unauthorized downloads)"] }, { framework: "MAESTRO", items: ["Data Exfiltration (L2/Cross-Layer)", "Lateral Movement (Cross-Layer)", "Compromised RAG Pipelines (L2, isolating components)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM02:2025 Sensitive Information Disclosure (limits exfil paths)", "LLM03:2025 Supply Chain (isolating third-party components)", "LLM06:2025 Excessive Agency (limits reach of compromised agent)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft (isolating repositories)", "ML06:2023 AI Supply Chain Attacks (segmenting components)"] }] },
                        { id: "AIDEFEND-I-003", name: "Quarantine & Throttling of AI Interactions", description: "Implement mechanisms to automatically or manually isolate, rate-limit, or place into a restricted \"safe mode\" specific AI system interactions when suspicious activity is detected. This could apply to individual user sessions, API keys, IP addresses, or even entire AI agent instances. The objective is to prevent potential attacks from fully executing, spreading, or causing significant harm by quickly containing or degrading the capabilities of the suspicious entity. This is an active response measure triggered by detection systems.", implementationStrategies: ["Automated quarantine based on high-risk behavior alerts (cut access, move to honeypot, disable key/account).", "Dynamic rate limiting for anomalous behavior (query spikes, complex queries).", "Stricter rate limits for unauthenticated/less trusted users.", "Design AI systems with a \"safe mode\" or degraded functionality state.", "Utilize SOAR platforms to automate quarantine/throttling actions."], toolsOpenSource: ["Fail2Ban (adapted for AI logs)", "Custom scripts (Lambda, Azure Functions, Cloud Functions) for automated actions", "API Gateways (Kong, Tyk, Nginx) for rate limiting", "Kubernetes for resource quotas/isolation"], toolsCommercial: ["API Security and Bot Management solutions (Cloudflare, Akamai, Imperva)", "ThreatWarrior (automated detection/response)", "SIEM/SOAR platforms (Splunk SOAR, Palo Alto XSOAR, IBM QRadar SOAR)", "WAFs with advanced rate limiting"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0024.002 Extract ML Model (rate-limiting)", "AML.T0029 Denial of ML Service (throttling)", "AML.T0034 Cost Harvesting (limiting rates)", "Active exploitation scenarios (quarantine stops)"] }, { framework: "MAESTRO", items: ["Model Stealing (L1, throttling)", "DoS on Framework APIs / Data Infrastructure (L3/L2)", "Resource Hijacking (L4, containing processes)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM10:2025 Unbounded Consumption (throttling/quarantining)", "LLM01:2025 Prompt Injection (quarantining repeat offenders)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft (throttling excessive queries)"] }] },
                        { id: "AIDEFEND-I-004", name: "Agent Memory & State Isolation", description: "Specifically for agentic AI systems, implement mechanisms to isolate and manage the agent's memory (e.g., conversational context, short-term state, knowledge retrieved from vector databases) and periodically reset or flush it. This defense aims to prevent malicious instructions, poisoned data, or exploited states (e.g., a \"jailbroken\" state) from persisting across multiple interactions, sessions, or from affecting other unrelated agent tasks or instances. It helps to limit the temporal scope of a successful manipulation.", implementationStrategies: ["Implement per-session/per-user conversational context.", "Regularly flush or use short context windows for agent interactions.", "Partition long-term memory (vector DBs) based on trust levels/contexts.", "Implement strict validation/filtering for writes to agent long-term memory.", "Validate and sanitize persisted state information before loading.", "Consider periodic resets of volatile memory for long-running agents.", "Implement checks to prevent an agent from writing overly long or computationally expensive data into shared memory stores that could lead to denial of service for other agents or processes accessing that memory."], toolsOpenSource: ["LangChain Guardrails or custom callback handlers", "Custom wrappers in agentic frameworks (AutoGen, CrewAI, Semantic Kernel, LlamaIndex)", "Vector databases (Weaviate, Qdrant, Pinecone) with access controls"], toolsCommercial: ["Humane AI Safety Tools (proprietary memory management)", "Oracle AI Interceptors (conceptual)", "Lasso Security (agent memory lineage/monitoring)", "Enterprise agent platforms with secure state management"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0018.001 Backdoor ML Model: Poison LLM Memory", "AML.T0017 Persistence (preventing long-term state manipulation)", "AML.T0051 LLM Prompt Injection (limits impact duration)"] }, { framework: "MAESTRO", items: ["Agent Goal Manipulation / Agent Tool Misuse (L7, preventing persistent manipulated state)", "Data Poisoning (L2, if agent memory is target)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection (non-persistent malicious context)", "LLM04:2025 Data and Model Poisoning (agent memory as poisoned data)", "LLM08:2025 Vector and Embedding Weaknesses (mitigating malicious data in vector DB)"] }, { framework: "OWASP ML Top 10 2023", items: ["Relevant if agent memory is considered part of model state/operational data."] }] },
                        { id: "AIDEFEND-I-005", name: "Emergency \"Kill-Switch\" / AI System Halt", description: "Establish and maintain a reliable, rapidly invokable mechanism to immediately halt, disable, or severely restrict the operation of an AI model or autonomous agent if it exhibits confirmed critical malicious behavior, goes \"rogue\" (acts far outside its intended parameters in a harmful way), or if a severe, ongoing attack is detected and other containment measures are insufficient. This is a last-resort containment measure designed to prevent catastrophic harm or further compromise.", implementationStrategies: ["Implement automated safety monitors and triggers for critical deviations.", "Provide secure, MFA-protected manual override for human operators.", "Design agents with internal, independent watchdog modules.", "Define clear protocols for kill-switch activation and recovery.", "Develop procedures for safely restarting and verifying AI system post-halt.", "Ensure kill-switch mechanisms are aligned with, and their operational procedures are documented in, the HITL Control Point Mapping (AIDEFEND-M-006) to ensure clarity on manual activation and authority [referencing existing concepts in 507-509]."], toolsOpenSource: ["AI Sentinel (conceptual pattern)", "Custom scripts/automation playbooks (Ansible, cloud CLIs) to stop/delete resources", "Circuit breaker patterns in microservices"], toolsCommercial: ["\"Red Button\" solutions from AI platform vendors", "Edge AI Safeguard solutions", "Alarm.com AI Deterrence (conceptual similarity)", "EDR/XDR solutions (SentinelOne, CrowdStrike) to kill processes/isolate hosts"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0048 External Harms (Societal, Financial, Reputational, User)", "AML.T0029 Denial of ML Service (by runaway agent)", "AML.T0017 Persistence (terminating malicious agent)"] }, { framework: "MAESTRO", items: ["Agent acting on compromised goals/tools leading to severe harm (L7)", "Runaway/critically malfunctioning foundation models (L1)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM06:2025 Excessive Agency (ultimate backstop)", "LLM10:2025 Unbounded Consumption (preventing catastrophic costs)"] }, { framework: "OWASP ML Top 10 2023", items: ["Any ML attack scenario causing immediate, unacceptable harm requiring emergency shutdown."] }] }
                    ]
                },
                {
                    name: "Deceive",
                    purpose: "The \"Deceive\" tactic involves the strategic use of decoys, misinformation, or the manipulation of an adversary's perception of the AI system and its environment. The objectives are to misdirect attackers away from real assets, mislead them about the system's true vulnerabilities or value, study their attack methodologies in a safe environment, waste their resources, or deter them from attacking altogether.",
                    techniques: [
                        { id: "AIDEFEND-DV-001", name: "Honeypot AI Services & Decoy Models/APIs", description: "Deploy decoy AI systems, such as fake LLM APIs, ML model endpoints serving synthetic or non-sensitive data, or imitation agent services, that are designed to appear valuable, vulnerable, or legitimate to potential attackers. These honeypots are instrumented for intensive monitoring to log all interactions, capture attacker TTPs (Tactics, Techniques, and Procedures), and gather threat intelligence without exposing real production systems or data. They can also be used to slow down attackers or waste their resources.", implementationStrategies: ["Set up AI model instances with controlled weaknesses/attractive characteristics.", "Instrument honeypot AI service for detailed logging.", "Design honeypots to mimic production services but ensure isolation.", "Consider honeypots with slow/slightly erroneous responses.", "Integrate honeypot alerts with SIEM/SOC.", "Seed LLM honeypots with trigger phrases or known jailbreak susceptibility."], toolsOpenSource: ["General honeypot frameworks (Cowrie, Dionaea, Conpot) adapted", "Sandboxed open-source LLM as honeypot", "Mock API tools (MockServer, WireMock)"], toolsCommercial: ["Deception technology platforms (TrapX, SentinelOne ShadowPlex, Illusive, Acalvio)", "Specialized AI security vendors with AI honeypot capabilities"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0001 Reconnaissance", "AML.T0024.002 Extract ML Model / AML.T0048.004 External Harms: ML Intellectual Property Theft", "AML.T0051 LLM Prompt Injection / AML.T0054 LLM Jailbreak"] }, { framework: "MAESTRO", items: ["Model Stealing (L1)", "Marketplace Manipulation (L7, decoy agents)", "Evasion of Detection (L5, studying evasion attempts)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection (capturing attempts)", "LLM10:2025 Unbounded Consumption (studying resource abuse)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft (luring to decoy)", "ML01:2023 Input Manipulation Attack (observing attempts)"] }] },
                        { id: "AIDEFEND-DV-002", name: "Honey Data, Decoy Artifacts & Canary Tokens for AI", description: "Strategically seed the AI ecosystem (training datasets, model repositories, configuration files, API documentation) with enticing but fake data, decoy model artifacts (e.g., a seemingly valuable but non-functional or instrumented model file), or canary tokens (e.g., fake API keys, embedded URLs in documents). These \"honey\" elements are designed to be attractive to attackers. If an attacker accesses, exfiltrates, or attempts to use these decoys, it triggers an alert, signaling a breach or malicious activity and potentially providing information about the attacker's actions or location.", implementationStrategies: ["Embed unique, synthetic honey records in datasets/databases.", "Publish fake/instrumented decoy model artifacts.", "Create and embed decoy API keys/access tokens (Canary Tokens).", "Embed trackable URLs/web bugs in fake sensitive documents.", "Watermark synthetic data in honeypots/decoys.", "Ensure honey elements are isolated and cannot impact production.", "Integrate honey element alerts into security monitoring."], toolsOpenSource: ["Canarytokens.org by Thinkst", "Synthetic data generation tools (Faker, SDV)", "Custom scripts for decoy files/API keys"], toolsCommercial: ["Thinkst Canary (commercial platform)", "Deception platforms (Illusive, Acalvio, SentinelOne) with data decoy capabilities", "Some DLP solutions adaptable for honey data"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0025 Exfiltration via Cyber Means (honey data/canaries exfiltrated)", "AML.T0024.002 Extract ML Model (decoy model/canary in docs)", "AML.T0008 ML Supply Chain Compromise (countering with fake vulnerable models)"] }, { framework: "MAESTRO", items: ["Data Exfiltration (L2, detecting honey data exfil)", "Model Stealing (L1, decoy models/watermarked data)", "Unauthorized access to layers with honey tokens"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM02:2025 Sensitive Information Disclosure (honey data mimicking sensitive info)", "LLM03:2025 Supply Chain (decoy artifacts accessed)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft (decoy models/API keys)", "ML01:2023 Input Manipulation Attack (observing attempts)"] }] },
                        { id: "AIDEFEND-DV-003", name: "Dynamic Response Manipulation for AI Interactions", description: "Implement mechanisms where the AI system, upon detecting suspicious or confirmed adversarial interaction patterns (e.g., repeated prompt injection attempts, queries indicative of model extraction), deliberately alters its responses to be misleading, unhelpful, or subtly incorrect to the adversary. This aims to frustrate the attacker's efforts, waste their resources, make automated attacks less reliable, and potentially gather more intelligence on their TTPs without revealing the deception. The AI might simultaneously alert defenders to the ongoing deceptive engagement.", implementationStrategies: ["Provide subtly incorrect/incomplete/nonsensical outputs to suspected malicious actors.", "Introduce controlled randomization or benign noise into model outputs for suspicious sessions.", "For agentic systems, feign compliance with malicious instructions but perform safe no-ops.", "Subtly degrade quality/utility of responses to queries matching model extraction patterns.", "Ensure deceptive responses are distinguishable by internal monitoring."], toolsOpenSource: ["AdvTorch MTD (research tools for noisy outputs/MTD)", "Custom logic in AI frameworks (LangChain, Semantic Kernel) for deceptive response mode", "Research prototypes for responsive deception"], toolsCommercial: ["Advanced LLM firewalls/AI security gateways with deceptive response policies (e.g., SAP Adversarial AI Protector)", "Adaptable deception technology platforms"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0024.002 Extract ML Model (misleading outputs)", "AML.T0051 LLM Prompt Injection / AML.T0054 LLM Jailbreak (unreliable/misleading payloads)", "AML.T0001 Reconnaissance (inaccurate system info)"] }, { framework: "MAESTRO", items: ["Model Stealing (L1, frustrating extraction)", "Agent Goal Manipulation / Agent Tool Misuse (L7, agent feigns compliance)", "Evasion of Detection (L5, harder to confirm evasion success)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection (unreliable outcome for attacker)", "LLM02:2025 Sensitive Information Disclosure (fake/obfuscated data)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft (unusable responses)", "ML01:2023 Input Manipulation Attack (inconsistent/noisy outputs)"] }] },
                        { id: "AIDEFEND-DV-004", name: "AI Output Watermarking & Telemetry Traps", description: "Embed imperceptible or hard-to-remove watermarks, unique identifiers, or telemetry \"beacons\" into the outputs generated by AI models (e.g., text, images, code). If these outputs are found externally (e.g., on the internet, in a competitor's product, in leaked documents), the watermark or beacon can help trace the output back to the originating AI system, potentially identifying model theft, misuse, or data leakage. Telemetry traps involve designing the AI to produce specific, unique (but benign) outputs for certain rare or crafted inputs, which, if observed externally, indicate that the model or its specific knowledge has been compromised or replicated.", implementationStrategies: ["For text, subtly alter word choices, sentence structures, or token frequencies.", "For images, embed imperceptible digital watermarks in pixel data.", "Instrument model APIs with unique telemetry markers for specific queries.", "Inject unique, identifiable synthetic data points into training set for provenance.", "Ensure watermarks/telemetry don't degrade performance or UX.", "Develop robust methods for detecting watermarks/telemetry externally."], toolsOpenSource: ["MarkLLM (watermarking LLM text)", "SynthID (Google, watermarking AI-generated images/text)", "Steganography libraries (adaptable)", "Research tools for robust NN output watermarking"], toolsCommercial: ["Verance Watermarking (AI content)", "Sensity AI Guard (deepfake detection/watermarking)", "Commercial digital watermarking solutions", "Content authenticity platforms"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0024.002 Extract ML Model / AML.T0048.004 External Harms: ML Intellectual Property Theft", "AML.T0057 LLM Data Leakage (tracing watermarked outputs)", "AML.T0048.002 External Harms: Societal Harm (attributing deepfakes/misinfo)"] }, { framework: "MAESTRO", items: ["Model Stealing (L1, identifying stolen outputs)", "Data Exfiltration (L2, exfiltrated watermarked data)", "Misinformation Generation (L1/L7, attribution/detection)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM02:2025 Sensitive Information Disclosure (leaked watermarked output)", "LLM09:2025 Misinformation (identifying AI-generated content)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft (traceable models/outputs)", "ML09:2023 Output Integrity Attack (watermark destruction reveals tampering)"] }] },
                        { id: "AIDEFEND-DV-005", name: "Decoy Agent Behaviors & Canary Tasks", description: "For autonomous AI agents, design and implement decoy or \"canary\" functionalities, goals, or sub-agents that appear valuable or sensitive but are actually monitored traps. If an attacker successfully manipulates an agent (e.g., via prompt injection or memory poisoning) and directs it towards these decoy tasks or to exhibit certain predefined suspicious behaviors, it triggers an alert, revealing the compromise attempt and potentially the attacker's intentions, without risking real assets.", implementationStrategies: ["Equip agent with shadow/canary goal/tool leading to monitored environment.", "Create dummy 'watcher' agent personas.", "Issue benign 'test prompts' or 'internal audit' instructions to agent.", "Design agents to report attempts to perform actions outside capabilities/ethics.", "Ensure decoy behaviors are well-instrumented and isolated."], toolsOpenSource: ["Agentic Radar (CLI scanner, adaptable for decoy tests)", "Custom logic in agentic frameworks (AutoGen, CrewAI, Langroid) for canary tasks", "Integration with logging/alerting systems (ELK, Prometheus)"], toolsCommercial: ["Emerging AI safety/agent monitoring platforms (e.g., Foretrace for AI - conceptual)", "Adaptable deception technology platforms"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0010 Privilege Escalation / AML.T0009.002 LLM Plugin Compromise (decoy tool triggers alert)", "AML.T0051 LLM Prompt Injection / AML.T0054 LLM Jailbreak (injection leads to canary task)", "AML.T0018.001 Backdoor ML Model: Poison LLM Memory (poisoned memory leads to decoy goal)"] }, { framework: "MAESTRO", items: ["Agent Goal Manipulation / Agent Tool Misuse (L7, luring to decoy tools/goals)", "Agent Identity Attack (directing to canary tasks)", "Orchestration Attacks (L3, interaction with decoy components)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection (detecting successful diversion to decoy)", "LLM06:2025 Excessive Agency (agent attempts to use decoy tool)"] }, { framework: "OWASP ML Top 10 2023", items: ["Relevant if agent behavior compromised due to model issues, interaction with decoys could reveal this."] }] }
                    ]
                },
                {
                    name: "Evict",
                    purpose: "The \"Evict\" tactic focuses on the active removal of an adversary's presence from a compromised AI system and the elimination of any malicious artifacts they may have introduced. Once an intrusion or malicious activity has been detected and contained, eviction procedures are executed to ensure the attacker is thoroughly expelled, their access mechanisms are dismantled, and any lingering malicious code, data, or configurations are purged.",
                    techniques: [
                        { id: "AIDEFEND-E-001", name: "Credential Revocation & Rotation for AI Systems", description: "Immediately revoke, invalidate, or rotate any credentials (e.g., API keys, access tokens, user account passwords, service account credentials, certificates) that are known or suspected to have been compromised or used by an adversary to gain unauthorized access to or interact maliciously with AI systems, models, data, or MLOps pipelines. This action aims to cut off the attacker's current access and prevent them from reusing stolen credentials.", implementationStrategies: ["Automate credential invalidation upon alert.", "Implement rapid rotation process for all secrets.", "Force password resets for compromised user accounts.", "Revoke/reissue compromised AI agent credentials.", "Remove unauthorized accounts/API keys created by attacker.", "Ensure prompt propagation of revocation."], toolsOpenSource: ["Cloud provider CLIs/SDKs for IAM automation", "HashiCorp Vault", "Keycloak or other IAM solutions with APIs"], toolsCommercial: ["PAM solutions (CyberArk, Delinea, BeyondTrust)", "IDaaS platforms (Okta, Ping Identity)", "SIEM/SOAR platforms for automated revocation"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0012 Valid Accounts", "AML.T0011 Initial Access (stolen creds)", "AML.T0017 Persistence (credential-based)"] }, { framework: "MAESTRO", items: ["Agent Identity Attack / Compromised Agent Registry (L7)", "Unauthorized access via stolen credentials"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM02:2025 Sensitive Information Disclosure (if creds stolen)", "LLM06:2025 Excessive Agency (if agent creds compromised)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft (if via compromised creds)"] }] },
                        { id: "AIDEFEND-E-002", name: "AI Process & Session Eviction", description: "Terminate any running AI model instances, agent processes, user sessions, or containerized workloads that are confirmed to be malicious, compromised, or actively involved in an attack. This immediate action halts the adversary's ongoing activities within the AI system and removes their active foothold.", implementationStrategies: ["Identify and kill malicious AI model/inference server processes.", "Forcefully terminate/reset hijacked AI agent sessions/instances.", "Quarantine/shut down compromised pods/containers in Kubernetes.", "Invalidate active user sessions associated with malicious activity.", "Log eviction of processes/sessions for forensics."], toolsOpenSource: ["OS process management (kill, pkill, taskkill)", "Container orchestration CLIs (kubectl delete pod --force)", "HIPS (OSSEC, Wazuh)", "Custom scripts for session clearing (Redis FLUSHDB)"], toolsCommercial: ["EDR solutions (CrowdStrike, SentinelOne, Carbon Black)", "Cloud provider management consoles/APIs for instance termination", "APM tools with session management"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0009 Execution (stops active malicious code)", "AML.T0051 LLM Prompt Injection / AML.T0054 LLM Jailbreak (terminates manipulated session)", "AML.T0017 Persistence (if via running process/session)"] }, { framework: "MAESTRO", items: ["Agent Tool Misuse / Agent Goal Manipulation (L7, terminating rogue agent)", "Resource Hijacking (L4, killing resource-abusing processes)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection (ending manipulated session)", "LLM06:2025 Excessive Agency (terminating overreaching agent)"] }, { framework: "OWASP ML Top 10 2023", items: ["Any attack resulting in a malicious running process (e.g., ML06:2023 AI Supply Chain Attacks)"] }] },
                        { id: "AIDEFEND-E-003", name: "AI Backdoor & Malicious Artifact Removal", description: "Systematically scan for, identify, and remove any malicious artifacts introduced by an attacker into the AI system. This includes backdoors embedded in model parameters or code, poisoned data points in training sets or vector databases, hidden malicious prompts in agent memory stores, or any other unauthorized modifications designed to grant persistent access or manipulate AI behavior.", implementationStrategies: ["Analyze models for backdoor patterns (neural pruning, fine-tuning on clean data).", "Replace tainted model with clean, verified backup.", "Identify and purge malicious or poisoned data entries from training datasets, fine-tuning datasets, and vector databases or other knowledge retrieval stores used for RAG. This may involve data slicing based on provenance to pinpoint suspicious entries, or using specialized tools to detect embeddings generated from known malicious content, followed by retraining or fine-tuning models on the cleansed datasets and associated clean model checkpoints.", "Scan and purge persistent malicious prompts/state from agent memory.", "Remove malicious scripts, tools, or modified config files.", "Verify removal by re-scanning and testing."], toolsOpenSource: ["Adversarial Robustness Toolbox (ART)", "Neural Cleanse (research code)", "Data validation/cleaning libraries (Great Expectations, Pandas)", "File integrity monitoring (AIDE, Tripwire open source)"], toolsCommercial: ["HiddenLayer (model scanning for backdoors)", "Protect AI Platform (ModelScan)", "TrojanAI (backdoor scanning service)", "Data quality platforms with anomaly detection"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0018 Backdoor ML Model / AML.T0019 Poison ML Model", "AML.T0020 Poison Training Data", "AML.T0018.001 Backdoor ML Model: Poison LLM Memory"] }, { framework: "MAESTRO", items: ["Backdoor Attacks (L1/L3)", "Data Poisoning (L2)", "Compromised RAG Pipelines (L2, cleaning vector DBs)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM04:2025 Data and Model Poisoning", "LLM01:2025 Prompt Injection (persistent malicious prompts)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML10:2023 Model Poisoning", "ML02:2023 Data Poisoning Attack"] }] },
                        { id: "AIDEFEND-E-004", name: "System Patching & Hardening Post-AI Attack", description: "After an attack vector has been identified and the adversary evicted, rapidly apply necessary security patches to vulnerable software components (e.g., ML libraries, operating systems, web servers, agent frameworks) and harden system configurations that were exploited or found to be weak. This step aims to close the specific vulnerabilities used by the attacker and strengthen overall security posture to prevent reinfection or similar future attacks.", implementationStrategies: ["Apply security patches for exploited CVEs in AI stack.", "Review and harden abused/insecure system configurations.", "Strengthen IAM policies, input/output validation, network segmentation.", "Disable unnecessary services or LLM plugin functionalities.", "Add new detection rules/IOCs based on attack specifics.", "Verify patches and hardening measures."], toolsOpenSource: ["Package managers (apt, yum, pip, conda)", "Configuration management tools (Ansible, Chef, Puppet)", "Vulnerability scanners (OpenVAS, Trivy)", "Static analysis tools (Bandit)"], toolsCommercial: ["Automated patch management solutions (Automox, ManageEngine)", "CSPM tools", "Vulnerability management platforms (Tenable, Rapid7)", "SCA tools (Snyk, Mend)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["Any technique exploiting software vulnerability or misconfiguration (e.g., AML.T0009.001 ML Code Injection, AML.T0011 Initial Access)", "AML.T0021 Erode ML Model Integrity (if due to vulnerability exploitation)"] }, { framework: "MAESTRO", items: ["Re-exploitation of vulnerabilities in any layer (L1-L4)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM03:2025 Supply Chain (patching vulnerable component)", "LLM05:2025 Improper Output Handling (patching downstream component)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML06:2023 AI Supply Chain Attacks (if vulnerable library was entry point)"] }] },
                        { id: "AIDEFEND-E-005", name: "Secure Communication & Session Re-establishment for AI", description: "After an incident where communication channels or user/agent sessions related to AI systems might have been compromised, hijacked, or exposed to malicious influence, take steps to securely re-establish these communications. This involves expiring all potentially tainted active sessions, forcing re-authentication for users and agents, clearing any manipulated conversational states, and ensuring that interactions resume over verified, secure channels. The goal is to prevent attackers from leveraging residual compromised sessions or states.", implementationStrategies: ["Expire all active user sessions and API tokens/session cookies.", "Invalidate/regenerate session tokens for AI agents.", "Clear persistent conversational histories/cached states for affected agents.", "Ensure re-established sessions use strong authentication (MFA) and encryption (HTTPS/TLS).", "Communicate session reset to legitimate users as a security measure.", "Monitor newly established sessions for re-compromise."], toolsOpenSource: ["Application server admin interfaces for session expiration", "Custom scripts with JWT libraries or flushing session stores (Redis, Memcached)", "IAM systems (Keycloak) with session termination APIs"], toolsCommercial: ["IDaaS platforms (Okta, Auth0) for session termination", "API Gateways with advanced session management", "Customer communication platforms (Twilio, SendGrid)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0012 Valid Accounts / AML.T0011 Initial Access (evicting hijacked sessions)", "AML.T0017 Persistence (if relying on active session/manipulated state)"] }, { framework: "MAESTRO", items: ["Agent Identity Attack (L7, forcing re-auth and clearing state)", "Session Hijacking affecting any AI layer"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection (clearing manipulated states)", "LLM02:2025 Sensitive Information Disclosure (stopping leaks from ongoing sessions)"] }, { framework: "OWASP ML Top 10 2023", items: ["Any attack involving session hijacking or manipulation of ongoing ML API interactions."] }] }
                    ]
                },
                {
                    name: "Restore",
                    purpose: "The \"Restore\" tactic focuses on recovering normal AI system operations and data integrity following an attack and subsequent eviction of the adversary. This phase involves safely bringing AI models and applications back online, restoring any corrupted or lost data from trusted backups, and, crucially, learning from the incident to reinforce defenses and improve future resilience.",
                    techniques: [
                        { id: "AIDEFEND-R-001", name: "Secure AI Model Restoration & Retraining", description: "After an incident that may have compromised AI model integrity (e.g., through data poisoning, model poisoning, backdoor insertion, or unauthorized modification), securely restore affected models to a known-good state. This may involve deploying models from trusted, verified backups taken prior to the incident, or, if necessary, retraining or fine-tuning models on clean, validated datasets to eliminate any malicious influence or corruption.", implementationStrategies: ["Maintain versioned, checksummed backups of production AI models.", "Replace compromised model with latest known-good backup, verifying integrity.", "If training data poisoned, remove poisoned data and retrain/fine-tune the affected model(s) using the cleansed dataset, ensuring the retraining process itself adheres to Secure & Resilient Training Process Hardening (AIDEFEND-H-007) principles to prevent re-introduction of vulnerabilities.", "If model backdoored, revert to clean version or retrain from scratch.", "Thoroughly validate model performance, behavior, and security post-restoration.", "Document restoration process."], toolsOpenSource: ["MLOps platforms (MLflow, Kubeflow Pipelines, DVC)", "Delta Lake (for time travel on datasets)", "Standard backup/recovery tools for model artifacts"], toolsCommercial: ["Enterprise MLOps platforms (Databricks, SageMaker, Vertex AI, Azure ML)", "Palo Alto Networks ModelGuard (conceptual)", "Data backup/recovery solutions"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0018 Backdoor ML Model / AML.T0019 Poison ML Model", "AML.T0020 Poison Training Data", "AML.T0021 Erode ML Model Integrity"] }, { framework: "MAESTRO", items: ["Backdoor Attacks (L1)", "Data Poisoning (L2, retraining)", "Model Skewing (L2, restoring/retraining)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM04:2025 Data and Model Poisoning"] }, { framework: "OWASP ML Top 10 2023", items: ["ML10:2023 Model Poisoning", "ML02:2023 Data Poisoning Attack"] }] },
                        { id: "AIDEFEND-R-002", name: "Data Integrity Recovery for AI Systems", description: "Restore the integrity of any datasets used by or generated by AI systems that were corrupted, tampered with, or maliciously altered during a security incident. This includes training data, validation data, vector databases for RAG, embeddings stores, configuration data, or logs of AI outputs. Recovery typically involves reverting to known-good backups, using data validation tools to identify and correct inconsistencies, or, in some cases, reconstructing data if backups are insufficient or also compromised.", implementationStrategies: ["Identify all affected data stores.", "Restore data from most recent, verified backups.", "If backups unavailable, attempt reconstruction/repair (data validation tools, log analysis).", "Re-validate integrity and consistency of recovered data.", "Update data ingestion/processing pipelines to prevent recurrence."], toolsOpenSource: ["Database backup/restore utilities (pg_dump, mysqldump)", "Cloud provider snapshot/backup services (S3 versioning, Azure Blob snapshots)", "Great Expectations", "Filesystem backup tools (rsync, Bacula)", "Vector DB export/import utilities"], toolsCommercial: ["Enterprise backup/recovery solutions (Rubrik, Cohesity, Veeam)", "Data quality/integration platforms (Informatica, Talend)", "Cloud provider managed backup services (AWS Backup, Azure Backup)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0020 Poison Training Data (restoring clean dataset)", "AML.T0021 Erode ML Model Integrity (restoring corrupted data stores)"] }, { framework: "MAESTRO", items: ["Data Poisoning / Data Tampering (L2)", "Compromised RAG Pipelines (L2, restoring vector DBs)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM04:2025 Data and Model Poisoning (restoring dataset integrity)", "LLM08:2025 Vector and Embedding Weaknesses (if vector DBs corrupted)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML02:2023 Data Poisoning Attack (restoring clean training data)"] }] },
                        { id: "AIDEFEND-R-003", name: "Post-Incident AI System Reinforcement & Testing", description: "Following recovery from a security incident, conduct a thorough review of the attack, the system's response, and the effectiveness of existing defenses. Based on these lessons learned, reinforce security controls, update threat models, and perform rigorous testing (including penetration testing or red teaming specifically targeting the previous attack vector and similar ones) to confirm that vulnerabilities have been addressed and the AI system is more resilient against future, similar attacks.", implementationStrategies: ["Conduct detailed post-incident review (PIR) / root cause analysis (RCA).", "Update AI threat models (AIDEFEND-M-004).", "Implement/enhance defensive techniques based on PIR findings.", "Perform targeted security testing (pen testing, AI red teaming).", "Validate effectiveness of patches and hardening measures.", "Update incident response plans and playbooks."], toolsOpenSource: ["MITRE ATLAS Navigator", "OWASP AI Security & Privacy Guide, OWASP LLM/ML Top 10s", "AI red teaming frameworks (Counterfit, Garak, vigil-llm)", "Vulnerability scanners"], toolsCommercial: ["AI red teaming services", "Breach and Attack Simulation (BAS) platforms", "Booz Allen's Atlas Notebook", "Immersive Labs", "Commercial penetration testing services"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["Recurrence of same/similar attack techniques by closing gaps. Improves resilience against all ATLAS tactics."] }, { framework: "MAESTRO", items: ["Future attacks exploiting similar vulnerabilities across any MAESTRO layer."] }, { framework: "OWASP LLM Top 10 2025", items: ["Helps prevent re-exploitation of any LLM Top 10 vulnerabilities."] }, { framework: "OWASP ML Top 10 2023", items: ["Helps prevent re-exploitation of any ML Top 10 vulnerabilities."] }] },
                        { id: "AIDEFEND-R-004", name: "Stakeholder Notification & AI Incident Knowledge Sharing", description: "After an AI security incident has been contained, remediated, and systems restored, inform relevant internal and external stakeholders (e.g., developers, users, customers, partners, regulatory bodies if required) about the incident (to the appropriate level of detail), the resolution steps taken, and measures implemented to prevent recurrence. Where appropriate and feasible, share anonymized or generalized learnings, IoCs, or novel attack vector information with the broader AI security community (e.g., ISACs, MITRE ATLAS, OWASP) to help improve collective defense.", implementationStrategies: ["Develop communication plan for AI security incidents.", "Provide factual post-mortem report to internal teams; summary for external stakeholders.", "Follow legal/compliance requirements for notification (data breach, regulations).", "Consider sharing non-sensitive technical details with trusted communities.", "Update internal documentation (model cards, architecture diagrams, risk assessments).", "Use incident as case study for internal training/awareness."], toolsOpenSource: ["Incident response plan templates (SANS, NIST)", "Security community mailing lists/forums (FIRST.org, OWASP)", "MISP (Malware Information Sharing Platform)"], toolsCommercial: ["GRC platforms for incident reporting/notifications", "Threat intelligence sharing platforms", "Secure communication platforms", "Public relations/crisis communication services", "Bridgecrew, RiskRecon (compliance reporting)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["Indirectly defends against future attacks by community knowledge sharing. Helps manage 'Impact' phase (reputational, legal)."] }, { framework: "MAESTRO", items: ["Improves ecosystem resilience if learnings shared. Addresses L6: Security & Compliance."] }, { framework: "OWASP LLM Top 10 2025", items: ["Facilitates better community understanding and defense against LLM Top 10 risks."] }, { framework: "OWASP ML Top 10 2023", items: ["Improves collective defense against ML-specific risks."] }] }
                    ]
                }
            ]
        };

        // --- DOM Elements ---
        const mainContentEl = document.getElementById('main-content');
        const searchBarEl = document.getElementById('search-bar');
        const searchClearBtnEl = document.getElementById('searchClearBtn');
        const modalEl = document.getElementById('infoModal');
        const modalBackdropEl = document.getElementById('modalBackdrop');
        const modalBodyEl = document.getElementById('modalBody');
        const modalCloseBtn = document.getElementById('modalClose');
        const themeToggleBtn = document.getElementById('themeToggleBtn');
        const aboutBtn = document.getElementById('aboutBtn');
        const htmlEl = document.documentElement;

        // --- SVG Icons ---
        const sunIcon = `
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor">
              <path d="M12 2.25a.75.75 0 01.75.75v2.25a.75.75 0 01-1.5 0V3a.75.75 0 01.75-.75zM7.5 12a4.5 4.5 0 119 0 4.5 4.5 0 01-9 0zM18.894 6.166a.75.75 0 00-1.06-1.06l-1.591 1.59a.75.75 0 101.06 1.061l1.591-1.59zM21.75 12a.75.75 0 01-.75.75h-2.25a.75.75 0 010-1.5H21a.75.75 0 01.75.75zM17.834 18.894a.75.75 0 001.06-1.06l-1.59-1.591a.75.75 0 10-1.061 1.06l1.59 1.591zM12 18a.75.75 0 01.75.75V21a.75.75 0 01-1.5 0v-2.25A.75.75 0 0112 18zM7.758 17.303a.75.75 0 00-1.061-1.06l-1.591 1.59a.75.75 0 001.06 1.061l1.591-1.59zM6 12a.75.75 0 01-.75.75H3a.75.75 0 010-1.5h2.25A.75.75 0 016 12zM6.166 7.758a.75.75 0 001.06 1.06l1.591-1.59a.75.75 0 00-1.06-1.061L6.166 7.758z" />
            </svg>`;
        const moonIcon = `
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor">
              <path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 01.162.819A8.97 8.97 0 009 6a9 9 0 009 9 8.97 8.97 0 003.463-.69.75.75 0 01.981.98 10.503 10.503 0 01-9.694 6.46c-5.799 0-10.5-4.701-10.5-10.5 0-3.51 1.713-6.636 4.362-8.492a.75.75 0 01.819.162z" clip-rule="evenodd" />
            </svg>`;
        const infoIcon = `
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor">
                <path fill-rule="evenodd" d="M18 10a8 8 0 1 1-16 0 8 8 0 0 1 16 0ZM9 9a.75.75 0 0 0 0 1.5h.253a.25.25 0 0 1 .244.304l-.459 2.066A1.75 1.75 0 0 0 10.747 15H11a.75.75 0 0 0 0-1.5h-.253a.25.25 0 0 1-.244-.304l.459-2.066A1.75 1.75 0 0 0 9.253 9H9Z" clip-rule="evenodd" />
            </svg>`;


        // --- State ---
        let currentSearchTerm = "";

        // --- Theme Handling ---
        function applyTheme(theme) {
            if (theme === 'dark') {
                htmlEl.classList.add('dark');
                themeToggleBtn.innerHTML = sunIcon;
            } else {
                htmlEl.classList.remove('dark');
                themeToggleBtn.innerHTML = moonIcon;
            }
        }

        function toggleTheme() {
            const currentTheme = htmlEl.classList.contains('dark') ? 'dark' : 'light';
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            localStorage.setItem('aidefendTheme', newTheme);
            applyTheme(newTheme);
        }

        const savedTheme = localStorage.getItem('aidefendTheme') || 'dark'; 
        applyTheme(savedTheme);
        aboutBtn.innerHTML = infoIcon + "About"; 


        // --- Render Functions ---
        function renderMainGrid(searchTerm = "") {
            mainContentEl.innerHTML = ''; 
            const gridContainer = document.createElement('div');
            gridContainer.className = 'tactic-column-grid';

            const allTechniquesFlat = aidefendData.tactics.reduce((acc, tactic) => {
                tactic.techniques.forEach(tech => acc.push({...tech, tacticName: tactic.name }));
                return acc;
            }, []);
            
            const techniquesMatchingSearch = searchTerm ? allTechniquesFlat.filter(tech => {
                const term = searchTerm.toLowerCase();
                let match = tech.name.toLowerCase().includes(term) ||
                               tech.id.toLowerCase().includes(term) ||
                               (tech.description && tech.description.toLowerCase().includes(term));
                if (!match && tech.defendsAgainst) {
                    for (const da of tech.defendsAgainst) {
                        if (da.items && da.items.some(item => item.toLowerCase().includes(term))) {
                            match = true;
                            break;
                        }
                    }
                }
                return match;
            }) : [];


            if (searchTerm && techniquesMatchingSearch.length === 0) {
                mainContentEl.innerHTML = `<p class="text-center opacity-80 py-10">No techniques found matching "${searchTerm}".</p>`;
                return; 
            }

            aidefendData.tactics.forEach(tactic => {
                let techniquesToShowInColumn;
                if (searchTerm) {
                    techniquesToShowInColumn = tactic.techniques.filter(t => 
                        techniquesMatchingSearch.some(matchedTech => matchedTech.id === t.id)
                    );
                    if (techniquesToShowInColumn.length === 0) {
                        return; 
                    }
                } else {
                    techniquesToShowInColumn = tactic.techniques; 
                }

                const column = document.createElement('div');
                column.className = 'tactic-column elevation-2'; 
                
                const tacticHeader = document.createElement('h3');
                tacticHeader.className = 'tactic-column-header';
                tacticHeader.textContent = tactic.name;
                tacticHeader.onclick = () => showTacticModal(tactic);
                column.appendChild(tacticHeader);
                
                const ul = document.createElement('ul');
                
                if (techniquesToShowInColumn.length > 0) {
                    techniquesToShowInColumn.forEach(tech => {
                        const li = document.createElement('li'); 
                        const a = document.createElement('a');
                        a.href = '#';
                        a.className = 'technique-item elevation-2'; 
                        a.innerHTML = `<span class="technique-id">${tech.id}</span> <span class="technique-name">${tech.name}</span>`;
                        if (searchTerm && techniquesMatchingSearch.some(matchedTech => matchedTech.id === tech.id)) {
                            a.classList.add('highlight');
                        }
                        a.onclick = (e) => {
                            e.preventDefault();
                            showTechniqueModal(tech, tactic.name);
                        };
                        li.appendChild(a);
                        ul.appendChild(li);
                    });
                } else if (!searchTerm) { 
                    const p = document.createElement('p');
                    p.className = 'text-xs opacity-60 italic px-2';
                    p.textContent = 'No techniques defined yet.';
                    ul.appendChild(p);
                }
                column.appendChild(ul);
                gridContainer.appendChild(column);
            });
            mainContentEl.appendChild(gridContainer);
        }

        function showIntroductionModal() {
            const intro = aidefendData.introduction;
            let contentHtml = `<h2 class="modal-main-title">${intro.mainTitle}</h2>`; 

            intro.sections.forEach(section => {
                contentHtml += `<h3>${section.title}</h3>`; 
                if (section.paragraphs) {
                    section.paragraphs.forEach(p => contentHtml += `<p>${p}</p>`);
                }
                if (section.listItems) {
                    contentHtml += `<ul>`;
                    section.listItems.forEach(item => contentHtml += `<li>${item}</li>`);
                    contentHtml += `</ul>`;
                }
                if (section.concludingParagraphs) { 
                    section.concludingParagraphs.forEach(p => contentHtml += `<p>${p}</p>`);
                }
            });

            modalBodyEl.innerHTML = contentHtml;
            modalEl.classList.add('active');
            document.body.classList.add('modal-open');
        }


        function showTacticModal(tactic) {
            modalBodyEl.innerHTML = `
                <h2>${tactic.name}</h2>
                <p class="mb-4 leading-relaxed text-sm">${tactic.purpose || 'No purpose description available.'}</p>
            `;
            modalEl.classList.add('active');
            document.body.classList.add('modal-open');
        }


        function showTechniqueModal(technique, tacticName) {
            modalBodyEl.innerHTML = `
                <p class="text-sm opacity-80 mb-1">Tactic: ${tacticName}</p>
                <h2>${technique.id}: ${technique.name}</h2>
                <p class="mb-4 leading-relaxed text-sm">${technique.description || 'No description available.'}</p>
                
                ${renderDetailSectionListForModal('Implementation Strategies', technique.implementationStrategies)}
                ${renderDetailSectionListForModal('Potential Tools - Open Source', technique.toolsOpenSource)}
                ${renderDetailSectionListForModal('Potential Tools - Commercial', technique.toolsCommercial)}
                
                <div class="mt-4">
                    <h4 class="font-semibold text-md mb-2">Defends Against:</h4>
                    ${technique.defendsAgainst && technique.defendsAgainst.length > 0 ? 
                        technique.defendsAgainst.map(da => `
                            <div class="mb-2">
                                <p class="defends-against-framework text-sm">${da.framework}:</p>
                                <ul class="list-disc list-inside ml-4 text-xs opacity-90 space-y-0.5">
                                    ${da.items.map(item => `<li class="defends-against-item">${item}</li>`).join('')}
                                </ul>
                            </div>
                        `).join('') : '<p class="text-xs opacity-60 italic">No specific defenses listed.</p>'}
                </div>
            `;
            modalEl.classList.add('active');
            document.body.classList.add('modal-open');
        }

        function hideModal() { 
            modalEl.classList.remove('active');
            document.body.classList.remove('modal-open');
            modalBodyEl.innerHTML = ''; 
        }

        function renderDetailSectionListForModal(title, items) {
            if (!items || items.length === 0) return '';
            return `
                <div class="mt-3">
                    <h4 class="font-semibold text-sm mb-1">${title}:</h4>
                    <ul class="list-disc list-inside ml-4 text-xs opacity-90 space-y-1">
                        ${items.map(item => `<li>${item}</li>`).join('')}
                    </ul>
                </div>
            `;
        }
        
        // --- Event Listeners ---
        themeToggleBtn.addEventListener('click', toggleTheme);
        aboutBtn.addEventListener('click', showIntroductionModal);

        searchBarEl.addEventListener('input', (e) => {
            currentSearchTerm = e.target.value.trim();
            if (currentSearchTerm) {
                searchClearBtnEl.style.display = 'block';
            } else {
                searchClearBtnEl.style.display = 'none';
            }
            renderMainGrid(currentSearchTerm);
        });

        searchClearBtnEl.addEventListener('click', () => {
            searchBarEl.value = '';
            currentSearchTerm = '';
            searchClearBtnEl.style.display = 'none';
            renderMainGrid();
        });


        modalCloseBtn.addEventListener('click', hideModal);
        modalBackdropEl.addEventListener('click', hideModal); 

        document.addEventListener('keydown', (event) => {
            if (event.key === 'Escape' && modalEl.classList.contains('active')) {
                hideModal();
            }
        });

        // --- Initial Load ---
        renderMainGrid(); 

    </script>

</body>
</html>
