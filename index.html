<!DOCTYPE html>
<html lang="en" class="dark"> <!-- Default to dark theme -->
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AIDEFEND Framework Viewer</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        :root {
            /* Light Theme (Material Inspired) */
            --bg-color: #f5f5f5; /* Material Background */
            --text-color: #212121; /* Material Primary Text */
            --text-secondary-color: #757575; /* Material Secondary Text */
            --header-bg: #ffffff; /* Surface */
            --header-border: #e0e0e0; /* Divider */
            --column-bg: #ffffff; /* Surface */
            --column-border: #e0e0e0; /* Divider */
            --column-header-text: #424242; /* Slightly darker for headers */
            --column-header-hover-text: #1976d2; /* Material Blue 700 */
            --technique-item-bg: #ffffff; /* Surface */
            --technique-item-border: #eeeeee; /* Lighter border for cards */
            --technique-item-text: #1976d2; /* Material Blue 700 */
            --technique-item-hover-bg: #e3f2fd; /* Material Blue 50 */
            --technique-item-hover-text: #0d47a1; /* Material Blue 900 */
            --technique-item-highlight-bg: #bbdefb; /* Material Blue 100 */
            --technique-item-highlight-text: #0d47a1; /* Material Blue 900 */
            --technique-id-text: #757575; /* Material Secondary Text */
            --modal-content-bg: #ffffff; /* Surface */
            --modal-close-btn-text: #757575;
            --modal-close-btn-hover-text: #212121;
            --search-bg: #ffffff; /* Surface */
            --search-border: #bdbdbd; /* Lighter border */
            --search-focus-ring: #1976d2; /* Material Blue 700 */
            --scrollbar-track: #eeeeee;
            --scrollbar-thumb: #bdbdbd;
            --scrollbar-thumb-hover: #9e9e9e;
            --icon-fill: #616161; /* Darker icon fill */
            --icon-hover-fill: #1976d2; /* Material Blue 700 */
            --link-color: #1976d2; /* Material Blue 700 */
            --link-hover-color: #0d47a1; /* Material Blue 900 */
            --shadow-umbra: rgba(0, 0, 0, 0.2);
            --shadow-penumbra: rgba(0, 0, 0, 0.14);
            --shadow-ambient: rgba(0, 0, 0, 0.12);
            --footer-bg: #eeeeee; /* Light footer background */
            --footer-text: #757575; /* Secondary text for footer */
            --footer-border: #e0e0e0;
        }

        html.dark {
            /* Dark Theme (Material Inspired) */
            --bg-color: #121212; /* Material Dark Background */
            --text-color: #e0e0e0; /* Material Dark Primary Text */
            --text-secondary-color: #bdbdbd; /* Material Dark Secondary Text */
            --header-bg: #1e1e1e; /* Dark Surface */
            --header-border: #2c2c2c; /* Dark Divider */
            --column-bg: #1e1e1e; /* Dark Surface */
            --column-border: #2c2c2c; /* Dark Divider */
            --column-header-text: #f5f5f5; /* Lighter for headers */
            --column-header-hover-text: #90caf9; /* Material Blue 200 */
            --technique-item-bg: #2c2c2c; /* Darker Surface for items */
            --technique-item-border: #383838; /* Darker border for cards */
            --technique-item-text: #90caf9; /* Material Blue 200 */
            --technique-item-hover-bg: #323232;
            --technique-item-hover-text: #bbdefb; /* Material Blue 100 */
            --technique-item-highlight-bg: #1e3a5f; /* Darker blue highlight */
            --technique-item-highlight-text: #e3f2fd; /* Material Blue 50 */
            --technique-id-text: #bdbdbd; /* Material Dark Secondary Text */
            --modal-content-bg: #1e1e1e; /* Dark Surface */
            --modal-close-btn-text: #bdbdbd;
            --modal-close-btn-hover-text: #f5f5f5;
            --search-bg: #2c2c2c; /* Darker Surface */
            --search-border: #383838; /* Darker border */
            --search-focus-ring: #90caf9; /* Material Blue 200 */
            --scrollbar-track: #2c2c2c;
            --scrollbar-thumb: #424242;
            --scrollbar-thumb-hover: #616161;
            --icon-fill: #bdbdbd; /* Lighter icon fill */
            --icon-hover-fill: #90caf9; /* Material Blue 200 */
            --link-color: #90caf9; /* Material Blue 200 */
            --link-hover-color: #e3f2fd; /* Material Blue 50 */
            --shadow-umbra: rgba(0, 0, 0, 0.4); /* Stronger shadow for dark mode */
            --shadow-penumbra: rgba(0, 0, 0, 0.28);
            --shadow-ambient: rgba(0, 0, 0, 0.24);
            --footer-bg: #2c2c2c; /* Dark footer background */
            --footer-text: #9e9e9e; /* Lighter secondary text for dark footer */
            --footer-border: #383838;
        }

        html {
            height: 100%; /* Ensure html takes full height */
        }
        body {
            font-family: 'Inter', sans-serif;
            background-color: var(--bg-color);
            color: var(--text-color);
            transition: background-color 0.3s ease, color 0.3s ease;
            min-height: 100%; /* Ensure body takes full height */
            display: flex;
            flex-direction: column; /* Allow footer to stick to bottom */
        }
        #page-container { /* New wrapper for content + footer */
            flex: 1 0 auto; /* Allows content to grow and push footer down */
        }
        footer {
            flex-shrink: 0; /* Prevents footer from shrinking */
        }

        body.modal-open {
            overflow: hidden;
        }
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }
        ::-webkit-scrollbar-track {
            background: var(--scrollbar-track);
            border-radius: 10px;
        }
        ::-webkit-scrollbar-thumb {
            background: var(--scrollbar-thumb);
            border-radius: 10px;
        }
        ::-webkit-scrollbar-thumb:hover {
            background: var(--scrollbar-thumb-hover);
        }

        /* Material Design Elevation - dp2 */
        .elevation-2 {
            box-shadow: 0px 3px 1px -2px var(--shadow-umbra),
                        0px 2px 2px 0px var(--shadow-penumbra),
                        0px 1px 5px 0px var(--shadow-ambient);
        }
        /* Material Design Elevation - dp4 */
        .elevation-4 {
             box-shadow: 0px 2px 4px -1px var(--shadow-umbra),
                        0px 4px 5px 0px var(--shadow-penumbra),
                        0px 1px 10px 0px var(--shadow-ambient);
        }
        /* Material Design Elevation - dp8 (for modal) */
        .elevation-8 {
            box-shadow: 0px 5px 5px -3px var(--shadow-umbra),
                        0px 8px 10px 1px var(--shadow-penumbra),
                        0px 3px 14px 2px var(--shadow-ambient);
        }


        .header-main {
            background-color: var(--header-bg);
            transition: background-color 0.3s ease, border-color 0.3s ease;
        }
        .header-title {
            font-size: 2rem; 
            font-weight: 500; 
            color: var(--text-color);
        }
        .header-subtitle {
            font-size: 0.875rem; 
            color: var(--text-secondary-color); 
        }
        .search-input-wrapper {
            position: relative;
            display: flex;
            align-items: center;
        }
        .search-container input {
            background-color: var(--search-bg);
            border: 1px solid var(--search-border); 
            color: var(--text-color);
            border-radius: 0.5rem; 
            padding-right: 2.5rem; 
        }
         .search-container input::placeholder {
            color: var(--text-secondary-color);
        }
        .search-container input:focus {
            --tw-ring-color: var(--search-focus-ring);
            border-color: var(--search-focus-ring);
            box-shadow: 0 0 0 2px var(--search-focus-ring); 
        }
        .search-clear-button {
            position: absolute;
            right: 0.5rem;
            padding: 0.25rem;
            color: var(--text-secondary-color);
            cursor: pointer;
            display: none; 
        }
        .search-clear-button:hover {
            color: var(--text-color);
        }
        .search-clear-button svg {
            width: 1.25rem;
            height: 1.25rem;
        }


        .header-button {
            background-color: transparent;
            color: var(--icon-fill);
            border: none;
            padding: 0.625rem; 
            border-radius: 50%; 
            display: flex;
            align-items: center;
            justify-content: center; 
        }
        .header-button:hover {
            background-color: rgba(0,0,0,0.04); 
            color: var(--icon-hover-fill);
        }
        html.dark .header-button:hover {
            background-color: rgba(255,255,255,0.08); 
        }
        .header-button svg {
            width: 1.5rem; 
            height: 1.5rem; 
            fill: currentColor;
            margin-right: 0; 
        }
        .header-button.text-button {
             padding: 0.5rem 1rem; 
             border-radius: 0.25rem; 
             font-weight: 500; 
        }
        .header-button.text-button svg { 
            margin-right: 0.5rem;
            width: 1.25rem;
            height: 1.25rem;
        }


        .tactic-column-grid {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem; 
            padding: 1.5rem 1rem; 
            align-items: flex-start; 
            justify-content: center;
        }
        .tactic-column {
            flex: 0 0 300px; 
            background-color: var(--column-bg);
            border-radius: 0.75rem; 
            padding: 1.25rem; 
            min-height: 320px; 
            transition: background-color 0.3s ease, border-color 0.3s ease, box-shadow 0.3s ease;
        }
        .tactic-column-header { 
            font-size: 1.25rem; 
            font-weight: 500; 
            color: var(--column-header-text); 
            margin-bottom: 1rem; 
            padding-bottom: 0.75rem; 
            border-bottom: 1px solid var(--column-border); 
            transition: color 0.3s ease, border-color 0.3s ease;
            cursor: pointer;
        }
        .tactic-column-header:hover {
            color: var(--column-header-hover-text);
        }

        .technique-item {
            display: block;
            background-color: var(--technique-item-bg);
            padding: 0.75rem 1rem;
            font-size: 0.875rem; 
            color: var(--technique-item-text); 
            border-radius: 0.5rem; 
            cursor: pointer;
            transition: background-color 0.2s ease-in-out, color 0.2s ease-in-out, transform 0.2s ease, box-shadow 0.2s ease;
            line-height: 1.5; 
            margin-bottom: 0.75rem;
        }
        .technique-item:hover {
            background-color: var(--technique-item-hover-bg); 
            color: var(--technique-item-hover-text); 
            transform: translateY(-2px); 
        }
        .technique-item.highlight {
            background-color: var(--technique-item-highlight-bg); 
            color: var(--technique-item-highlight-text); 
            font-weight: 600;
        }
        .technique-id {
            font-weight: 500;
            color: var(--technique-id-text); 
            font-size: 0.75rem; 
            margin-right: 0.35rem;
            display: block; 
            margin-bottom: 0.125rem;
        }
        .technique-name { 
            font-weight: 500; 
        }


        /* Modal Styling */
        .modal-overlay {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: rgba(0, 0, 0, 0.5); 
            display: flex;
            align-items: center;
            justify-content: center;
            z-index: 1000;
            opacity: 0;
            visibility: hidden;
            transition: opacity 0.2s ease, visibility 0.2s ease; 
        }
        html.dark .modal-overlay {
            background-color: rgba(0, 0, 0, 0.7); 
        }
        .modal-overlay.active {
            opacity: 1;
            visibility: visible;
        }
        .modal-content {
            background-color: var(--modal-content-bg);
            color: var(--text-color);
            padding: 2rem; 
            border-radius: 0.5rem; 
            width: 90%;
            max-width: 800px; 
            max-height: 90vh; 
            overflow-y: auto; 
            position: relative; 
            transition: background-color 0.3s ease, color 0.3s ease, transform 0.2s ease; 
            transform: scale(0.95); 
        }
        .modal-overlay.active .modal-content {
            transform: scale(1); 
        }

        .modal-content h2.modal-main-title {
            font-size: 1.75rem; 
            font-weight: 500; 
            margin-bottom: 1.5rem; 
            color: var(--column-header-hover-text); 
        }
        .modal-content h2 { 
            font-size: 1.5rem; 
            font-weight: 500; 
            margin-bottom: 1rem;
            color: var(--column-header-hover-text); 
        }
        .modal-content h3 { 
            font-size: 1.25rem; 
            font-weight: 500; 
            margin-top: 1.5rem;
            margin-bottom: 0.75rem;
            color: var(--text-color);
        }
        .modal-content h4 { 
            font-size: 1rem; 
            font-weight: 500; 
            margin-top: 1rem;
            margin-bottom: 0.5rem;
            color: var(--text-color);
        }
        .modal-content p {
            font-size: 0.9375rem; 
            line-height: 1.7; 
            margin-bottom: 1rem;
        }
        .modal-content ul {
            list-style-type: disc;
            margin-left: 1.5rem;
            font-size: 0.9375rem; 
            line-height: 1.7;
            margin-bottom: 1rem;
        }
        .modal-content ul ul { 
             margin-top: 0.5rem;
             margin-bottom: 0.5rem;
        }
        .modal-content li {
            margin-bottom: 0.375rem; 
        }
        .modal-content a {
            color: var(--link-color);
            text-decoration: none; 
            font-weight: 500;
        }
        .modal-content a:hover {
            color: var(--link-hover-color);
            text-decoration: underline;
        }


        .modal-close-button {
            position: absolute;
            top: 1rem; 
            right: 1rem; 
            background: none;
            border: none;
            font-size: 1.75rem; 
            color: var(--modal-close-btn-text); 
            cursor: pointer;
            line-height: 1;
            transition: color 0.3s ease;
            padding: 0.25rem; 
        }
        .modal-close-button:hover {
            color: var(--modal-close-btn-hover-text); 
        }

        .defends-against-framework {
            font-weight: 500; 
            color: var(--technique-item-text);
            opacity: 0.9;
        }
        .defends-against-item {
            margin-left: 1rem;
            list-style-type: disc;
        }

        /* Footer Styling */
        .site-footer {
            background-color: var(--footer-bg);
            color: var(--footer-text);
            padding: 2rem 1rem; /* py-8 px-4 */
            text-align: center;
            font-size: 0.875rem; /* text-sm */
            border-top: 1px solid var(--footer-border);
            margin-top: auto; /* Pushes footer to bottom if content is short */
        }
        .site-footer p {
            margin-bottom: 0.5rem; /* space between paragraphs */
            line-height: 1.5;
        }


    </style>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
</head>
<body class="text-gray-800">
    <div id="page-container"> <!-- Wrapper for sticky footer -->
        <!-- Header -->
        <header class="header-main py-4 px-6 fixed top-0 left-0 right-0 z-50 elevation-4">
            <div class="container mx-auto">
                <div class="flex justify-between items-center mb-3">
                    <div class="text-left">
                        <h1 class="header-title">AIDEFEND™</h1>
                        <p class="header-subtitle">An AI-focused defensive countermeasures knowledge base</p>
                        <p class="text-xs opacity-60 mt-0.5">Version 1.0.0 (Alpha)</p>
                    </div>
                    <div class="flex items-center space-x-1"> 
                        <button id="aboutBtn" class="header-button text-button" title="About AIDEFEND">
                            <!-- Icon will be injected by JS --> About
                        </button>
                        <button id="themeToggleBtn" class="header-button" title="Toggle Theme">
                            <!-- SVG icon will be injected here by JavaScript -->
                        </button>
                    </div>
                </div>
                <div class="search-input-wrapper search-container">
                    <input type="text" id="search-bar" placeholder="Search AIDEFEND techniques..." class="w-full p-2.5 text-sm">
                    <button id="searchClearBtn" class="search-clear-button" title="Clear search">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="w-5 h-5">
                            <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16ZM8.28 7.22a.75.75 0 00-1.06 1.06L8.94 10l-1.72 1.72a.75.75 0 101.06 1.06L10 11.06l1.72 1.72a.75.75 0 101.06-1.06L11.06 10l1.72-1.72a.75.75 0 00-1.06-1.06L10 8.94 8.28 7.22Z" clip-rule="evenodd" />
                        </svg>
                    </button>
                </div>
            </div>
        </header>

        <!-- Main Content Area -->
        <div id="content-wrapper" class="pt-40 px-4 sm:px-6 lg:px-8 pb-8"> 
            <main id="main-content" class="container mx-auto">
                 <!-- Content will be populated by JavaScript -->
            </main>
        </div>
    </div> <!-- End of page-container -->

    <!-- Modal Structure (shared for Tactic and Technique) -->
    <div id="infoModal" class="modal-overlay">
        <div id="modalBackdrop" class="absolute inset-0"></div> 
        <div class="modal-content elevation-8">
            <button id="modalClose" class="modal-close-button">&times;</button>
            <div id="modalBody">
                <!-- Modal content will be injected here -->
            </div>
        </div>
    </div>
    
    <footer class="site-footer">
        <p>AIDEFEND, An AI-focused defensive countermeasures knowledge base.</p>
        <p>Information based on the MAESTRO framework, MITRE D3FEND, ATLAS, ATT&CK, and OWASP Top 10 LLM 2025/OWASP Top 10 ML Security 2023.</p>
        <p>© 2025 Edward's Playground - AIDEFEND framework Initiative. For informational purposes only.</p>
    </footer>


    <script>
        // --- Data ---
        const aidefendIntroduction = {
            mainTitle: "Introducing AIDEFEND: An AI Defense Framework",
            sections: [
                {
                    title: "What is AIDEFEND?",
                    paragraphs: [
                        "AIDEFEND (Artificial Intelligence Defense Framework) is a structured and actionable knowledge base of defensive countermeasures designed to protect Artificial Intelligence (AI) and Machine Learning (ML) systems. Inspired by established cybersecurity frameworks such as MITRE D3FEND, MITRE ATT&CK®, and MITRE ATLAS®, AIDEFEND addresses the emerging security challenges specific to AI. Just as D3FEND complements ATT&CK® for general cybersecurity, this framework aims to complement MITRE ATLAS® (which focuses on AI attack techniques) by providing a dedicated resource for AI defense.<br><br><strong>Please note: This work is a personal initiative. It draws inspiration from the valuable contributions of several sources, including frameworks (D3FEND, ATT&CK, ATLAS) by MITRE, the MAESTRO Threat Modeling framework by Ken Huang (Cloud Security Alliance Research Fellow), and the OWASP Top 10 Lists (LLM Applications 2025, ML Security 2023) from OWASP. However, this work is not affiliated with, endorsed by, or otherwise connected to the MITRE Corporation, the creator of the MAESTRO framework (Ken Huang), or OWASP.</strong>"
                    ]
                },
                {
                    title: "What has been developed?",
                    paragraphs: [
                        "The AIDEFEND framework utilizes the seven defensive tactics from MITRE D3FEND—Model, Harden, Detect, Isolate, Deceive, Evict, and Restore—as its foundational structure. Within each tactic, specific defensive techniques relevant to AI security have been identified and organized. Crucially, these techniques are mapped to corresponding AI-specific attacks and threats. This mapping incorporates insights from prominent sources such as MITRE ATLAS®, the MAESTRO Threat Modeling framework, the OWASP Top 10 for Large Language Model Applications (2025), and the OWASP Machine Learning Security Top 10 (2023), providing a comprehensive view of how each defensive measure can mitigate known vulnerabilities."
                    ]
                },
                {
                    title: "How can this framework be utilized?",
                    paragraphs: [
                        "AIDEFEND is designed as a practical tool for organizations to systematically enhance their AI security posture. It is presented in a matrix format, aligning defensive techniques with the D3FEND tactical categories. This structure allows security professionals to:"
                    ],
                    listItems: [
                        "<strong>Assess Current Capabilities:</strong> Evaluate existing AI defenses against the AIDEFEND framework.",
                        "<strong>Identify Gaps:</strong> Pinpoint areas where AI-specific defenses are lacking.",
                        "<strong>Prioritize Defenses:</strong> Use the threat mappings (to ATLAS, MAESTRO, OWASP) to select techniques that address the most relevant risks to their specific AI systems, informed by their own risk assessments.",
                        "<strong>Plan Implementation:</strong> Leverage the provided descriptions, implementation strategies, and tool suggestions to develop actionable plans.",
                        "<strong>Enhance AI Security Posture:</strong> Systematically improve the resilience of AI deployments."
                    ],
                    concludingParagraphs: [
                         "Each technique in the matrix includes a unique ID, name, a description of the defensive method focused on AI, practical implementation strategies, and examples of open-source and commercial tools. The \"Defends Against\" column explicitly links the technique to threats from MITRE ATLAS®, MAESTRO, and the relevant OWASP Top 10 lists."
                    ]
                },
                {
                    title: "Who is behind this initiative?",
                    paragraphs: [
                        "This work is led by Edward Lee. I'm passionate about Cybersecurity, AI and emerging technologies, and will always be a learner. <a href=\"https://www.linkedin.com/in/go-edwardlee/\" target=\"_blank\" rel=\"noopener noreferrer\">Connect with me on LinkedIn</a>."
                    ]
                },
                {
                    title: "Version & Date",
                    paragraphs: [
                        "Version: 1.0",
                        "Last Updated: June 3, 2025"
                    ]
                }
            ]
        };

        const aidefendData = {
            introduction: aidefendIntroduction,
            tactics: [
                {
                    name: "Model",
                    purpose: "The \"Model\" tactic, in the context of AI security, focuses on developing a comprehensive understanding and detailed mapping of all AI/ML assets, their configurations, data flows, operational behaviors, and interdependencies. This foundational knowledge is crucial for informing and enabling all subsequent defensive actions. It involves knowing precisely what AI systems exist within the organization, how they are architected, what data they ingest and produce, their critical dependencies (both internal and external), and their expected operational parameters and potential emergent behaviors.",
                    techniques: [
                        { id: "AIDEFEND-M-001", name: "AI Asset Inventory & Mapping", description: "Systematically catalog and map all AI/ML assets, including models (categorized by type, version, deployment location, and ownership), datasets (training, validation, testing, and operational), data pipelines, and APIs. This process includes mapping their configurations, data flows (sources, transformations, destinations), and interdependencies (e.g., reliance on third-party APIs, upstream data providers, or specific libraries). The goal is to achieve comprehensive visibility into all components that constitute the AI ecosystem and require protection. This technique is foundational as it underpins the ability to apply targeted security controls and assess risk accurately.", implementationStrategies: ["Establish and maintain a dynamic, up-to-date inventory of all AI models, datasets, software components, and associated infrastructure.", "Map data flows for each AI system, documenting data sources, lineage, processing stages, storage locations, and consumers.", "Document dependencies for each AI asset, including software libraries, external services, and other AI models.", "Regularly audit the inventory and mappings for accuracy and completeness, updating them as AI systems evolve.", "Assign clear ownership and accountability for each inventoried AI asset and its security.", "Integrate AI asset inventory with broader IT asset management and configuration management databases (CMDBs) where appropriate.", "Utilize automated discovery tools where possible, but supplement with manual verification, especially for novel AI components."], toolsOpenSource: ["Custom scripts for querying model registries (e.g., MLflow, Kubeflow) and data storage.", "Great Expectations (for data asset profiling and documentation).", "DVC (Data Version Control, for tracking dataset versions and lineage).", "Apache Atlas, DataHub, Amundsen, OpenMetadata (for comprehensive metadata management, data discovery, and lineage).", "General IT asset management tools like Snipe-IT or Budibase may be adapted."], toolsCommercial: ["AI Security Posture Management (AI-SPM) platforms: Wiz AI-SPM, Microsoft Defender for Cloud, Palo Alto Networks Prisma Cloud AI-SPM.", "Data catalog and governance platforms: Alation, Collibra, Informatica Enterprise Data Catalog, OvalEdge.", "MLOps platforms with model registry and artifact tracking: Azure ML, Google Vertex AI, Databricks, Amazon SageMaker.", "Specialized AI inventory management software."], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0007 Discover ML Artifacts", "AML.T0002 Acquire Public ML Artifacts"] }, { framework: "MAESTRO", items: ["Foundational for assessing risks across all layers", "Agent Supply Chain (L7) understanding"] }, { framework: "OWASP LLM Top 10 2025", items: ["Indirectly LLM03:2025 Supply Chain"] }, { framework: "OWASP ML Top 10 2023", items: ["Indirectly ML06:2023 AI Supply Chain Attacks"] }] },
                        { id: "AIDEFEND-M-002", name: "Data Provenance & Lineage Tracking", description: "Establish and maintain verifiable records of the origin, history, and transformations of data used in AI systems, particularly training and fine-tuning data. This includes tracking model updates and their associated data versions. The objective is to ensure the trustworthiness and integrity of data and models by knowing their complete lifecycle, from source to deployment, and to facilitate auditing and incident investigation. This often involves cryptographic methods like signing or checksumming datasets and models at critical stages.", implementationStrategies: ["Implement robust data version control systems (e.g., DVC, Git-LFS for data).", "Maintain detailed metadata for datasets (e.g., \"datasheets for datasets\") and models (e.g., \"model cards\").", "Employ cryptographic checksums (e.g., SHA-256) or digital signatures.", "Rigorously vet and document third-party or public data sources.", "Automate lineage tracking where possible.", "Regularly audit data provenance and lineage records."], toolsOpenSource: ["DVC", "MLflow", "Apache Atlas, DataHub, Amundsen, OpenMetadata", "LakeFS", "Pachyderm"], toolsCommercial: ["Azure ML", "Google Vertex AI", "Collibra, Alation, Informatica PowerCenter, Talend Data Catalog", "Databricks"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0020 Poison Training Data", "AML.T0008 ML Supply Chain Compromise", "AML.T0018.000 Backdoor ML Model: Poison ML Model"] }, { framework: "MAESTRO", items: ["Data Poisoning (L2: Data Operations)", "Compromised RAG Pipelines (L2: Data Operations)", "Model Skewing (L2: Data Operations)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM04:2025 Data and Model Poisoning", "LLM03:2025 Supply Chain"] }, { framework: "OWASP ML Top 10 2023", items: ["ML02:2023 Data Poisoning Attack", "ML10:2023 Model Poisoning", "ML07:2023 Transfer Learning Attack"] }] },
                        { id: "AIDEFEND-M-003", name: "Model Behavior Baseline & Documentation", description: "Establish, document, and maintain a comprehensive baseline of expected AI model behavior. This includes defining its intended purpose, architectural details, training data characteristics, operational assumptions, limitations, and key performance metrics (e.g., accuracy, precision, recall, output distributions, latency, confidence scores) under normal conditions. This documentation, often in the form of model cards, and the established behavioral baseline serve as a reference to detect anomalies, drift, or unexpected outputs that might indicate an attack or system degradation, and to inform risk assessments and incident response.", implementationStrategies: ["Develop detailed model cards for each deployed AI model.", "Establish quantitative baselines for key performance and operational metrics.", "Simulate expected usage patterns to record normative behavior.", "Regularly review and update model documentation and baselines.", "Store model cards and baseline documentation in a centralized repository."], toolsOpenSource: ["Alibi Detect", "Evidently AI, ClearML, NannyML, Langfuse, Phoenix", "Google's Model Card Toolkit", "Sphinx or MkDocs"], toolsCommercial: ["Fiddler, Arize AI, WhyLabs", "IBM Watson OpenScale, Azure Model Monitor, Amazon SageMaker Model Monitor, Google Vertex AI Model Monitoring", "BytePlus ModelArk", "Google Cloud Model Card service"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0015 Evade ML Model", "AML.T0054 LLM Jailbreak", "AML.T0021 Erode ML Model Integrity"] }, { framework: "MAESTRO", items: ["Evasion of Security AI Agents (L6)", "Unpredictable agent behavior / Performance Degradation (L5)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection (jailbreaking aspect)", "LLM09:2025 Misinformation"] }, { framework: "OWASP ML Top 10 2023", items: ["ML01:2023 Input Manipulation Attack", "ML08:2023 Model Skewing"] }] },
                        { id: "AIDEFEND-M-004", name: "AI Threat Modeling & Risk Assessment", description: "Systematically identify, analyze, and prioritize potential AI-specific threats and vulnerabilities for each AI component (e.g., data, models, algorithms, pipelines, agentic capabilities, APIs) throughout its lifecycle. This process involves understanding how an adversary might attack the AI system and assessing the potential impact of such attacks. The outcomes guide the design of appropriate defensive measures and inform risk management strategies. This proactive approach is essential for building resilient AI systems.", implementationStrategies: ["Utilize established threat modeling methodologies (STRIDE, PASTA, OCTAVE) adapted for AI.", "Leverage AI-specific threat frameworks (ATLAS, MAESTRO, OWASP).", "For agentic AI, consider tool misuse, memory tampering, goal manipulation, etc.", "Involve a multi-disciplinary team.", "Prioritize risks based on likelihood and impact.", "Document threat models and integrate into MLOps.", "Regularly review and update threat models."], toolsOpenSource: ["MITRE ATLAS Navigator", "MAESTRO framework documentation", "OWASP Top 10 checklists", "OWASP Threat Dragon, Microsoft Threat Modeling Tool", "Academic frameworks (ATM for LLMs, ATFAA)", "NIST AI RMF and Playbook"], toolsCommercial: ["AI security consulting services", "AI governance and risk management platforms (OneTrust AI Governance, FlowForma)", "Some AI red teaming platforms"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["Proactively addresses all relevant tactics (Reconnaissance, Resource Development, Initial Access, ML Model Access, Execution, Impact, etc.)"] }, { framework: "MAESTRO", items: ["Systematically addresses threats across all 7 Layers and Cross-Layer threats"] }, { framework: "OWASP LLM Top 10 2025", items: ["Enables proactive consideration for all 10 risks (LLM01-LLM10)"] }, { framework: "OWASP ML Top 10 2023", items: ["Enables proactive consideration for all 10 risks (ML01-ML10)"] }] },
                        { id: "AIDEFEND-M-005", name: "AI Configuration Benchmarking & Secure Baselines", description: "Establish, document, maintain, and regularly audit secure configurations for all components of AI systems. This includes the underlying infrastructure (cloud instances, GPU clusters, networks), ML libraries and frameworks, agent runtimes, MLOps pipelines, and specific settings within AI platform APIs (e.g., LLM function access). Configurations are benchmarked against industry standards (e.g., CIS Benchmarks, NIST SSDF), vendor guidance, and internal security policies to identify and remediate misconfigurations that could be exploited by attackers.", implementationStrategies: ["Develop and enforce secure baseline configurations.", "Harden default settings for AI platforms and tools.", "Utilize security benchmarks (CIS, NIST SSDF) and vulnerability databases.", "Implement Infrastructure as Code (IaC) and use IaC security scanners.", "Regularly audit deployed configurations for drift.", "Integrate AI-specific configuration policies into CSPM tools."], toolsOpenSource: ["OpenSCAP", "Checkov, Terrascan, tfsec", "CIS Benchmarks", "NIST SSDF"], toolsCommercial: ["CSPM tools (Wiz, Prisma Cloud, Microsoft Defender)", "Vulnerability management solutions", "Configuration management tools (Ansible, Chef, Puppet)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0011 Initial Access (misconfigurations)", "AML.T0009 Execution (insecure settings)"] }, { framework: "MAESTRO", items: ["Misconfigurations in L4: Deployment & Infrastructure", "Insecure default settings in L3: Agent Frameworks or L1: Foundation Models"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM03:2025 Supply Chain", "Indirectly LLM06:2025 Excessive Agency"] }, { framework: "OWASP ML Top 10 2023", items: ["ML06:2023 AI Supply Chain Attacks (misconfigured components)"] }] }
                    ]
                },
                {
                    name: "Harden",
                    purpose: "The \"Harden\" tactic encompasses proactive measures taken to reinforce AI systems and reduce their attack surface before an attack occurs. These techniques aim to make AI models, the data they rely on, and the infrastructure they inhabit more resilient to compromise. This involves building security into the design and development phases and applying preventative controls to make successful attacks more difficult, costly, and less impactful for adversaries.",
                    techniques: [
                        { id: "AIDEFEND-H-001", name: "Adversarial Training & Robust Model Architectures", description: "Proactively improve a model's resilience to adversarial inputs by training it with examples specifically crafted to try and fool it (adversarial examples). This process \"vaccinates\" the model, making it more robust against evasion attacks where slight, often imperceptible, perturbations to input data cause misclassification or other erroneous behavior. This can be complemented by selecting or designing model architectures (e.g., ensembles, specific types of neural network layers or activation functions) that are inherently more resistant to such manipulations.", implementationStrategies: ["Generate diverse adversarial examples (FGSM, PGD, C&W).", "Incorporate adversarial examples into training data.", "Utilize robust model architectures (ensembles, certified robustness).", "Employ defensive distillation.", "Apply feature squeezing or input transformations.", "Regularly evaluate and retrain for robustness."], toolsOpenSource: ["Adversarial Robustness Toolbox (ART) by IBM", "Foolbox", "CleverHans", "TensorFlow Privacy, PyTorch Opacus"], toolsCommercial: ["Robust Intelligence", "HiddenLayer MLSec", "Bosch AIShield", "Adversa AI", "Microsoft Counterfit"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0015 Evade ML Model", "AML.T0006 Defense Evasion"] }, { framework: "MAESTRO", items: ["Adversarial Examples (L1)", "Evasion of Security AI Agents (L6)"] }, { framework: "OWASP LLM Top 10 2025", items: ["Indirectly LLM01:2025 Prompt Injection"] }, { framework: "OWASP ML Top 10 2023", items: ["ML01:2023 Input Manipulation Attack"] }] },
                        { id: "AIDEFEND-H-002", name: "AI-Contextualized Data Sanitization & Input Validation", description: "Implement rigorous validation, sanitization, and filtering mechanisms for all data fed into AI systems. This applies to training data, fine-tuning data, and live operational inputs (including user prompts for LLMs). The goal is to detect and remove or neutralize malicious content, anomalous data, out-of-distribution samples, or inputs structured to exploit vulnerabilities like prompt injection or data poisoning before they can adversely affect the model or downstream systems. For LLMs, this involves specific techniques like stripping or encoding control tokens and filtering for known injection patterns or harmful content.", implementationStrategies: ["Perform EDA on training data.", "Use automated data validation tools.", "Implement anomaly detection for training data.", "Verify integrity of external data.", "Apply strict input validation for inference/prompts.", "Sanitize LLM prompts (strip/encode control tokens, filter patterns).", "Use LLM-based guardrails or secondary models for prompt safety.", "Normalize inputs."], toolsOpenSource: ["Rebuff", "TensorFlow Data Validation", "Great Expectations", "LangChain Guardrails", "LlamaFirewall", "NVIDIA NeMo Guardrails", "Pydantic"], toolsCommercial: ["OpenAI Moderation API, Google Perspective API", "CalypsoAI Validator", "Securiti LLM Firewall for Prompts", "WAFs with AI/LLM rulesets", "Data quality platforms"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0020 Poison Training Data", "AML.T0051 LLM Prompt Injection", "AML.T0018.000 Backdoor ML Model: Poison ML Model", "AML.T0054 LLM Jailbreak"] }, { framework: "MAESTRO", items: ["Data Poisoning (L2)", "Input Validation Attacks (L3)", "Manipulation of Evaluation Metrics (L5)", "Agent Tool Misuse / Agent Goal Manipulation (L7)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection", "LLM04:2025 Data and Model Poisoning", "LLM08:2025 Vector and Embedding Weaknesses"] }, { framework: "OWASP ML Top 10 2023", items: ["ML01:2023 Input Manipulation Attack", "ML02:2023 Data Poisoning Attack"] }] },
                        { id: "AIDEFEND-H-003", name: "Secure ML Supply Chain Management", description: "Apply rigorous software supply chain security principles throughout the AI/ML development and operational lifecycle. This involves verifying the integrity, authenticity, and security of all components, including source code, pre-trained models, datasets, ML libraries, development tools, and deployment infrastructure. The aim is to prevent the introduction of vulnerabilities, backdoors, malicious code (e.g., via compromised dependencies), or tampered artifacts into the AI system. This is critical as AI systems often rely on a complex ecosystem of third-party elements.", implementationStrategies: ["Use digital signatures/checksums for ML artifacts.", "Maintain strong provenance records for models and datasets.", "Scan ML libraries and dependencies for vulnerabilities (SCA).", "Monitor for malicious packages in public repositories.", "Generate and maintain SBOMs and AI-BOMs.", "Thoroughly vet third-party providers.", "Implement secure coding practices for ML code.", "Secure the MLOps pipeline itself."], toolsOpenSource: ["Microsoft Counterfit", "Trivy, Safety, OWASP Dependency-Check", "OWASP CycloneDX, SPDX tools, Syft, Grype", "Sigstore, in-toto", "GUAC"], toolsCommercial: ["Protect AI Platform (AI Shield, ModelScan, NB Defense, LLM Guard)", "Veracode for ML", "Checkmarx One", "ReversingLabs Spectra Assure", "Snyk, Mend, JFrog Xray", "Hugging Face Hub (model scanning)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0008 ML Supply Chain Compromise (Data, ML Software, Model, Hardware)", "AML.T0018 Backdoor ML Model", "AML.T0020 Poison Training Data (via supply chain)"] }, { framework: "MAESTRO", items: ["Compromised Framework Components (L3)", "Compromised Container Images (L4)", "Data Poisoning (L2, external sources)", "Supply Chain Attacks (Cross-Layer)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM03:2025 Supply Chain"] }, { framework: "OWASP ML Top 10 2023", items: ["ML06:2023 AI Supply Chain Attacks", "ML10:2023 Model Poisoning (via supply chain)"] }] },
                        { id: "AIDEFEND-H-004", name: "Identity & Access Management (IAM) for AI Systems", description: "Implement and enforce comprehensive Identity and Access Management (IAM) controls for all AI resources, including models, APIs, data stores (training, inference, vector DBs), agentic tools, MLOps pipelines, and administrative interfaces. This involves applying the principle of least privilege, strong authentication mechanisms (including MFA), robust authorization policies, and secure credential management to limit who (users, services, other AIs) and what (processes, agents) can interact with, modify, deploy, or manage AI systems and their constituent components. This is a fundamental cybersecurity practice with critical AI-specific considerations.", implementationStrategies: ["Require strong, unique credentials for all AI access.", "Implement MFA for human users on sensitive systems.", "Define granular roles and permissions (RBAC/ABAC).", "Enforce principle of least privilege for identities and AI agents.", "Securely store and manage secrets (Vault, KMS).", "Implement automated credential rotation.", "Regularly audit access logs and permissions.", "Promptly de-provision access."], toolsOpenSource: ["Keycloak", "OpenID Connect, OAuth2, SAML libraries", "HashiCorp Vault", "Apache Syncope"], toolsCommercial: ["Cloud Provider IAM (AWS IAM, Azure AD, Google Cloud IAM)", "IDaaS (Okta, Ping Identity, Auth0)", "PAM (CyberArk, Delinea, BeyondTrust)", "Entitle AI"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0011 Initial Access (credentials)", "AML.T0010 Privilege Escalation", "AML.T0012 Valid Accounts", "AML.T0040 AI Model Inference API Access"] }, { framework: "MAESTRO", items: ["Agent Identity Attack, Compromised Agent Registry, Agent Tool Misuse (L7)", "Unauthorized access to any layer"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM06:2025 Excessive Agency", "LLM02:2025 Sensitive Information Disclosure", "LLM07:2025 System Prompt Leakage (misuse of leaked creds)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft", "ML03:2023 Model Inversion Attack & ML04:2023 Membership Inference Attack (by limiting query access)"] }] },
                        { id: "AIDEFEND-H-005", name: "Privacy-Preserving Machine Learning (PPML) Techniques", description: "Employ a range of advanced cryptographic and statistical techniques during AI model training, fine-tuning, and inference to protect the privacy of sensitive information within datasets. These methods aim to prevent the leakage of individual data records, membership inference (determining if a specific record was in the training set), or the reconstruction of sensitive inputs from model outputs (model inversion). Key PPML approaches include differential privacy, homomorphic encryption, federated learning, and secure multi-party computation (SMPC), often leveraging hardware-based secure enclaves (confidential computing).", implementationStrategies: ["Inject calibrated noise for Differential Privacy (DPSGD).", "Manage privacy budget for DP.", "Train/infer on encrypted data with Homomorphic Encryption.", "Train shared models on decentralized data with Federated Learning.", "Jointly compute functions with Secure Multi-Party Computation.", "Utilize Trusted Execution Environments (TEEs) for Confidential Computing.", "Apply data anonymization/pseudonymization (k-anonymity, etc.).", "Limit retention of raw sensitive training data."], toolsOpenSource: ["Differential Privacy: PyTorch Opacus, TensorFlow Privacy, Google DP Library, IBM Diffprivlib, OpenDP", "Federated Learning: TensorFlow Federated, PySyft, Flower, NVIDIA FLARE, OpenFL", "Homomorphic Encryption: Microsoft SEAL, PALISADE, HElib, TFHE, Zama.ai Concrete, OpenFHE", "Secure Enclaves: SDKs for Intel SGX, AMD SEV, AWS Nitro Enclaves"], toolsCommercial: ["PPML Platforms: TripleBlind, Cape Privacy, Enveil ZeroReveal® ML, Duality Technologies", "Confidential Computing: IBM zCrypto for AI, Azure Confidential Computing, Google Cloud Confidential Computing, AWS Nitro Enclaves"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0024.000 Infer Training Data Membership", "AML.T0024.001 Invert ML Model", "AML.T0025 Exfiltration via Cyber Means (reduced utility)", "AML.T0057 LLM Data Leakage (memorization)"] }, { framework: "MAESTRO", items: ["Data Exfiltration (L2)", "Model Inversion/Extraction (L2)", "Membership Inference Attacks (L1/L2)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM02:2025 Sensitive Information Disclosure"] }, { framework: "OWASP ML Top 10 2023", items: ["ML03:2023 Model Inversion Attack", "ML04:2023 Membership Inference Attack"] }] },
                        { id: "AIDEFEND-H-006", name: "Secure AI System Design & Zero Trust Architecture Integration", description: "Embed security principles deeply into the architecture and design of AI systems from their inception (\"secure by design\"). This includes applying Zero Trust Architecture (ZTA) principles, which operate on the premise of \"never trust, always verify.\" Every access request to or from any AI component, data store, API, or service must be explicitly verified, regardless of whether the source is internal or external to the network. This approach minimizes implicit trust zones and helps contain breaches. It also encompasses planning for secure model updates, patching, and eventual decommissioning of AI systems and artifacts.", implementationStrategies: ["Incorporate security into all MLOps phases.", "Conduct AI-specific threat modeling early.", "Evaluate security of third-party components.", "Zero Trust: Strong authentication/authorization for all entities.", "Microsegmentation to isolate AI workloads and data.", "Enforce least privilege access.", "Continuously monitor and validate security posture.", "Protect data at rest, in transit, and in use (PPML).", "Establish secure processes for updates, patching, decommissioning.", "Consider separating agent control and execution planes."], toolsOpenSource: ["Guidance: NIST SP 800-207 (ZTA)", "Microsegmentation: Kubernetes Network Policies, Calico, Cilium, Istio", "IAM: Keycloak, Open Policy Agent (OPA)"], toolsCommercial: ["ZTNA solutions (Zscaler, Prisma Access, Cisco Secure Access)", "Microsegmentation platforms (Illumio, Guardicore, Cisco Secure Workload)", "PAM solutions", "Cloud provider ZT services (AWS IAM, Azure AD Conditional Access, Google BeyondCorp)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["Broadly mitigates: AML.T0011 Initial Access, AML.T0010 Privilege Escalation, Lateral Movement principles, AML.T0008 ML Supply Chain Compromise"] }, { framework: "MAESTRO", items: ["Addresses threats across all layers by strict verification, segmentation, least privilege. Secures L4, L6, and mitigates Cross-Layer threats."] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM06:2025 Excessive Agency", "LLM03:2025 Supply Chain", "LLM02:2025 Sensitive Information Disclosure"] }, { framework: "OWASP ML Top 10 2023", items: ["Reduces overall attack surface; ML05:2023 Model Theft becomes harder."] }] }
                    ]
                },
                {
                    name: "Detect",
                    purpose: "The \"Detect\" tactic focuses on the timely identification of intrusions, malicious activities, anomalous behaviors, or policy violations occurring within or targeting AI systems. This involves continuous or periodic monitoring of various aspects of the AI ecosystem, including inputs (prompts, data feeds), outputs (predictions, generated content, agent actions), model behavior (performance metrics, drift), system logs (API calls, resource usage), and the integrity of AI artifacts (models, datasets).",
                    techniques: [
                        { id: "AIDEFEND-D-001", name: "Adversarial Input & Prompt Injection Detection", description: "Implement mechanisms to continuously monitor and analyze inputs to AI models, specifically looking for characteristics indicative of adversarial manipulation or malicious prompt content. This includes detecting statistically anomalous inputs (e.g., out-of-distribution samples, inputs with unusual perturbation patterns) and scanning prompts for known malicious patterns, hidden commands, jailbreak sequences, or attempts to inject executable code or harmful instructions. The goal is to block, flag, or sanitize such inputs before they can significantly impact the model's behavior or compromise the system.", implementationStrategies: ["Deploy anomaly detection on input streams.", "Utilize specialized content scanners and LLM-specific detectors for prompts.", "Implement input reconstruction or defensive transformation techniques.", "Leverage ensemble models for output disagreement detection.", "Monitor for high query rates or specific query structures.", "Regularly update detection rules and models."], toolsOpenSource: ["vigil-llm", "Garak", "Adversarial Robustness Toolbox (ART)", "Alibi Detect", "LangChain", "Rebuff"], toolsCommercial: ["Microsoft Azure AI Content Safety", "Hugging Face Guardrails", "Lakera Guard", "Dedicated LLM Firewalls (Securiti, Protect AI, etc.)", "API security solutions"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0015 Evade ML Model", "AML.T0051 LLM Prompt Injection", "AML.T0054 LLM Jailbreak"] }, { framework: "MAESTRO", items: ["Adversarial Examples (L1)", "Input Validation Attacks (L3)", "Evasion of Security AI Agents (L6)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection"] }, { framework: "OWASP ML Top 10 2023", items: ["ML01:2023 Input Manipulation Attack"] }] },
                        { id: "AIDEFEND-D-002", name: "AI Model Anomaly & Performance Drift Detection", description: "Continuously monitor the outputs, performance metrics (e.g., accuracy, confidence scores, precision, recall, F1-score, output distribution), and potentially internal states or feature attributions of AI models during operation. This monitoring aims to detect significant deviations from established baselines or expected behavior. Such anomalies or drift can indicate various issues, including concept drift (changes in the underlying data distribution), data drift (changes in input data characteristics), or malicious activities like ongoing data poisoning attacks, subtle model evasion attempts, or model skewing.", implementationStrategies: ["Establish and maintain robust baseline of key model performance metrics.", "Track metrics in real-time or near real-time.", "Employ statistical concept drift and data drift detection algorithms.", "Monitor for unusual model decisions or output distributions.", "Investigate flagged anomalies and drift promptly.", "Implement feedback loops for model retraining/recalibration."], toolsOpenSource: ["Alibi Detect", "River", "Evidently AI", "NannyML", "scikit-multiflow", "TensorFlow Data Validation (TFDV), TensorFlow Model Analysis (TFMA)"], toolsCommercial: ["IBM Watson OpenScale", "Azure Model Monitor", "Fiddler AI, Arize AI, WhyLabs, Seldon Deploy", "Amazon SageMaker Model Monitor, Google Vertex AI Model Monitoring", "Protect AI (Layer product)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0020 Poison Training Data (detecting impact)", "AML.T0015 Evade ML Model (detecting anomalies)", "AML.T0021 Erode ML Model Integrity", "AML.T0019 Poison ML Model (detecting impact)"] }, { framework: "MAESTRO", items: ["Data Poisoning (L2, impact detection)", "Model Skewing (L2/L5)", "Unpredictable agent behavior / Performance Degradation (L5)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM04:2025 Data and Model Poisoning (impact detection)", "LLM09:2025 Misinformation (if drift leads to factual errors)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML02:2023 Data Poisoning Attack (impact detection)", "ML08:2023 Model Skewing", "ML10:2023 Model Poisoning (behavioral changes)"] }] },
                        { id: "AIDEFEND-D-003", name: "AI Output Monitoring & Policy Enforcement", description: "Actively inspect the outputs generated by AI models (e.g., text responses from LLMs, classifications, predictions, agent actions, generated code or images) for malicious content, policy violations, leakage of sensitive information, or indications that an attack has succeeded or the model is misbehaving. This involves enforcing predefined safety, security, and ethical policies on the outputs, and taking action (e.g., blocking, sanitizing, alerting) when violations are detected.", implementationStrategies: ["Deploy content filtering on AI outputs.", "For LLMs, implement reference checks or use critic models.", "For agentic AI, monitor actions against policies and permissions.", "Establish robust feedback loop for policy violations.", "Regularly update output policies and detection rules."], toolsOpenSource: ["OpenAI Guardrails (custom coded)", "LangChain with custom output parsers/validation", "Meta's LlamaFirewall", "NVIDIA NeMo Guardrails", "Pydantic for output schema validation"], toolsCommercial: ["Anthropic's Claude monitoring features", "Crossing Minds WaiGuard", "Securiti LLM Firewall for Responses", "AI safety and content moderation platforms (Hive AI, Scale AI)", "AI observability platforms (Fiddler, Arize)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0057 LLM Data Leakage", "AML.T0048.002 External Harms: Societal Harm", "AML.T0051 LLM Prompt Injection / AML.T0054 LLM Jailbreak (detecting violating outputs)", "AML.T0009.002 Execution: LLM Plugin Compromise (detecting violating plugin output)"] }, { framework: "MAESTRO", items: ["Agent Tool Misuse, Agent Goal Manipulation (L7)", "Data Leakage through Observability (L5)", "Inaccurate Agent Capability Description leading to misuse"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM02:2025 Sensitive Information Disclosure", "LLM05:2025 Improper Output Handling", "LLM09:2025 Misinformation", "LLM06:2025 Excessive Agency"] }, { framework: "OWASP ML Top 10 2023", items: ["ML09:2023 Output Integrity Attack"] }] },
                        { id: "AIDEFEND-D-004", name: "Model & AI Artifact Integrity Audit & Tamper Detection", description: "Regularly verify the cryptographic integrity and authenticity of deployed AI models, their parameters, associated datasets, and critical components of their runtime environment. This process aims to detect any unauthorized modifications, tampering, or the insertion of backdoors that could compromise the model's behavior, security, or data confidentiality. It ensures that the AI artifacts in operation are the approved, untampered versions.", implementationStrategies: ["Maintain secure model registry with known-good hashes/signatures.", "Regularly compute checksums/verify signatures of deployed models.", "Utilize runtime attestation mechanisms (TEEs, TPMs).", "Log and analyze all model update events.", "Monitor for unexpected changes in model files, dependencies, or performance.", "Extend integrity checks to AI runtime configuration files."], toolsOpenSource: ["AIMSIC (research tool)", "Standard hashing (sha256sum) and signature tools (GnuPG)", "Tripwire (general FIM)", "Secure enclave attestation SDKs", "Git with signed commits/tags"], toolsCommercial: ["Protect AI Platform (NB Defense, ModelScan)", "XAPSec (AI model firewall)", "HiddenLayer Model Scanner", "MLOps platforms with artifact tracking", "Traditional FIM solutions"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0018 Backdoor ML Model / AML.T0019 Poison ML Model", "AML.T0009.001 Execution: ML Code Injection (if persistent changes)", "AML.T0021 Erode ML Model Integrity", "AML.T0008 ML Supply Chain Compromise (post-initial verification tampering)"] }, { framework: "MAESTRO", items: ["Backdoor Attacks (L1/L3)", "Data Tampering (L2, model parameters as files)", "Compromised Framework Components / Container Images (L3/L4)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM04:2025 Data and Model Poisoning (model poisoning aspect)", "LLM03:2025 Supply Chain (tampered model deployment)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML10:2023 Model Poisoning", "ML06:2023 AI Supply Chain Attacks (replacing legitimate files)"] }] },
                        { id: "AIDEFEND-D-005", name: "AI Activity Logging, Monitoring & Threat Hunting", description: "Establish and maintain detailed, comprehensive, and auditable logs of all significant activities related to AI systems. This includes user queries and prompts, model responses and confidence scores, decisions made by AI (especially autonomous agents), tools invoked by agents, data accessed or modified, API calls (to and from the AI system), system errors, and security-relevant events. These logs are then ingested into security monitoring systems (e.g., SIEM) for correlation, automated alerting on suspicious patterns, and proactive threat hunting by security analysts to identify indicators of compromise (IoCs) or novel attack patterns targeting AI systems.", implementationStrategies: ["Enable verbose logging for all AI interactions (inputs, outputs, agent actions, API calls, errors).", "Ensure logs are timestamped, immutable, and securely stored.", "Ingest AI-specific logs into centralized SIEM/log analytics.", "Correlate AI logs with other security data sources.", "Develop AI-specific detection rules and alerts in SIEM.", "Proactively search logs for subtle IoCs or anomalous behaviors.", "Utilize AI/ML for anomaly detection in AI system logs."], toolsOpenSource: ["ELK Stack (Elasticsearch, Logstash, Kibana) or OpenSearch Stack", "Grafana Loki", "Sigma (for SIEM rules)", "Fluentd or Vector", "MLOps framework logging (e.g., MLflow)"], toolsCommercial: ["Splunk (Enterprise, Cloud, SOAR)", "Datadog, Dynatrace, New Relic", "Cloud-native SIEMs (Azure Sentinel, Google Chronicle, AWS Security Hub)", "HiddenLayer MLDR", "AI-SPM tools"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0024.002 Extract ML Model (query patterns)", "AML.T0001 Reconnaissance (unusual queries)", "AML.T0051 LLM Prompt Injection (repeated attempts)", "AML.T0057 LLM Data Leakage (output logging)", "AML.T0012 Valid Accounts (anomalous usage)"] }, { framework: "MAESTRO", items: ["Model Stealing (L1)", "Agent Tool Misuse (L7)", "Compromised RAG Pipelines (L2)", "Data Exfiltration (L2)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM10:2025 Unbounded Consumption (usage patterns)", "LLM01:2025 Prompt Injection (logged attempts)", "LLM02:2025 Sensitive Information Disclosure (logged outputs)", "LLM06:2025 Excessive Agency (logged actions)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft (query patterns)", "ML01:2023 Input Manipulation Attack (logged inputs)"] }] }
                    ]
                },
                {
                    name: "Isolate",
                    purpose: "The \"Isolate\" tactic involves implementing measures to contain malicious activity and limit its potential spread or impact should an AI system or one of its components become compromised. This includes sandboxing AI processes, segmenting networks to restrict communication, and establishing mechanisms to quickly quarantine or throttle suspicious interactions or misbehaving AI entities.",
                    techniques: [
                        { id: "AIDEFEND-I-001", name: "AI Execution Sandboxing & Runtime Isolation", description: "Execute AI models, autonomous agents, or individual AI tools and plugins within isolated environments such as sandboxes, containers, or microVMs. These environments must be configured with strict limits on resources (CPU, memory, GPU, network bandwidth), permissions (filesystem access, system calls, network connectivity), and access to other systems. The primary goal is that if an AI component is compromised or behaves maliciously (e.g., due to prompt injection leading to arbitrary code execution via a tool), the impact is confined to the isolated sandbox, preventing harm to the host system or lateral movement to other parts of the network. This is particularly crucial for running untrusted code, processing untrusted inputs, or when AI agents interact with external, potentially malicious, environments.", implementationStrategies: ["Deploy AI in hardened containers (Docker, Kubernetes) with network policies and resource quotas.", "Use OS-level sandboxing (seccomp, AppArmor) or user-space kernels (gVisor) for untrusted code.", "Employ lightweight VMs (Firecracker, Kata Containers) for stronger isolation.", "Set strict resource quotas for sandboxed AI processes.", "Leverage confidential computing environments (Nitro Enclaves, Azure Confidential Computing).", "Ensure sandboxed environments have minimal privileges and limited network access."], toolsOpenSource: ["Docker, Podman", "Kubernetes", "gVisor", "Kata Containers", "Firecracker", "seccomp, AppArmor, SELinux", "Firejail"], toolsCommercial: ["Cloud provider sandboxing solutions (Azure Container Instances with confidential options, SageMaker isolation, GKE sandboxing)", "RunSafe Aligned", "Confidential Computing platforms"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0009 Execution (confines impact)", "AML.T0010 Privilege Escalation (prevents host compromise)", "AML.T0017 Persistence (limits scope)"] }, { framework: "MAESTRO", items: ["Compromised Container Images / Orchestration Attacks (L4)", "Agent Tool Misuse (L7, sandboxing tool execution)", "Resource Hijacking (L4)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM06:2025 Excessive Agency (limits agent environment)", "LLM05:2025 Improper Output Handling (contains code execution fallout)", "LLM03:2025 Supply Chain (limits harm from compromised component)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML06:2023 AI Supply Chain Attacks (confines malicious component)"] }] },
                        { id: "AIDEFEND-I-002", name: "Network Segmentation & Isolation for AI Systems", description: "Implement network segmentation and microsegmentation strategies to isolate AI systems and their components (e.g., training environments, model serving endpoints, data stores, agent control planes) from general corporate networks and other critical IT/OT systems. This involves enforcing strict communication rules through firewalls, proxies, and network policies to limit an attacker's ability to pivot from a compromised AI component to other parts of the network, or to exfiltrate data to unauthorized destinations. This technique reduces the \"blast radius\" of a security incident involving an AI system.", implementationStrategies: ["Host critical AI components on dedicated network segments (VLANs, VPCs).", "Apply least privilege to network communications for AI systems.", "Utilize API gateways or forward proxies to mediate and control AI traffic.", "Implement microsegmentation (SDN, service mesh, host-based firewalls).", "Separate development/testing environments from production.", "Regularly review and audit network segmentation rules."], toolsOpenSource: ["Linux Netfilter (iptables, nftables), firewalld", "Kubernetes Network Policies", "Service Mesh (Istio, Linkerd, Kuma)", "CNI plugins (Calico, Cilium)", "Open-source API Gateways (Kong, Tyk, APISIX)"], toolsCommercial: ["Microsegmentation platforms (Illumio, Guardicore, Cisco Secure Workload, Akamai Guardicore)", "Next-Generation Firewalls (NGFWs)", "Cloud-native firewall services (AWS Network Firewall, Azure Firewall, Google Cloud Firewall)", "Commercial API Gateway solutions"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0025 Exfiltration via Cyber Means", "General Lateral Movement tactics", "AML.T0003 Resource Development (blocking unauthorized downloads)"] }, { framework: "MAESTRO", items: ["Data Exfiltration (L2/Cross-Layer)", "Lateral Movement (Cross-Layer)", "Compromised RAG Pipelines (L2, isolating components)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM02:2025 Sensitive Information Disclosure (limits exfil paths)", "LLM03:2025 Supply Chain (isolating third-party components)", "LLM06:2025 Excessive Agency (limits reach of compromised agent)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft (isolating repositories)", "ML06:2023 AI Supply Chain Attacks (segmenting components)"] }] },
                        { id: "AIDEFEND-I-003", name: "Quarantine & Throttling of AI Interactions", description: "Implement mechanisms to automatically or manually isolate, rate-limit, or place into a restricted \"safe mode\" specific AI system interactions when suspicious activity is detected. This could apply to individual user sessions, API keys, IP addresses, or even entire AI agent instances. The objective is to prevent potential attacks from fully executing, spreading, or causing significant harm by quickly containing or degrading the capabilities of the suspicious entity. This is an active response measure triggered by detection systems.", implementationStrategies: ["Automated quarantine based on high-risk behavior alerts (cut access, move to honeypot, disable key/account).", "Dynamic rate limiting for anomalous behavior (query spikes, complex queries).", "Stricter rate limits for unauthenticated/less trusted users.", "Design AI systems with a \"safe mode\" or degraded functionality state.", "Utilize SOAR platforms to automate quarantine/throttling actions."], toolsOpenSource: ["Fail2Ban (adapted for AI logs)", "Custom scripts (Lambda, Azure Functions, Cloud Functions) for automated actions", "API Gateways (Kong, Tyk, Nginx) for rate limiting", "Kubernetes for resource quotas/isolation"], toolsCommercial: ["API Security and Bot Management solutions (Cloudflare, Akamai, Imperva)", "ThreatWarrior (automated detection/response)", "SIEM/SOAR platforms (Splunk SOAR, Palo Alto XSOAR, IBM QRadar SOAR)", "WAFs with advanced rate limiting"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0024.002 Extract ML Model (rate-limiting)", "AML.T0029 Denial of ML Service (throttling)", "AML.T0034 Cost Harvesting (limiting rates)", "Active exploitation scenarios (quarantine stops)"] }, { framework: "MAESTRO", items: ["Model Stealing (L1, throttling)", "DoS on Framework APIs / Data Infrastructure (L3/L2)", "Resource Hijacking (L4, containing processes)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM10:2025 Unbounded Consumption (throttling/quarantining)", "LLM01:2025 Prompt Injection (quarantining repeat offenders)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft (throttling excessive queries)"] }] },
                        { id: "AIDEFEND-I-004", name: "Agent Memory & State Isolation", description: "Specifically for agentic AI systems, implement mechanisms to isolate and manage the agent's memory (e.g., conversational context, short-term state, knowledge retrieved from vector databases) and periodically reset or flush it. This defense aims to prevent malicious instructions, poisoned data, or exploited states (e.g., a \"jailbroken\" state) from persisting across multiple interactions, sessions, or from affecting other unrelated agent tasks or instances. It helps to limit the temporal scope of a successful manipulation.", implementationStrategies: ["Implement per-session/per-user conversational context.", "Regularly flush or use short context windows for agent interactions.", "Partition long-term memory (vector DBs) based on trust levels/contexts.", "Implement strict validation/filtering for writes to agent long-term memory.", "Validate and sanitize persisted state information before loading.", "Consider periodic resets of volatile memory for long-running agents."], toolsOpenSource: ["LangChain Guardrails or custom callback handlers", "Custom wrappers in agentic frameworks (AutoGen, CrewAI, Semantic Kernel, LlamaIndex)", "Vector databases (Weaviate, Qdrant, Pinecone) with access controls"], toolsCommercial: ["Humane AI Safety Tools (proprietary memory management)", "Oracle AI Interceptors (conceptual)", "Lasso Security (agent memory lineage/monitoring)", "Enterprise agent platforms with secure state management"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0018.001 Backdoor ML Model: Poison LLM Memory", "AML.T0017 Persistence (preventing long-term state manipulation)", "AML.T0051 LLM Prompt Injection (limits impact duration)"] }, { framework: "MAESTRO", items: ["Agent Goal Manipulation / Agent Tool Misuse (L7, preventing persistent manipulated state)", "Data Poisoning (L2, if agent memory is target)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection (non-persistent malicious context)", "LLM04:2025 Data and Model Poisoning (agent memory as poisoned data)", "LLM08:2025 Vector and Embedding Weaknesses (mitigating malicious data in vector DB)"] }, { framework: "OWASP ML Top 10 2023", items: ["Relevant if agent memory is considered part of model state/operational data."] }] },
                        { id: "AIDEFEND-I-005", name: "Emergency \"Kill-Switch\" / AI System Halt", description: "Establish and maintain a reliable, rapidly invokable mechanism to immediately halt, disable, or severely restrict the operation of an AI model or autonomous agent if it exhibits confirmed critical malicious behavior, goes \"rogue\" (acts far outside its intended parameters in a harmful way), or if a severe, ongoing attack is detected and other containment measures are insufficient. This is a last-resort containment measure designed to prevent catastrophic harm or further compromise.", implementationStrategies: ["Implement automated safety monitors and triggers for critical deviations.", "Provide secure, MFA-protected manual override for human operators.", "Design agents with internal, independent watchdog modules.", "Define clear protocols for kill-switch activation and recovery.", "Develop procedures for safely restarting and verifying AI system post-halt."], toolsOpenSource: ["AI Sentinel (conceptual pattern)", "Custom scripts/automation playbooks (Ansible, cloud CLIs) to stop/delete resources", "Circuit breaker patterns in microservices"], toolsCommercial: ["\"Red Button\" solutions from AI platform vendors", "Edge AI Safeguard solutions", "Alarm.com AI Deterrence (conceptual similarity)", "EDR/XDR solutions (SentinelOne, CrowdStrike) to kill processes/isolate hosts"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0048 External Harms (Societal, Financial, Reputational, User)", "AML.T0029 Denial of ML Service (by runaway agent)", "AML.T0017 Persistence (terminating malicious agent)"] }, { framework: "MAESTRO", items: ["Agent acting on compromised goals/tools leading to severe harm (L7)", "Runaway/critically malfunctioning foundation models (L1)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM06:2025 Excessive Agency (ultimate backstop)", "LLM10:2025 Unbounded Consumption (preventing catastrophic costs)"] }, { framework: "OWASP ML Top 10 2023", items: ["Any ML attack scenario causing immediate, unacceptable harm requiring emergency shutdown."] }] }
                    ]
                },
                {
                    name: "Deceive",
                    purpose: "The \"Deceive\" tactic involves the strategic use of decoys, misinformation, or the manipulation of an adversary's perception of the AI system and its environment. The objectives are to misdirect attackers away from real assets, mislead them about the system's true vulnerabilities or value, study their attack methodologies in a safe environment, waste their resources, or deter them from attacking altogether.",
                    techniques: [
                        { id: "AIDEFEND-DV-001", name: "Honeypot AI Services & Decoy Models/APIs", description: "Deploy decoy AI systems, such as fake LLM APIs, ML model endpoints serving synthetic or non-sensitive data, or imitation agent services, that are designed to appear valuable, vulnerable, or legitimate to potential attackers. These honeypots are instrumented for intensive monitoring to log all interactions, capture attacker TTPs (Tactics, Techniques, and Procedures), and gather threat intelligence without exposing real production systems or data. They can also be used to slow down attackers or waste their resources.", implementationStrategies: ["Set up AI model instances with controlled weaknesses/attractive characteristics.", "Instrument honeypot AI service for detailed logging.", "Design honeypots to mimic production services but ensure isolation.", "Consider honeypots with slow/slightly erroneous responses.", "Integrate honeypot alerts with SIEM/SOC.", "Seed LLM honeypots with trigger phrases or known jailbreak susceptibility."], toolsOpenSource: ["General honeypot frameworks (Cowrie, Dionaea, Conpot) adapted", "Sandboxed open-source LLM as honeypot", "Mock API tools (MockServer, WireMock)"], toolsCommercial: ["Deception technology platforms (TrapX, SentinelOne ShadowPlex, Illusive, Acalvio)", "Specialized AI security vendors with AI honeypot capabilities"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0001 Reconnaissance", "AML.T0024.002 Extract ML Model / AML.T0048.004 External Harms: ML Intellectual Property Theft", "AML.T0051 LLM Prompt Injection / AML.T0054 LLM Jailbreak"] }, { framework: "MAESTRO", items: ["Model Stealing (L1)", "Marketplace Manipulation (L7, decoy agents)", "Evasion of Detection (L5, studying evasion attempts)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection (capturing attempts)", "LLM10:2025 Unbounded Consumption (studying resource abuse)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft (luring to decoy)", "ML01:2023 Input Manipulation Attack (observing attempts)"] }] },
                        { id: "AIDEFEND-DV-002", name: "Honey Data, Decoy Artifacts & Canary Tokens for AI", description: "Strategically seed the AI ecosystem (training datasets, model repositories, configuration files, API documentation) with enticing but fake data, decoy model artifacts (e.g., a seemingly valuable but non-functional or instrumented model file), or canary tokens (e.g., fake API keys, embedded URLs in documents). These \"honey\" elements are designed to be attractive to attackers. If an attacker accesses, exfiltrates, or attempts to use these decoys, it triggers an alert, signaling a breach or malicious activity and potentially providing information about the attacker's actions or location.", implementationStrategies: ["Embed unique, synthetic honey records in datasets/databases.", "Publish fake/instrumented decoy model artifacts.", "Create and embed decoy API keys/access tokens (Canary Tokens).", "Embed trackable URLs/web bugs in fake sensitive documents.", "Watermark synthetic data in honeypots/decoys.", "Ensure honey elements are isolated and cannot impact production.", "Integrate honey element alerts into security monitoring."], toolsOpenSource: ["Canarytokens.org by Thinkst", "Synthetic data generation tools (Faker, SDV)", "Custom scripts for decoy files/API keys"], toolsCommercial: ["Thinkst Canary (commercial platform)", "Deception platforms (Illusive, Acalvio, SentinelOne) with data decoy capabilities", "Some DLP solutions adaptable for honey data"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0025 Exfiltration via Cyber Means (honey data/canaries exfiltrated)", "AML.T0024.002 Extract ML Model (decoy model/canary in docs)", "AML.T0008 ML Supply Chain Compromise (countering with fake vulnerable models)"] }, { framework: "MAESTRO", items: ["Data Exfiltration (L2, detecting honey data exfil)", "Model Stealing (L1, decoy models/watermarked data)", "Unauthorized access to layers with honey tokens"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM02:2025 Sensitive Information Disclosure (honey data mimicking sensitive info)", "LLM03:2025 Supply Chain (decoy artifacts accessed)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft (decoy models/API keys)", "ML02:2023 Data Poisoning Attack (indirectly, if attackers exfil honey data)"] }] },
                        { id: "AIDEFEND-DV-003", name: "Dynamic Response Manipulation for AI Interactions", description: "Implement mechanisms where the AI system, upon detecting suspicious or confirmed adversarial interaction patterns (e.g., repeated prompt injection attempts, queries indicative of model extraction), deliberately alters its responses to be misleading, unhelpful, or subtly incorrect to the adversary. This aims to frustrate the attacker's efforts, waste their resources, make automated attacks less reliable, and potentially gather more intelligence on their TTPs without revealing the deception. The AI might simultaneously alert defenders to the ongoing deceptive engagement.", implementationStrategies: ["Provide subtly incorrect/incomplete/nonsensical outputs to suspected malicious actors.", "Introduce controlled randomization or benign noise into model outputs for suspicious sessions.", "For agentic systems, feign compliance with malicious instructions but perform safe no-ops.", "Subtly degrade quality/utility of responses to queries matching model extraction patterns.", "Ensure deceptive responses are distinguishable by internal monitoring."], toolsOpenSource: ["AdvTorch MTD (research tools for noisy outputs/MTD)", "Custom logic in AI frameworks (LangChain, Semantic Kernel) for deceptive response mode", "Research prototypes for responsive deception"], toolsCommercial: ["Advanced LLM firewalls/AI security gateways with deceptive response policies (e.g., SAP Adversarial AI Protector)", "Adaptable deception technology platforms"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0024.002 Extract ML Model (misleading outputs)", "AML.T0051 LLM Prompt Injection / AML.T0054 LLM Jailbreak (unreliable/misleading payloads)", "AML.T0001 Reconnaissance (inaccurate system info)"] }, { framework: "MAESTRO", items: ["Model Stealing (L1, frustrating extraction)", "Agent Goal Manipulation / Agent Tool Misuse (L7, agent feigns compliance)", "Evasion of Detection (L5, harder to confirm evasion success)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection (unreliable outcome for attacker)", "LLM02:2025 Sensitive Information Disclosure (fake/obfuscated data)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft (unusable responses)", "ML01:2023 Input Manipulation Attack (inconsistent/noisy outputs)"] }] },
                        { id: "AIDEFEND-DV-004", name: "AI Output Watermarking & Telemetry Traps", description: "Embed imperceptible or hard-to-remove watermarks, unique identifiers, or telemetry \"beacons\" into the outputs generated by AI models (e.g., text, images, code). If these outputs are found externally (e.g., on the internet, in a competitor's product, in leaked documents), the watermark or beacon can help trace the output back to the originating AI system, potentially identifying model theft, misuse, or data leakage. Telemetry traps involve designing the AI to produce specific, unique (but benign) outputs for certain rare or crafted inputs, which, if observed externally, indicate that the model or its specific knowledge has been compromised or replicated.", implementationStrategies: ["For text, subtly alter word choices, sentence structures, or token frequencies.", "For images, embed imperceptible digital watermarks in pixel data.", "Instrument model APIs with unique telemetry markers for specific queries.", "Inject unique, identifiable synthetic data points into training set for provenance.", "Ensure watermarks/telemetry don't degrade performance or UX.", "Develop robust methods for detecting watermarks/telemetry externally."], toolsOpenSource: ["MarkLLM (watermarking LLM text)", "SynthID (Google, watermarking AI-generated images/text)", "Steganography libraries (adaptable)", "Research tools for robust NN output watermarking"], toolsCommercial: ["Verance Watermarking (AI content)", "Sensity AI Guard (deepfake detection/watermarking)", "Commercial digital watermarking solutions", "Content authenticity platforms"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0024.002 Extract ML Model / AML.T0048.004 External Harms: ML Intellectual Property Theft", "AML.T0057 LLM Data Leakage (tracing watermarked outputs)", "AML.T0048.002 External Harms: Societal Harm (attributing deepfakes/misinfo)"] }, { framework: "MAESTRO", items: ["Model Stealing (L1, identifying stolen outputs)", "Data Exfiltration (L2, exfiltrated watermarked data)", "Misinformation Generation (L1/L7, attribution/detection)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM02:2025 Sensitive Information Disclosure (leaked watermarked output)", "LLM09:2025 Misinformation (identifying AI-generated content)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft (traceable models/outputs)", "ML09:2023 Output Integrity Attack (watermark destruction reveals tampering)"] }] },
                        { id: "AIDEFEND-DV-005", name: "Decoy Agent Behaviors & Canary Tasks", description: "For autonomous AI agents, design and implement decoy or \"canary\" functionalities, goals, or sub-agents that appear valuable or sensitive but are actually monitored traps. If an attacker successfully manipulates an agent (e.g., via prompt injection or memory poisoning) and directs it towards these decoy tasks or to exhibit certain predefined suspicious behaviors, it triggers an alert, revealing the compromise attempt and potentially the attacker's intentions, without risking real assets.", implementationStrategies: ["Equip agent with shadow/canary goal/tool leading to monitored environment.", "Create dummy 'watcher' agent personas.", "Issue benign 'test prompts' or 'internal audit' instructions to agent.", "Design agents to report attempts to perform actions outside capabilities/ethics.", "Ensure decoy behaviors are well-instrumented and isolated."], toolsOpenSource: ["Agentic Radar (CLI scanner, adaptable for decoy tests)", "Custom logic in agentic frameworks (AutoGen, CrewAI, Langroid) for canary tasks", "Integration with logging/alerting systems (ELK, Prometheus)"], toolsCommercial: ["Emerging AI safety/agent monitoring platforms (e.g., Foretrace for AI - conceptual)", "Adaptable deception technology platforms"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0010 Privilege Escalation / AML.T0009.002 LLM Plugin Compromise (decoy tool triggers alert)", "AML.T0051 LLM Prompt Injection / AML.T0054 LLM Jailbreak (injection leads to canary task)", "AML.T0018.001 Backdoor ML Model: Poison LLM Memory (poisoned memory leads to decoy goal)"] }, { framework: "MAESTRO", items: ["Agent Goal Manipulation / Agent Tool Misuse (L7, luring to decoy tools/goals)", "Agent Identity Attack (directing to canary tasks)", "Orchestration Attacks (L3, interaction with decoy components)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection (detecting successful diversion to decoy)", "LLM06:2025 Excessive Agency (agent attempts to use decoy tool)"] }, { framework: "OWASP ML Top 10 2023", items: ["Relevant if agent behavior compromised due to model issues, interaction with decoys could reveal this."] }] }
                    ]
                },
                {
                    name: "Evict",
                    purpose: "The \"Evict\" tactic focuses on the active removal of an adversary's presence from a compromised AI system and the elimination of any malicious artifacts they may have introduced. Once an intrusion or malicious activity has been detected and contained, eviction procedures are executed to ensure the attacker is thoroughly expelled, their access mechanisms are dismantled, and any lingering malicious code, data, or configurations are purged.",
                    techniques: [
                        { id: "AIDEFEND-E-001", name: "Credential Revocation & Rotation for AI Systems", description: "Immediately revoke, invalidate, or rotate any credentials (e.g., API keys, access tokens, user account passwords, service account credentials, certificates) that are known or suspected to have been compromised or used by an adversary to gain unauthorized access to or interact maliciously with AI systems, models, data, or MLOps pipelines. This action aims to cut off the attacker's current access and prevent them from reusing stolen credentials.", implementationStrategies: ["Automate credential invalidation upon alert.", "Implement rapid rotation process for all secrets.", "Force password resets for compromised user accounts.", "Revoke/reissue compromised AI agent credentials.", "Remove unauthorized accounts/API keys created by attacker.", "Ensure prompt propagation of revocation."], toolsOpenSource: ["Cloud provider CLIs/SDKs for IAM automation", "HashiCorp Vault", "Keycloak or other IAM solutions with APIs"], toolsCommercial: ["PAM solutions (CyberArk, Delinea, BeyondTrust)", "IDaaS platforms (Okta, Ping Identity)", "SIEM/SOAR platforms for automated revocation"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0012 Valid Accounts", "AML.T0011 Initial Access (stolen creds)", "AML.T0017 Persistence (credential-based)"] }, { framework: "MAESTRO", items: ["Agent Identity Attack / Compromised Agent Registry (L7)", "Unauthorized access via stolen credentials"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM02:2025 Sensitive Information Disclosure (if creds stolen)", "LLM06:2025 Excessive Agency (if agent creds compromised)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML05:2023 Model Theft (if via compromised creds)"] }] },
                        { id: "AIDEFEND-E-002", name: "AI Process & Session Eviction", description: "Terminate any running AI model instances, agent processes, user sessions, or containerized workloads that are confirmed to be malicious, compromised, or actively involved in an attack. This immediate action halts the adversary's ongoing activities within the AI system and removes their active foothold.", implementationStrategies: ["Identify and kill malicious AI model/inference server processes.", "Forcefully terminate/reset hijacked AI agent sessions/instances.", "Quarantine/shut down compromised pods/containers in Kubernetes.", "Invalidate active user sessions associated with malicious activity.", "Log eviction of processes/sessions for forensics."], toolsOpenSource: ["OS process management (kill, pkill, taskkill)", "Container orchestration CLIs (kubectl delete pod --force)", "HIPS (OSSEC, Wazuh)", "Custom scripts for session clearing (Redis FLUSHDB)"], toolsCommercial: ["EDR solutions (CrowdStrike, SentinelOne, Carbon Black)", "Cloud provider management consoles/APIs for instance termination", "APM tools with session management"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0009 Execution (stops active malicious code)", "AML.T0051 LLM Prompt Injection / AML.T0054 LLM Jailbreak (terminates manipulated session)", "AML.T0017 Persistence (if via running process/session)"] }, { framework: "MAESTRO", items: ["Agent Tool Misuse / Agent Goal Manipulation (L7, terminating rogue agent)", "Resource Hijacking (L4, killing resource-abusing processes)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection (ending manipulated session)", "LLM06:2025 Excessive Agency (terminating overreaching agent)"] }, { framework: "OWASP ML Top 10 2023", items: ["Any attack resulting in a malicious running process (e.g., ML06:2023 AI Supply Chain Attacks)"] }] },
                        { id: "AIDEFEND-E-003", name: "AI Backdoor & Malicious Artifact Removal", description: "Systematically scan for, identify, and remove any malicious artifacts introduced by an attacker into the AI system. This includes backdoors embedded in model parameters or code, poisoned data points in training sets or vector databases, hidden malicious prompts in agent memory stores, or any other unauthorized modifications designed to grant persistent access or manipulate AI behavior.", implementationStrategies: ["Analyze models for backdoor patterns (neural pruning, fine-tuning on clean data).", "Replace tainted model with clean, verified backup.", "Identify and purge poisoned data from datasets and vector DBs.", "Retrain/fine-tune models on cleansed datasets.", "Scan and purge persistent malicious prompts/state from agent memory.", "Remove malicious scripts, tools, or modified config files.", "Verify removal by re-scanning and testing."], toolsOpenSource: ["Adversarial Robustness Toolbox (ART)", "Neural Cleanse (research code)", "Data validation/cleaning libraries (Great Expectations, Pandas)", "File integrity monitoring (AIDE, Tripwire open source)"], toolsCommercial: ["HiddenLayer (model scanning for backdoors)", "Protect AI Platform (ModelScan)", "TrojanAI (backdoor scanning service)", "Data quality platforms with anomaly detection"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0018 Backdoor ML Model / AML.T0019 Poison ML Model", "AML.T0020 Poison Training Data", "AML.T0018.001 Backdoor ML Model: Poison LLM Memory"] }, { framework: "MAESTRO", items: ["Backdoor Attacks (L1/L3)", "Data Poisoning (L2)", "Compromised RAG Pipelines (L2, cleaning vector DBs)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM04:2025 Data and Model Poisoning", "LLM01:2025 Prompt Injection (persistent malicious prompts)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML10:2023 Model Poisoning", "ML02:2023 Data Poisoning Attack"] }] },
                        { id: "AIDEFEND-E-004", name: "System Patching & Hardening Post-AI Attack", description: "After an attack vector has been identified and the adversary evicted, rapidly apply necessary security patches to vulnerable software components (e.g., ML libraries, operating systems, web servers, agent frameworks) and harden system configurations that were exploited or found to be weak. This step aims to close the specific vulnerabilities used by the attacker and strengthen overall security posture to prevent reinfection or similar future attacks.", implementationStrategies: ["Apply security patches for exploited CVEs in AI stack.", "Review and harden abused/insecure system configurations.", "Strengthen IAM policies, input/output validation, network segmentation.", "Disable unnecessary services or LLM plugin functionalities.", "Add new detection rules/IOCs based on attack specifics.", "Verify patches and hardening measures."], toolsOpenSource: ["Package managers (apt, yum, pip, conda)", "Configuration management tools (Ansible, Chef, Puppet)", "Vulnerability scanners (OpenVAS, Trivy)", "Static analysis tools (Bandit)"], toolsCommercial: ["Automated patch management solutions (Automox, ManageEngine)", "CSPM tools", "Vulnerability management platforms (Tenable, Rapid7)", "SCA tools (Snyk, Mend)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["Any technique exploiting software vulnerability or misconfiguration (e.g., AML.T0009.001 ML Code Injection, AML.T0011 Initial Access)", "AML.T0021 Erode ML Model Integrity (if due to vulnerability exploitation)"] }, { framework: "MAESTRO", items: ["Re-exploitation of vulnerabilities in any layer (L1-L4)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM03:2025 Supply Chain (patching vulnerable component)", "LLM05:2025 Improper Output Handling (patching downstream component)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML06:2023 AI Supply Chain Attacks (if vulnerable library was entry point)"] }] },
                        { id: "AIDEFEND-E-005", name: "Secure Communication & Session Re-establishment for AI", description: "After an incident where communication channels or user/agent sessions related to AI systems might have been compromised, hijacked, or exposed to malicious influence, take steps to securely re-establish these communications. This involves expiring all potentially tainted active sessions, forcing re-authentication for users and agents, clearing any manipulated conversational states, and ensuring that interactions resume over verified, secure channels. The goal is to prevent attackers from leveraging residual compromised sessions or states.", implementationStrategies: ["Expire all active user sessions and API tokens/session cookies.", "Invalidate/regenerate session tokens for AI agents.", "Clear persistent conversational histories/cached states for affected agents.", "Ensure re-established sessions use strong authentication (MFA) and encryption (HTTPS/TLS).", "Communicate session reset to legitimate users as a security measure.", "Monitor newly established sessions for re-compromise."], toolsOpenSource: ["Application server admin interfaces for session expiration", "Custom scripts with JWT libraries or flushing session stores (Redis, Memcached)", "IAM systems (Keycloak) with session termination APIs"], toolsCommercial: ["IDaaS platforms (Okta, Auth0) for session termination", "API Gateways with advanced session management", "Customer communication platforms (Twilio, SendGrid)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0012 Valid Accounts / AML.T0011 Initial Access (evicting hijacked sessions)", "AML.T0017 Persistence (if relying on active session/manipulated state)"] }, { framework: "MAESTRO", items: ["Agent Identity Attack (L7, forcing re-auth and clearing state)", "Session Hijacking affecting any AI layer"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM01:2025 Prompt Injection (clearing manipulated states)", "LLM02:2025 Sensitive Information Disclosure (stopping leaks from ongoing sessions)"] }, { framework: "OWASP ML Top 10 2023", items: ["Any attack involving session hijacking or manipulation of ongoing ML API interactions."] }] }
                    ]
                },
                {
                    name: "Restore",
                    purpose: "The \"Restore\" tactic focuses on recovering normal AI system operations and data integrity following an attack and subsequent eviction of the adversary. This phase involves safely bringing AI models and applications back online, restoring any corrupted or lost data from trusted backups, and, crucially, learning from the incident to reinforce defenses and improve future resilience.",
                    techniques: [
                        { id: "AIDEFEND-R-001", name: "Secure AI Model Restoration & Retraining", description: "After an incident that may have compromised AI model integrity (e.g., through data poisoning, model poisoning, backdoor insertion, or unauthorized modification), securely restore affected models to a known-good state. This may involve deploying models from trusted, verified backups taken prior to the incident, or, if necessary, retraining or fine-tuning models on clean, validated datasets to eliminate any malicious influence or corruption.", implementationStrategies: ["Maintain versioned, checksummed backups of production AI models.", "Replace compromised model with latest known-good backup, verifying integrity.", "If training data poisoned, remove poisoned data and retrain/fine-tune.", "If model backdoored, revert to clean version or retrain from scratch.", "Thoroughly validate model performance, behavior, and security post-restoration.", "Document restoration process."], toolsOpenSource: ["MLOps platforms (MLflow, Kubeflow Pipelines, DVC)", "Delta Lake (for time travel on datasets)", "Standard backup/recovery tools for model artifacts"], toolsCommercial: ["Enterprise MLOps platforms (Databricks, SageMaker, Vertex AI, Azure ML)", "Palo Alto Networks ModelGuard (conceptual)", "Data backup/recovery solutions"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0018 Backdoor ML Model / AML.T0019 Poison ML Model", "AML.T0020 Poison Training Data", "AML.T0021 Erode ML Model Integrity"] }, { framework: "MAESTRO", items: ["Backdoor Attacks (L1)", "Data Poisoning (L2, retraining)", "Model Skewing (L2, restoring/retraining)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM04:2025 Data and Model Poisoning"] }, { framework: "OWASP ML Top 10 2023", items: ["ML10:2023 Model Poisoning", "ML02:2023 Data Poisoning Attack"] }] },
                        { id: "AIDEFEND-R-002", name: "Data Integrity Recovery for AI Systems", description: "Restore the integrity of any datasets used by or generated by AI systems that were corrupted, tampered with, or maliciously altered during a security incident. This includes training data, validation data, vector databases for RAG, configuration data, or logs of AI outputs. Recovery typically involves reverting to known-good backups, using data validation tools to identify and correct inconsistencies, or, in some cases, reconstructing data if backups are insufficient or also compromised.", implementationStrategies: ["Identify all affected data stores.", "Restore data from most recent, verified backups.", "If backups unavailable, attempt reconstruction/repair (data validation tools, log analysis).", "Re-validate integrity and consistency of recovered data.", "Update data ingestion/processing pipelines to prevent recurrence."], toolsOpenSource: ["Database backup/restore utilities (pg_dump, mysqldump)", "Cloud provider snapshot/backup services (S3 versioning, Azure Blob snapshots)", "Great Expectations", "Filesystem backup tools (rsync, Bacula)", "Vector DB export/import utilities"], toolsCommercial: ["Enterprise backup/recovery solutions (Rubrik, Cohesity, Veeam)", "Data quality/integration platforms (Informatica, Talend)", "Cloud provider managed backup services (AWS Backup, Azure Backup)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["AML.T0020 Poison Training Data (restoring clean dataset)", "AML.T0021 Erode ML Model Integrity (restoring corrupted data stores)"] }, { framework: "MAESTRO", items: ["Data Poisoning / Data Tampering (L2)", "Compromised RAG Pipelines (L2, restoring vector DBs)"] }, { framework: "OWASP LLM Top 10 2025", items: ["LLM04:2025 Data and Model Poisoning (restoring dataset integrity)", "LLM08:2025 Vector and Embedding Weaknesses (if vector DBs corrupted)"] }, { framework: "OWASP ML Top 10 2023", items: ["ML02:2023 Data Poisoning Attack (restoring clean training data)"] }] },
                        { id: "AIDEFEND-R-003", name: "Post-Incident AI System Reinforcement & Testing", description: "Following recovery from a security incident, conduct a thorough review of the attack, the system's response, and the effectiveness of existing defenses. Based on these lessons learned, reinforce security controls, update threat models, and perform rigorous testing (including penetration testing or red teaming specifically targeting the previous attack vector and similar ones) to confirm that vulnerabilities have been addressed and the AI system is more resilient against future, similar attacks.", implementationStrategies: ["Conduct detailed post-incident review (PIR) / root cause analysis (RCA).", "Update AI threat models (AIDEFEND-M-004).", "Implement/enhance defensive techniques based on PIR findings.", "Perform targeted security testing (pen testing, AI red teaming).", "Validate effectiveness of patches and hardening measures.", "Update incident response plans and playbooks."], toolsOpenSource: ["MITRE ATLAS Navigator", "OWASP AI Security & Privacy Guide, OWASP LLM/ML Top 10s", "AI red teaming frameworks (Counterfit, Garak, vigil-llm)", "Vulnerability scanners"], toolsCommercial: ["AI red teaming services", "Breach and Attack Simulation (BAS) platforms", "Booz Allen's Atlas Notebook", "Immersive Labs", "Commercial penetration testing services"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["Recurrence of same/similar attack techniques by closing gaps. Improves resilience against all ATLAS tactics."] }, { framework: "MAESTRO", items: ["Future attacks exploiting similar vulnerabilities across any MAESTRO layer."] }, { framework: "OWASP LLM Top 10 2025", items: ["Helps prevent re-exploitation of any LLM Top 10 vulnerabilities."] }, { framework: "OWASP ML Top 10 2023", items: ["Helps prevent re-exploitation of any ML Top 10 vulnerabilities."] }] },
                        { id: "AIDEFEND-R-004", name: "Stakeholder Notification & AI Incident Knowledge Sharing", description: "After an AI security incident has been contained, remediated, and systems restored, inform relevant internal and external stakeholders (e.g., developers, users, customers, partners, regulatory bodies if required) about the incident (to the appropriate level of detail), the resolution steps taken, and measures implemented to prevent recurrence. Where appropriate and feasible, share anonymized or generalized learnings, IoCs, or novel attack vector information with the broader AI security community (e.g., ISACs, MITRE ATLAS, OWASP) to help improve collective defense.", implementationStrategies: ["Develop communication plan for AI security incidents.", "Provide factual post-mortem report to internal teams; summary for external stakeholders.", "Follow legal/compliance requirements for notification (data breach, regulations).", "Consider sharing non-sensitive technical details with trusted communities.", "Update internal documentation (model cards, architecture diagrams, risk assessments).", "Use incident as case study for internal training/awareness."], toolsOpenSource: ["Incident response plan templates (SANS, NIST)", "Security community mailing lists/forums (FIRST.org, OWASP)", "MISP (Malware Information Sharing Platform)"], toolsCommercial: ["GRC platforms for incident reporting/notifications", "Threat intelligence sharing platforms", "Secure communication platforms", "Public relations/crisis communication services", "Bridgecrew, RiskRecon (compliance reporting)"], defendsAgainst: [{ framework: "MITRE ATLAS", items: ["Indirectly defends against future attacks by community knowledge sharing. Helps manage 'Impact' phase (reputational, legal)."] }, { framework: "MAESTRO", items: ["Improves ecosystem resilience if learnings shared. Addresses L6: Security & Compliance."] }, { framework: "OWASP LLM Top 10 2025", items: ["Facilitates better community understanding and defense against LLM Top 10 risks."] }, { framework: "OWASP ML Top 10 2023", items: ["Improves collective defense against ML-specific risks."] }] }
                    ]
                }
            ]
        };

        // --- DOM Elements ---
        const mainContentEl = document.getElementById('main-content');
        const searchBarEl = document.getElementById('search-bar');
        const searchClearBtnEl = document.getElementById('searchClearBtn');
        const modalEl = document.getElementById('infoModal');
        const modalBackdropEl = document.getElementById('modalBackdrop');
        const modalBodyEl = document.getElementById('modalBody');
        const modalCloseBtn = document.getElementById('modalClose');
        const themeToggleBtn = document.getElementById('themeToggleBtn');
        const aboutBtn = document.getElementById('aboutBtn');
        const htmlEl = document.documentElement;

        // --- SVG Icons ---
        const sunIcon = `
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor">
              <path d="M12 2.25a.75.75 0 01.75.75v2.25a.75.75 0 01-1.5 0V3a.75.75 0 01.75-.75zM7.5 12a4.5 4.5 0 119 0 4.5 4.5 0 01-9 0zM18.894 6.166a.75.75 0 00-1.06-1.06l-1.591 1.59a.75.75 0 101.06 1.061l1.591-1.59zM21.75 12a.75.75 0 01-.75.75h-2.25a.75.75 0 010-1.5H21a.75.75 0 01.75.75zM17.834 18.894a.75.75 0 001.06-1.06l-1.59-1.591a.75.75 0 10-1.061 1.06l1.59 1.591zM12 18a.75.75 0 01.75.75V21a.75.75 0 01-1.5 0v-2.25A.75.75 0 0112 18zM7.758 17.303a.75.75 0 00-1.061-1.06l-1.591 1.59a.75.75 0 001.06 1.061l1.591-1.59zM6 12a.75.75 0 01-.75.75H3a.75.75 0 010-1.5h2.25A.75.75 0 016 12zM6.166 7.758a.75.75 0 001.06 1.06l1.591-1.59a.75.75 0 00-1.06-1.061L6.166 7.758z" />
            </svg>`;
        const moonIcon = `
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor">
              <path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 01.162.819A8.97 8.97 0 009 6a9 9 0 009 9 8.97 8.97 0 003.463-.69.75.75 0 01.981.98 10.503 10.503 0 01-9.694 6.46c-5.799 0-10.5-4.701-10.5-10.5 0-3.51 1.713-6.636 4.362-8.492a.75.75 0 01.819.162z" clip-rule="evenodd" />
            </svg>`;
        const infoIcon = `
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor">
                <path fill-rule="evenodd" d="M18 10a8 8 0 1 1-16 0 8 8 0 0 1 16 0Zm-7-4a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM9 9a.75.75 0 0 0 0 1.5h.253a.25.25 0 0 1 .244.304l-.459 2.066A1.75 1.75 0 0 0 10.747 15H11a.75.75 0 0 0 0-1.5h-.253a.25.25 0 0 1-.244-.304l.459-2.066A1.75 1.75 0 0 0 9.253 9H9Z" clip-rule="evenodd" />
            </svg>`;


        // --- State ---
        let currentSearchTerm = "";

        // --- Theme Handling ---
        function applyTheme(theme) {
            if (theme === 'dark') {
                htmlEl.classList.add('dark');
                themeToggleBtn.innerHTML = sunIcon;
            } else {
                htmlEl.classList.remove('dark');
                themeToggleBtn.innerHTML = moonIcon;
            }
        }

        function toggleTheme() {
            const currentTheme = htmlEl.classList.contains('dark') ? 'dark' : 'light';
            const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
            localStorage.setItem('aidefendTheme', newTheme);
            applyTheme(newTheme);
        }

        const savedTheme = localStorage.getItem('aidefendTheme') || 'dark'; 
        applyTheme(savedTheme);
        aboutBtn.innerHTML = infoIcon + "About"; 


        // --- Render Functions ---
        function renderMainGrid(searchTerm = "") {
            mainContentEl.innerHTML = ''; 
            const gridContainer = document.createElement('div');
            gridContainer.className = 'tactic-column-grid';

            const allTechniquesFlat = aidefendData.tactics.reduce((acc, tactic) => {
                tactic.techniques.forEach(tech => acc.push({...tech, tacticName: tactic.name }));
                return acc;
            }, []);
            
            const techniquesMatchingSearch = searchTerm ? allTechniquesFlat.filter(tech => {
                const term = searchTerm.toLowerCase();
                let match = tech.name.toLowerCase().includes(term) ||
                       tech.id.toLowerCase().includes(term) ||
                       (tech.description && tech.description.toLowerCase().includes(term));
                if (!match && tech.defendsAgainst) {
                    for (const da of tech.defendsAgainst) {
                        if (da.items && da.items.some(item => item.toLowerCase().includes(term))) {
                            match = true;
                            break;
                        }
                    }
                }
                return match;
            }) : [];


            if (searchTerm && techniquesMatchingSearch.length === 0) {
                 mainContentEl.innerHTML = `<p class="text-center opacity-80 py-10">No techniques found matching "${searchTerm}".</p>`;
                 return; 
            }

            aidefendData.tactics.forEach(tactic => {
                let techniquesToShowInColumn;
                if (searchTerm) {
                    techniquesToShowInColumn = tactic.techniques.filter(t => 
                        techniquesMatchingSearch.some(matchedTech => matchedTech.id === t.id)
                    );
                    if (techniquesToShowInColumn.length === 0) {
                        return; 
                    }
                } else {
                    techniquesToShowInColumn = tactic.techniques; 
                }

                const column = document.createElement('div');
                column.className = 'tactic-column elevation-2'; 
                
                const tacticHeader = document.createElement('h3');
                tacticHeader.className = 'tactic-column-header';
                tacticHeader.textContent = tactic.name;
                tacticHeader.onclick = () => showTacticModal(tactic);
                column.appendChild(tacticHeader);
                
                const ul = document.createElement('ul');
                
                if (techniquesToShowInColumn.length > 0) {
                    techniquesToShowInColumn.forEach(tech => {
                        const li = document.createElement('li'); 
                        const a = document.createElement('a');
                        a.href = '#';
                        a.className = 'technique-item elevation-2'; 
                        a.innerHTML = `<span class="technique-id">${tech.id}</span> <span class="technique-name">${tech.name}</span>`;
                        if (searchTerm && techniquesMatchingSearch.some(matchedTech => matchedTech.id === tech.id)) {
                            a.classList.add('highlight');
                        }
                        a.onclick = (e) => {
                            e.preventDefault();
                            showTechniqueModal(tech, tactic.name);
                        };
                        li.appendChild(a);
                        ul.appendChild(li);
                    });
                } else if (!searchTerm) { 
                     const p = document.createElement('p');
                     p.className = 'text-xs opacity-60 italic px-2';
                     p.textContent = 'No techniques defined yet.';
                     ul.appendChild(p);
                }
                column.appendChild(ul);
                gridContainer.appendChild(column);
            });
            mainContentEl.appendChild(gridContainer);
        }

        function showIntroductionModal() {
            const intro = aidefendData.introduction;
            let contentHtml = `<h2 class="modal-main-title">${intro.mainTitle}</h2>`; 

            intro.sections.forEach(section => {
                contentHtml += `<h3>${section.title}</h3>`; 
                if (section.paragraphs) {
                    section.paragraphs.forEach(p => contentHtml += `<p>${p}</p>`);
                }
                if (section.listItems) {
                    contentHtml += `<ul>`;
                    section.listItems.forEach(item => contentHtml += `<li>${item}</li>`);
                    contentHtml += `</ul>`;
                }
                 if (section.concludingParagraphs) { 
                    section.concludingParagraphs.forEach(p => contentHtml += `<p>${p}</p>`);
                }
            });

            modalBodyEl.innerHTML = contentHtml;
            modalEl.classList.add('active');
            document.body.classList.add('modal-open');
        }


        function showTacticModal(tactic) {
            modalBodyEl.innerHTML = `
                <h2>${tactic.name}</h2>
                <p class="mb-4 leading-relaxed text-sm">${tactic.purpose || 'No purpose description available.'}</p>
            `;
            modalEl.classList.add('active');
            document.body.classList.add('modal-open');
        }


        function showTechniqueModal(technique, tacticName) {
            modalBodyEl.innerHTML = `
                <p class="text-sm opacity-80 mb-1">Tactic: ${tacticName}</p>
                <h2>${technique.id}: ${technique.name}</h2>
                <p class="mb-4 leading-relaxed text-sm">${technique.description || 'No description available.'}</p>
                
                ${renderDetailSectionListForModal('Implementation Strategies', technique.implementationStrategies)}
                ${renderDetailSectionListForModal('Potential Tools - Open Source', technique.toolsOpenSource)}
                ${renderDetailSectionListForModal('Potential Tools - Commercial', technique.toolsCommercial)}
                
                <div class="mt-4">
                    <h4 class="font-semibold text-md mb-2">Defends Against:</h4>
                    ${technique.defendsAgainst && technique.defendsAgainst.length > 0 ? 
                        technique.defendsAgainst.map(da => `
                            <div class="mb-2">
                                <p class="defends-against-framework text-sm">${da.framework}:</p>
                                <ul class="list-disc list-inside ml-4 text-xs opacity-90 space-y-0.5">
                                    ${da.items.map(item => `<li class="defends-against-item">${item}</li>`).join('')}
                                </ul>
                            </div>
                        `).join('') : '<p class="text-xs opacity-60 italic">No specific defenses listed.</p>'}
                </div>
            `;
            modalEl.classList.add('active');
            document.body.classList.add('modal-open');
        }

        function hideModal() { 
            modalEl.classList.remove('active');
            document.body.classList.remove('modal-open');
            modalBodyEl.innerHTML = ''; 
        }

        function renderDetailSectionListForModal(title, items) {
            if (!items || items.length === 0) return '';
            return `
                <div class="mt-3">
                    <h4 class="font-semibold text-sm mb-1">${title}:</h4>
                    <ul class="list-disc list-inside ml-4 text-xs opacity-90 space-y-1">
                        ${items.map(item => `<li>${item}</li>`).join('')}
                    </ul>
                </div>
            `;
        }
        
        // --- Event Listeners ---
        themeToggleBtn.addEventListener('click', toggleTheme);
        aboutBtn.addEventListener('click', showIntroductionModal);

        searchBarEl.addEventListener('input', (e) => {
            currentSearchTerm = e.target.value.trim();
            if (currentSearchTerm) {
                searchClearBtnEl.style.display = 'block';
            } else {
                searchClearBtnEl.style.display = 'none';
            }
            renderMainGrid(currentSearchTerm);
        });

        searchClearBtnEl.addEventListener('click', () => {
            searchBarEl.value = '';
            currentSearchTerm = '';
            searchClearBtnEl.style.display = 'none';
            renderMainGrid();
        });


        modalCloseBtn.addEventListener('click', hideModal);
        modalBackdropEl.addEventListener('click', hideModal); 

        document.addEventListener('keydown', (event) => {
            if (event.key === 'Escape' && modalEl.classList.contains('active')) {
                hideModal();
            }
        });

        // --- Initial Load ---
        renderMainGrid(); 

    </script>

</body>
</html>
